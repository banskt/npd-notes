{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fa73b0-3cec-4a83-9a9e-c5643813862f",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Metrices for evaluating clusters given true labels\"\n",
    "author: \"Saikat Banerjee\"\n",
    "format:\n",
    "  html: default\n",
    "date: \"2023-06-23\"\n",
    "file-modified: \"2023-06-23\"\n",
    "abstract: \"We explore different external validation metrics to measure the quality of clustering results. These external indices measure the agreement between the predicted partition and the known partition.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb78332-343e-4a46-8e3f-9dda6b26a0dd",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "There are many aspects of \"rightness\" for clustering.\n",
    "Broadly, there are two kinds of validity indices to measure the quality of clustering results: \n",
    "external indices and internal indices.\n",
    "An external index is a measure of agreement between two partitions where the first partition \n",
    "is the a priori known clustering structure, and the second results from the clustering procedure.\n",
    "Internal indices are used to measure the goodness of a clustering structure without external information.\n",
    "For external indices, we evaluate the results of a clustering algorithm based on a known cluster structure of a data set (or cluster labels).\n",
    "Here, we look at several possible external validation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d51fc-f130-4329-b1d0-b56570db2157",
   "metadata": {},
   "source": [
    "# Getting set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3fdca5-edc5-4b1d-b13a-203143196f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import model_selection as skmodel\n",
    "from sklearn import metrics as skmetrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pymir import mpl_stylesheet\n",
    "from pymir import mpl_utils\n",
    "\n",
    "mpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852829c2-9cbb-425c-b51e-d8d0be584c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data\"\n",
    "beta_df_filename   = f\"{data_dir}/beta_df.pkl\"\n",
    "prec_df_filename   = f\"{data_dir}/prec_df.pkl\"\n",
    "se_df_filename     = f\"{data_dir}/se_df.pkl\"\n",
    "zscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n",
    "\n",
    "'''\n",
    "Data Frames for beta, precision, standard error and zscore.\n",
    "'''\n",
    "\n",
    "beta_df   = pd.read_pickle(beta_df_filename)\n",
    "prec_df   = pd.read_pickle(prec_df_filename)\n",
    "se_df     = pd.read_pickle(se_df_filename)\n",
    "zscore_df = pd.read_pickle(zscore_df_filename)\n",
    "\n",
    "trait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\n",
    "phenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be23156-8c52-457c-a46a-77005df90e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 69 samples (phenotypes) and 10068 features (variants)\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "\n",
    "'''\n",
    "X matrix (n_samples x n_features) -- obtain from Z-scores\n",
    "'''\n",
    "select_ids = beta_df.columns\n",
    "X = np.array(zscore_df[select_ids]).T # contain NaN values\n",
    "colmeans = np.nanmean(X, axis = 0, keepdims = True)\n",
    "Xcent = X - colmeans # contain NaN values\n",
    "Xcent = np.nan_to_num(X, nan=0) # remove NaN values\n",
    "\n",
    "'''\n",
    "Y vector (n_samples) -- contain class labels\n",
    "'''\n",
    "\n",
    "labels = [phenotype_dict[x] for x in select_ids]\n",
    "unique_labels = list(set(labels))\n",
    "encoding = {x:i for i, x in enumerate(unique_labels)}\n",
    "Ylabels = np.array([encoding[x] for x in labels])\n",
    "\n",
    "print (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4706d6-38bb-49c0-8efe-aa094e2a5c29",
   "metadata": {},
   "source": [
    "# Sample counts of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eac6035-5f24-4cab-bc97-4a97b8c7b5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count   Phenotype\n",
      "-----   ---------\n",
      "7\tSleep\n",
      "3\tSZ\n",
      "2\tASD\n",
      "2\tMigraine\n",
      "2\tADHD\n",
      "1\tOCD\n",
      "8\tDepression\n",
      "2\tIntel/education\n",
      "11\tEpilepsy\n",
      "10\tOther psych\n",
      "7\tCognition\n",
      "6\tBD\n",
      "8\tNeurodegenerative\n"
     ]
    }
   ],
   "source": [
    "sample_counts = {label : (Ylabels == idx).sum() for label, idx in encoding.items()}\n",
    "print (f\"Count   Phenotype\")\n",
    "print (f\"-----   ---------\")\n",
    "for phenotype, count in sample_counts.items():\n",
    "    print (f\"{count}\\t{phenotype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ca369-f343-41d0-91d5-ae261f0ca4b2",
   "metadata": {},
   "source": [
    "# Split into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51cf9fec-d1b3-4ad8-897f-2af463cfcf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train   Test    Phenotype\n",
      "-----   ----    ---------\n",
      "7\t0\tSleep\n",
      "2\t1\tSZ\n",
      "0\t2\tASD\n",
      "1\t1\tMigraine\n",
      "1\t1\tADHD\n",
      "0\t1\tOCD\n",
      "5\t3\tDepression\n",
      "1\t1\tIntel/education\n",
      "9\t2\tEpilepsy\n",
      "5\t5\tOther psych\n",
      "4\t3\tCognition\n",
      "6\t0\tBD\n",
      "5\t3\tNeurodegenerative\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection as skmodel\n",
    "\n",
    "'''\n",
    "One-liner to split:\n",
    "# X_train, X_test, y_train, y_test = skmodel.train_test_split(X, Ylabels, test_size = 0.33)\n",
    "but it does not return the index for the training and test data,\n",
    "so I use a little more verbose solution\n",
    "'''\n",
    "itrain, itest = skmodel.train_test_split(np.arange(Ylabels.shape[0]), test_size = 0.33)\n",
    "X_train = X[itrain, :]\n",
    "X_test  = X[itest, :]\n",
    "y_train = Ylabels[itrain]\n",
    "y_test  = Ylabels[itest]\n",
    "\n",
    "print (f\"Train   Test    Phenotype\")\n",
    "print (f\"-----   ----    ---------\")\n",
    "for phenotype, idx in encoding.items():\n",
    "    train_count = np.sum(y_train == idx)\n",
    "    test_count  = np.sum(y_test  == idx)\n",
    "    print (f\"{train_count}\\t{test_count}\\t{phenotype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b02b0d-1f78-40dc-9160-60ff95a9783a",
   "metadata": {},
   "source": [
    "# Clustering from distance matrix\n",
    "\n",
    "We want to cluster the samples based on the Euclidean distance between them, obtained from the feature matrix.\n",
    "There are hundreds of algorithms to choose from, for example:\n",
    "- Hierarchical clustering in it's myriad of variants. Cut the dendrogram as desired, e.g., to get k clusters\n",
    "- PAM, the closest match to k-means on a distance matrix (minimizes the average distance from the cluster center)\n",
    "- Spectral clustering\n",
    "- DBSCAN\n",
    "- OPTICS\n",
    "- HDBSCAN*\n",
    "- Affinity Propagation\n",
    "\n",
    "Available Software in Python:\n",
    "- [pyclustering](https://pyclustering.github.io) for fast Python implementation of different algorithms. They have nice documentation and examples.\n",
    "- [sklearn.cluster](https://scikit-learn.org/stable/modules/clustering.html)\n",
    "- [HDBSCAN*](https://hdbscan.readthedocs.io/en/latest/index.html) provides a very nice documentation for comparing different algorithms (albeit a bit biased, highlighting their own strength).\n",
    "- [scipy.cluster](https://docs.scipy.org/doc/scipy/reference/cluster.html) provides the `hierarchy` module which has functions for hierarchical and agglomerative clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0e83d9-444c-4349-b57e-2cb2c5dfb27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = skmetrics.pairwise.pairwise_distances(Xcent, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "250a814e-f74a-4179-9530-a9a9c7b26bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters = len(unique_labels), linkage = 'average', metric = 'precomputed')\n",
    "Y_pred = model.fit_predict(distance_matrix)\n",
    "#km = KMeans(n_clusters = len(unique_labels), random_state = 0, n_init=\"auto\")\n",
    "#km.fit(Xcent)\n",
    "#Y_pred = km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a9c5f83-f818-4ea6-9843-e0e34859b8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_random = np.random.choice(len(unique_labels), size=Ylabels.shape[0], replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bcd58d-b9c3-4229-94d5-58409fe9dc9b",
   "metadata": {},
   "source": [
    "# Comparison Metrics\n",
    "\n",
    "We can use several external validation techniques to assess the quality or \"correctness\" of the clusters\n",
    "since we have manually assigned the cluster labels.\n",
    "For example, we can use adjusted rand index, \n",
    "adjusted mutual information, homogeneity/completeness/v-measure, Fowlkes-Mallows score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acfdef-caf8-4ead-8739-6a89e6b2e885",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb8a8ece-0d35-4ac6-a14f-496f4fa405e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: -0.01573\n",
      "Predicted: 0.15229\n"
     ]
    }
   ],
   "source": [
    "print (f\"Random: {skmetrics.adjusted_rand_score(Ylabels, Y_random):.5f}\")\n",
    "print (f\"Predicted: {skmetrics.adjusted_rand_score(Ylabels, Y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06012c-1e6c-42e1-99c0-e1ccb6899563",
   "metadata": {},
   "source": [
    "### Adjusted Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e45dc60a-0802-4c8b-870b-68b1b3336ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: -0.02511\n",
      "Predicted: 0.36902\n"
     ]
    }
   ],
   "source": [
    "print (f\"Random: {skmetrics.adjusted_mutual_info_score(Ylabels, Y_random):.5f}\")\n",
    "print (f\"Predicted: {skmetrics.adjusted_mutual_info_score(Ylabels, Y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4219f11a-07e0-4f23-b073-d59d035afa7f",
   "metadata": {},
   "source": [
    "### Homogeneity and V-measure\n",
    "\n",
    "Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:\n",
    "- **homogeneity**: each cluster contains only members of a single class.\n",
    "- **completeness**: all members of a given class are assigned to the same cluster.\n",
    "\n",
    "We turn those concept as scores `homogeneity_score` and `completeness_score`. \n",
    "Both are bounded below by 0.0 and above by 1.0 (higher is better).\n",
    "Their harmonic mean called V-measure is computed by `v_measure_score`.\n",
    "\n",
    "**Note.** `v_measure_score` is symmetric: it can be used to evaluate the agreement of two independent assignments on the same dataset.\n",
    "This is not the case for `completeness_score` and `homogeneity_score`: both are bound by the relationship:\n",
    "```\n",
    "homogeneity_score(a, b) == completeness_score(b, a)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c40857f-64d7-4554-926a-62276be0d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Homogeneity \tCompleteness \tV-Measure\n",
      "Random:    0.36793 \t0.35036 \t0.35893\n",
      "Predicted: 0.47942 \t0.70042 \t0.56922\n"
     ]
    }
   ],
   "source": [
    "print (f\"        Homogeneity \\tCompleteness \\tV-Measure\")\n",
    "\n",
    "hcv_random = skmetrics.homogeneity_completeness_v_measure(Ylabels, Y_random)\n",
    "hcv_pred   = skmetrics.homogeneity_completeness_v_measure(Ylabels, Y_pred)\n",
    "\n",
    "print (\"Random:    \" + ' \\t'.join([f\"{x:.5f}\" for x in hcv_random]))\n",
    "print (\"Predicted: \" + ' \\t'.join([f\"{x:.5f}\" for x in hcv_pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7b9089-6bcc-4362-97ee-29acba8ad14e",
   "metadata": {},
   "source": [
    "### Fowlkes-Mallows scores\n",
    "\n",
    "FMI is defined as the geometric mean of the pairwise precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55dd5ec8-cc43-4c21-93f6-d1e1806b2505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random: 0.07035\n",
      "Random: 0.34056\n"
     ]
    }
   ],
   "source": [
    "print (f\"Random: {skmetrics.fowlkes_mallows_score(Ylabels, Y_random):.5f}\")\n",
    "print (f\"Random: {skmetrics.fowlkes_mallows_score(Ylabels, Y_pred):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c21c0c-4f60-4da1-ae9b-93797dcca6d6",
   "metadata": {},
   "source": [
    "# Does distance matrix from truncated SVD improve score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a851488-83a8-4bba-a53c-f51dc06c7d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3878603292380521"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 20\n",
    "\n",
    "U, S, Vt = np.linalg.svd(Xcent, full_matrices=False)\n",
    "pcomp_tsvd = U[:, :K] @ np.diag(S[:K])\n",
    "\n",
    "distance_matrix_tsvd = skmetrics.pairwise.pairwise_distances(pcomp_tsvd, metric='euclidean')\n",
    "\n",
    "model = AgglomerativeClustering(n_clusters = len(unique_labels), linkage = 'average', metric = 'precomputed')\n",
    "Y_pred_tsvd = model.fit_predict(distance_matrix_tsvd)\n",
    "\n",
    "skmetrics.adjusted_mutual_info_score(Ylabels, Y_pred_tsvd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f29ac3-61e3-491c-aa3d-f305e5ae5f84",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "- [Scikit: Clustering Performance Evaluation](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n",
    "- [How to compare a clustering algorithm partition to a \"ground truth\"?](https://stats.stackexchange.com/questions/260229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51bfea-3e81-4867-8430-d659883f54aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
