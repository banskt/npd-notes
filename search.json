[
  {
    "objectID": "meetings/2023-07-12.html",
    "href": "meetings/2023-07-12.html",
    "title": "2023-07-12",
    "section": "",
    "text": "I presented a high level summary of the methods we are developing.\nNeuropsychiatric disorders (NPD) are complex, heterogeneous disorders. Matrix factorization is a method that has been used to identify the shared and distinct factors of NPDs. Truncated SVD and genomic structural equation modeling (SEM) has been used earlier for multi-phenotype analysis. We are exploring different convex methods to obtain a low rank approximation of the input matrix before doing a PCA. Convex methods are reproducible. Currently, we are looking at 4 different methods for nuclear norm regularization to obtain the low rank approximation. We expect the low rank approximation will provide a more accurate estimate of the SNP’s true effect on a phenotype, cell type or other biological trait.\nAs of now, we hope to use the z-score and standard error for our model. GWAS data does not have to be limited to only patients with a diagnosis of BD, MDD or SZ as summary statistics from any phenotype will be modeled appropriately.\nCurrent main things to consider in regards to the model are:\n\nWhat is the normalisation method used in each of the GWAS summary statistics?\nIs there sample overlap between the studies?\n\nShane will follow-up with:\n\nResearching the normalisation method used in the selected studies.\nResearching the cohorts used / sample overlap between studies."
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Convex optimization for matrix factorization, Overleaf document (restricted access).\nLiterature review of GWAS on subphenotypes, curated by Shane Crinion and Niamh Mullins."
  },
  {
    "objectID": "resources/index.html#documents",
    "href": "resources/index.html#documents",
    "title": "Resources",
    "section": "",
    "text": "Convex optimization for matrix factorization, Overleaf document (restricted access).\nLiterature review of GWAS on subphenotypes, curated by Shane Crinion and Niamh Mullins."
  },
  {
    "objectID": "resources/index.html#github",
    "href": "resources/index.html#github",
    "title": "Resources",
    "section": "Github",
    "text": "Github\n\nPilot NPD Sumstat Project by David Knowles."
  },
  {
    "objectID": "resources/index.html#online-articles",
    "href": "resources/index.html#online-articles",
    "title": "Resources",
    "section": "Online Articles",
    "text": "Online Articles\n\nTips for formatting GWAS summary statistics.\nMatti Pirinen’s course on GWAS."
  },
  {
    "objectID": "notebooks/develop/2023-07-19-weighted-nnm-python-class.html",
    "href": "notebooks/develop/2023-07-19-weighted-nnm-python-class.html",
    "title": "Python Class for NNM using Frank-Wolfe algorithm",
    "section": "",
    "text": "About\nHere I develop the Python class for weighted nuclear norm minimization using Frank-Wolfe algorithm. This is not properly documented, but I keep this notebook for future reference.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_true)\n\n\n40\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_cent)\n\n\n499\n\n\n\n\nCode\nnp.linalg.norm(Y_cent, ord = 'nuc')\n\n\n6916.216191017533\n\n\n\n\nCode\nnp.linalg.norm(Y_std, ord = 'nuc')\n\n\n13488.398215158779\n\n\n\n\nCode\nnp.linalg.norm(Y_true, ord = 'nuc')\n\n\n562.4856656633167\n\n\n\n\nFunctions for NNMWF\n\n\nCode\nfrom sklearn.utils.extmath import randomized_svd\n\ndef nuclear_norm(X):\n    '''\n    Nuclear norm of input matrix\n    '''\n    return np.sum(np.linalg.svd(X)[1])\n\ndef f_objective(X, Y, W = None, mask = None):\n    '''\n    Objective function\n    Y is observed, X is estimated\n    W is the weight of each observation.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    # The * operator can be used as a shorthand for np.multiply on ndarrays.\n    if Wmask is None:\n        f_obj = 0.5 * np.linalg.norm(Y - Xmask, 'fro')**2\n    else:\n        f_obj = 0.5 * np.linalg.norm(Wmask * (Y - Xmask), 'fro')**2\n    return f_obj\n\n\ndef f_gradient(X, Y, W = None, mask = None):\n    '''\n    Gradient of the objective function.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    if Wmask is None:\n        f_grad = Xmask - Y\n    else:\n        f_grad = np.square(Wmask) * (Xmask - Y)\n    \n    return f_grad\n\n\ndef linopt_oracle(grad, r = 1.0, max_iter = 10, method = 'power'):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a nuclear norm ball for some r\n    '''\n    if method == 'power':\n        U1, V1_T = singular_vectors_power_method(grad, max_iter = max_iter)\n    elif method == 'randomized':\n        U1, V1_T = singular_vectors_randomized_method(grad, max_iter = max_iter)\n    S = - r * U1 @ V1_T\n    return S\n\n\ndef singular_vectors_randomized_method(X, max_iter = 10):\n    u, s, vh = randomized_svd(X, n_components = 1, n_iter = max_iter,\n                              power_iteration_normalizer = 'none',\n                              random_state = 0)\n    return u, vh\n\n\ndef singular_vectors_power_method(X, max_iter = 10):\n    '''\n    Power method.\n        \n        Computes approximate top left and right singular vector.\n        \n    Parameters:\n    -----------\n        X : array {m, n},\n            input matrix\n        max_iter : integer, optional\n            number of steps\n            \n    Returns:\n    --------\n        u, v : (n, 1), (p, 1)\n            two arrays representing approximate top left and right\n            singular vectors.\n    '''\n    n, p = X.shape\n    u = np.random.normal(0, 1, n)\n    u /= np.linalg.norm(u)\n    v = X.T.dot(u)\n    v /= np.linalg.norm(v)\n    for _ in range(max_iter):      \n        u = X.dot(v)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)       \n    return u.reshape(-1, 1), v.reshape(1, -1)\n\n\ndef do_step_size(dg, D, W = None, old_step = None):\n    if W is None:\n        denom = np.linalg.norm(D, 'fro')**2\n    else:\n        denom = np.linalg.norm(W * D, 'fro')**2\n    step_size = dg / denom\n    step_size = min(step_size, 1.0)\n    if step_size &lt; 0:\n        print (\"Warning: Step Size is less than 0\")\n        if old_step is not None and old_step &gt; 0:\n            print (\"Using previous step size\")\n            step_size = old_step\n        else:\n            step_size = 1.0\n    return step_size\n\n\ndef frank_wolfe_minimize_step(X, Y, r, istep, W = None, mask = None, old_step = None, svd_iter = None, svd_method = 'power'):\n    \n    # 1. Gradient for X_(t-1)\n    G = f_gradient(X, Y, W = W, mask = mask)\n    # 2. Linear optimization subproblem\n    if svd_iter is None: \n        svd_iter = 10 + int(istep / 20)\n        svd_iter = min(svd_iter, 25)\n    S = linopt_oracle(G, r, max_iter = svd_iter, method = svd_method)\n    # 3. Define D\n    D = X - S\n    # 4. Duality gap\n    dg = np.trace(D.T @ G)\n    # 5. Step size\n    step = do_step_size(dg, D, W = W, old_step = old_step)\n    # 6. Update\n    Xnew = X - step * D\n    return Xnew, G, dg, step\n\n\ndef frank_wolfe_minimize(Y, r, X0 = None,\n                         weight = None,\n                         mask = None,\n                         max_iter = 1000,\n                         svd_iter = None,\n                         svd_method = 'power',\n                         tol = 1e-4, step_tol = 1e-3, rel_tol = 1e-8,\n                         return_all = True,\n                         debug = False, debug_step = 10):\n    \n    # Step 0\n    old_X = np.zeros_like(Y) if X0 is None else X0.copy()\n    dg = np.inf\n    step = 1.0\n\n    if return_all:\n        dg_list = [dg]\n        fx_list = [f_objective(old_X, Y, W = weight, mask = mask)]\n        st_list = [1]\n        \n    # Steps 1, ..., max_iter\n    for istep in range(max_iter):\n        X, G, dg, step = \\\n            frank_wolfe_minimize_step(old_X, Y, r, istep, W = weight, mask = mask, old_step = step, svd_iter = svd_iter, svd_method = svd_method)\n        f_obj = f_objective(X, Y, W = weight, mask = mask)\n        fx_list.append(f_obj)\n\n        if return_all:\n            dg_list.append(dg)\n            st_list.append(step)\n        \n        if debug:\n            if (istep % debug_step == 0):\n                print (f\"Iteration {istep}. Step size {step:.3f}. Duality Gap {dg:g}\")\n                \n        # Stopping criteria\n        # duality gap\n        if np.abs(dg) &lt;= tol:\n            break\n        # step size\n        if step &gt; 0 and step &lt;= step_tol:\n            break\n        # relative tolerance of objective function\n        f_rel = np.abs((f_obj - fx_list[-2]) / f_obj)\n        if f_rel &lt;= rel_tol:\n            break\n            \n        old_X = X.copy()\n        \n    if return_all:\n        return X, dg_list, fx_list, st_list\n    else:\n        return X\n\n\n\n\nClass for NNMWF\n\n\nCode\nfrom sklearn.utils.extmath import randomized_svd\n\nclass TopCompSVD():\n    \n    def __init__(self, method = 'power', max_iter = 10):\n        self._method = method\n        self._max_iter = max_iter\n        return\n        \n    def fit(self, X):\n        if self._method == 'power':\n            self._u1, self._v1h = self.fit_power(X)\n        elif self._method == 'randomized':\n            self._u1, self._v1h = self.fit_randomized(X)\n        return\n    \n    def fit_power(self, X):\n        n, p = X.shape\n        u = np.random.normal(0, 1, n)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)\n        for _ in range(self._max_iter):      \n            u = X.dot(v)\n            u /= np.linalg.norm(u)\n            v = X.T.dot(u)\n            v /= np.linalg.norm(v)       \n        return u.reshape(-1, 1), v.reshape(1, -1)\n    \n    def fit_randomized(self, X):\n        u, s, vh = sp.randomized_svd(X, \n                        n_components = 1, n_iter = self._max_iter,\n                        power_iteration_normalizer = 'none',\n                        random_state = 0)\n        return u, vh\n    \n    @property\n    def u1(self):\n        return self._u1\n    \n    @property\n    def v1_t(self):\n        return self._v1h\n        \n\nclass NNMFW():\n    \n    def __init__(self, max_iter = 1000,\n            svd_method = 'power', svd_max_iter = None,\n            stop_criteria = ['duality_gap', 'step_size', 'relative_objective'],\n            tol = 1e-3, step_tol = 1e-3, rel_tol = 1e-8, \n            show_progress = False, print_skip = None,\n            debug = True):\n        self._max_iter = max_iter\n        self._svd_method = svd_method\n        self._svd_max_iter = svd_max_iter\n        self._stop_criteria = stop_criteria\n        self._tol = tol\n        self._step_size_tol = step_tol\n        self._fxrel_tol = rel_tol\n        self._show_progress = show_progress\n        self._prog_step_skip = print_skip\n        if self._show_progress and self._prog_step_skip is None:\n            self._prog_step_skip = max(1, int(self._max_iter / 100)) * 10\n        self._debug = debug\n        return\n\n    \n    def f_objective(self, X):\n        '''\n        Objective function\n        Y is observed, X is estimated\n        W is the weight of each observation.\n        '''\n        Xmask = self.get_masked(X)    \n        # The * operator can be used as a shorthand for np.multiply on ndarrays.\n        if self._weight_mask is None:\n            fx = 0.5 * np.linalg.norm(self._Y - Xmask, 'fro')**2\n        else:\n            fx = 0.5 * np.linalg.norm(self._weight_mask * (self._Y - Xmask), 'fro')**2\n        return fx\n\n\n    def f_gradient(self, X):\n        '''\n        Gradient of the objective function.\n        '''\n        Xmask = self.get_masked(X)       \n        if self._weight_mask is None:\n            gx = Xmask - self._Y\n        else:\n            gx = np.square(self._weight_mask) * (Xmask - self._Y)    \n        return gx\n    \n    \n    def fw_step_size(self, dg, D):\n        if self._weight_mask is None:\n            denom = np.linalg.norm(D, 'fro')**2\n        else:\n            denom = np.linalg.norm(self._weight_mask * D, 'fro')**2\n        ss = dg / denom\n        ss = min(ss, 1.0)\n        if ss &lt; 0:\n            print(\"Step Size is less than 0. Using last valid step size.\")\n            ss = self._st_list[-1]\n        return ss\n\n    \n    def get_masked(self, X):\n        if self._mask is None or X is None:\n            return X\n        else:\n            return X * self._mask\n        \n        \n    def linopt_oracle(self, X):\n        '''\n        Linear optimization oracle,\n        where the feasible region is a nuclear norm ball for some r\n        '''\n        U1, V1_T = self.get_singular_vectors(X)\n        S = - self._rank * U1 @ V1_T\n        return S\n    \n    \n    def get_singular_vectors(self, X):\n        max_iter = self._svd_max_iter\n        if max_iter is None:\n            nstep = len(self._st_list) + 1\n            max_iter = 10 + int(nstep / 20)\n            max_iter = min(max_iter, 25)\n        svd = TopCompSVD(method = self._svd_method, max_iter = max_iter)\n        svd.fit(X)\n        return svd.u1, svd.v1_t\n\n        \n    def fit(self, Y, r, weight = None, mask = None, X0 = None):\n        \n        '''\n        Wrapper function for the minimization\n        '''\n        \n        n, p = Y.shape\n        \n        # Make some variables available for the class\n        self._weight = weight\n        self._mask = mask\n        self._weight_mask = self.get_masked(self._weight)\n        self._Y = Y\n        self._rank = r\n        \n        # Step 0\n        X = np.zeros_like(Y) if X0 is None else X0.copy()\n        dg = np.inf\n        step = 1.0\n        fx = self.f_objective(X)\n        \n        # Save relevant variables in list\n        self._dg_list = [dg]\n        self._fx_list = [fx]\n        self._st_list = [step]\n\n        # Steps 1, ..., max_iter\n        for i in range(self._max_iter):\n            \n            X, G, dg, step = self.fw_one_step(X)\n            fx = self.f_objective(X)\n            \n            self._fx_list.append(fx)\n            self._dg_list.append(dg)\n            self._st_list.append(step)\n\n            if self._show_progress:\n                if (i % self._prog_step_skip == 0):\n                    print (f\"Iteration {i}. Step size {step:.3f}. Duality Gap {dg:g}\")\n                    \n            if self.do_stop():\n                break\n                \n        self._X = X\n\n        return\n    \n    def fw_one_step(self, X):\n    \n        # 1. Gradient for X_(t-1)\n        G = self.f_gradient(X)\n        \n        # 2. Linear optimization subproblem\n        S = self.linopt_oracle(G)\n        \n        # 3. Define D\n        D = X - S\n        \n        # 4. Duality gap\n        dg = np.trace(D.T @ G)\n        \n        # 5. Step size\n        step = self.fw_step_size(dg, D)\n        \n        # 6. Update\n        Xnew = X - step * D\n        return Xnew, G, dg, step\n    \n    def do_stop(self):\n        # self._stop_criteria = ['duality_gap', 'step_size', 'relative_objective']\n        #\n        if 'duality_gap' in self._stop_criteria:\n            dg = self._dg_list[-1]\n            if np.abs(dg) &lt;= self._tol:\n                return True\n        #\n        if 'step_size' in self._stop_criteria:\n            ss = self._st_list[-1]\n            if ss &lt;= self._step_size_tol:\n                return True\n        #\n        if 'relative_objective' in self._stop_criteria:\n            fx = self._fx_list[-1]\n            fx0 = self._fx_list[-2]\n            fx_rel = np.abs((fx - fx0) / fx0)\n            if fx_rel &lt;= self._fxrel_tol:\n                return True\n        #\n        return False\n\n\n\n\nTry the functions\n\n\nCode\nX_opt, dg_list, fx_list, step_list = frank_wolfe_minimize(Y_cent, 40.0, max_iter = 1000, debug = True, debug_step = 100, step_tol = 1e-4, svd_iter=100)\n\n\nIteration 0. Step size 1.000. Duality Gap 2528.37\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nkp = len(step_list)\nfx_arr = np.array(fx_list)\nfx_rel_diff_log10 = np.log10(np.abs(np.diff(fx_arr) / fx_arr[1:]))\n\nax1.plot(np.arange(kp - 2), dg_list[2:kp])\n# ax2.plot(np.arange(kp - 1), np.log10(fx_list[1:kp]))\nax2.plot(np.arange(kp - 1), fx_rel_diff_log10)\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\nTry the Python Class\n\n\nCode\nnnm = NNMFW(show_progress = True, svd_max_iter = 50)\nnnm.fit(Y_cent, 40.0)\n\n\nIteration 0. Step size 1.000. Duality Gap 2528.37\nStep Size is less than 0. Using last valid step size.\n\n\n\n\nCode\nnnm._st_list\n\n\n[1.0,\n 1.0,\n 0.4059508526803153,\n 0.19713776422660786,\n 0.031601077477726426,\n 0.011408112527596072,\n 0.011435454280280217,\n 0.011435454280280217,\n 0.0003653454546090821]\n\n\n\n\nCode\nnnm._fx_list\n\n\n[68953.79597625589,\n 67225.42267046764,\n 66961.74891880063,\n 66914.56295271001,\n 66914.19909538413,\n 66914.12717521715,\n 66914.04739067073,\n 66914.16163671855,\n 66914.16150434242]\n\n\n\n\nCode\nfx_list\n\n\n[68953.79597625589,\n 67225.42267046703,\n 66961.74891743576,\n 66914.56295259335,\n 66914.22530475237,\n 66914.11898081755,\n 66914.28171754857,\n 66914.21086735667,\n 66914.20897738854,\n 66914.1850500866,\n 66914.17070820235,\n 66914.16008819439,\n 66914.13124317658,\n 66914.18277041374,\n 66914.15440484512,\n 66914.13133305841,\n 66914.12544153685,\n 66914.12542199122]\n\n\n\n\nCode\nU, S, Vt = np.linalg.svd(mpy_simulate.do_standardize(X_opt, scale = False), full_matrices=False)\nU1, S1, Vt1 = np.linalg.svd(mpy_simulate.do_standardize(nnm._X, scale = False), full_matrices=False)\n\nfig = plt.figure(figsize = (18, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, U @ np.diag(S), vmax = 0.005)\nmpy_plotfn.plot_covariance_heatmap(ax2, U1 @ np.diag(S1), vmax = 0.005)\n\nplt.tight_layout(w_pad=2.0)\nplt.show()\n\n\n\n\n\n\n\nCode\nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\n\n\n\n\nCode\ndef psnr(original, recovered):\n    n, p = original.shape\n    maxsig2 = np.square(np.max(original) - np.min(original))\n    mse = np.sum(np.square(recovered - original)) / (n * p)\n    res = 10 * np.log10(maxsig2 / mse)\n    return res\n\n\n\n\nCode\npsnr(Y_true_cent, X_opt)\n\n\n21.65905757205794\n\n\n\n\nCode\npsnr(Y_true_cent, nnm._X)\n\n\n21.664554727300995\n\n\n\n\nCode\npsnr(Y_true_cent, Y_cent)\n\n\n9.943220233648011\n\n\n\n\nCode\nY_true_mean = np.mean(Y_true, axis = 0)\nY_obs_mean = np.mean(Y, axis = 0)\n\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\nax1.scatter(Y_true_mean, Y_obs_mean)\nplt.show()"
  },
  {
    "objectID": "notebooks/develop/index.html",
    "href": "notebooks/develop/index.html",
    "title": "Develop",
    "section": "",
    "text": "Develop ideas into software.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n23-07-29\n\n\nPython implementation of Frank-Wolfe algorithm for NNM with sparse penalty\n\n\n\n\n23-07-24\n\n\nPython Class for cross-validation of NNM-FW\n\n\n\n\n23-07-19\n\n\nPython Class for NNM using Frank-Wolfe algorithm\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/explore/2023-06-05-broad-trait-clusters.html",
    "href": "notebooks/explore/2023-06-05-broad-trait-clusters.html",
    "title": "Can the low rank approximation capture the distinct GWAS phenotypes?",
    "section": "",
    "text": "About\nHere, I try to qualitatively compare the different dimensionality reduction methods in terms of their ability to distinguish the different traits. Suppose \\mathbf{X} is the N \\times P input matrix, with N traits and P associated variants. The dimensionality reduction methods decompose the input matrix into a sparse low rank component, \\mathbf{L} and a background \\mathbf{E}, \n\\mathbf{Y} \\sim \\mathbf{X} + \\mathbf{E}\n We perform PCA on the low rank matrix \\mathbf{X} using the SVD, \n\\mathbf{X} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\intercal}\n Then, the principal components are given by the columns of \\mathbf{U}\\mathbf{S}. The traits are broadly classified into NPD phenotypes. For each “broad phenotype” T and principal component k, we define the trait-wise PC score as, \nV_{tk} = \\sum_{t \\in T}(U_{tk}S_k)^2\n Note, the total variance explained by the component is \\sum_{i}(U_{ik}S_k)^2 = S_k^2.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nfrom nnwmf.functions.frankwolfe import frank_wolfe_minimize, frank_wolfe_cv_minimize\nfrom nnwmf.functions.robustpca import RobustPCA\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nData\nSummary statistics data for NPD is collected from PGC, OpenGWAS and GTEx. See previous work for data cleaning and filtering. Our input is the Z-Score matrix for N diseases and P variants.\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\nselect_ids = beta_df.columns\n\nX = np.array(zscore_df.replace(np.nan, 0)[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\nWe perform PCA (using SVD) on the raw input data (mean centered). In Figure 1, we look at the proportion of variance explained by each principal component.\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices = False)\nS2 = np.square(S)\npcomp = U @ np.diag(S)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(np.arange(S.shape[0]), np.cumsum(S2 / np.sum(S2)), 'o-')\nplt.show()\n\n\n\n\n\nFigure 1: Proportion of variance explained by the principal components of the input matrix\n\n\n\n\n\n\nTrait-wise PC score\nWe break down the total variance V_1 explained by the first principal component for each trait. In Figure 2, we show $V_{t1} / V_1, and note that the first component explains the variance in SZ and BD.\n\n\nCode\nfig = plt.figure(figsize = (12, 6))\nax1 = fig.add_subplot(111)\nnsample = pcomp.shape[0]\nntrait  = len(unique_labels)\n\n\n\npcidx = 0\ntot_variance  = np.square(S[pcidx])\n\ntrait_indices = [np.array([i for i, x in enumerate(labels) if x == label]) for label in unique_labels]\ntrait_pcomps  = [np.square(pcomp[idx, pcidx]) / tot_variance for idx in trait_indices]\ntrait_colors  = {trait: color for trait, color in zip(unique_labels, (mpl_stylesheet.kelly_colors())[:ntrait])}\n\ndef rand_jitter(n, d = 0.1):\n    return np.random.randn(n) * d\n\nfor ilbl, label in enumerate(unique_labels):\n    xtrait = trait_pcomps[ilbl]\n    nsample = xtrait.shape[0]\n    \n    boxcolor = trait_colors[label]\n    boxface = f'#{boxcolor[1:]}80' #https://stackoverflow.com/questions/15852122/hex-transparency-in-colors\n    medianprops = dict(linewidth=0, color = boxcolor)\n    whiskerprops = dict(linewidth=2, color = boxcolor)\n    boxprops = dict(linewidth=2, color = boxcolor, facecolor = boxface)\n    flierprops = dict(marker='o', markerfacecolor=boxface, markersize=3, markeredgecolor = boxcolor)\n    \n    ax1.boxplot(xtrait, positions = [ilbl],\n                showcaps = False, showfliers = False,\n                widths = 0.7, patch_artist = True, notch = False,\n                flierprops = flierprops, boxprops = boxprops,\n                medianprops = medianprops, whiskerprops = whiskerprops)\n    \n    ax1.scatter(ilbl + rand_jitter(nsample), xtrait, edgecolor = boxcolor, facecolor = boxface, linewidths = 1)\n\n\nax1.axhline(y = 0, ls = 'dotted', color = 'grey')\nax1.set_xticks(np.arange(len(unique_labels)))\nax1.set_xticklabels(unique_labels, rotation = 90)\nax1.set_ylabel(f\"PC{pcidx + 1:d}\")\n\nplt.show()\n\n\n\n\n\nFigure 2: Trait-wise PVE by the first principal component of the input matrix\n\n\n\n\n\n\nCode\ndef plot_traitwise_pc_scores(ax, U, S, unique_labels, trait_colors, min_idx = 0, max_idx = 20, alpha = 0.6,\n                             use_proportion = True):\n    trait_pcomps_all = dict()\n    pcindices = np.arange(min_idx, max_idx)\n    pcomp = U @ np.diag(S)\n\n    for pcidx in pcindices:\n        tot_variance = np.square(S[pcidx])\n        if use_proportion:\n            trait_pcomps_all[pcidx] = [np.square(pcomp[idx, pcidx]) / tot_variance for idx in trait_indices]\n        else:\n            trait_pcomps_all[pcidx] = [np.square(pcomp[idx, pcidx]) for idx in trait_indices]\n    \n    comp_weights = {\n        trait: [np.sum(trait_pcomps_all[pcidx][ilbl]) for pcidx in pcindices] for ilbl, trait in enumerate(unique_labels)\n    }\n\n    bar_width = 1.0\n    bottom = np.zeros(len(pcindices))\n\n    for trait, comp_weight in comp_weights.items():\n        ax.bar(pcindices, comp_weight, bar_width, label=trait, bottom=bottom, color = trait_colors[trait], alpha = alpha)\n        bottom += comp_weight\n\n    ax.set_xticks(pcindices)\n    ax.set_xticklabels([f\"{i + 1}\" for i in pcindices])\n\n    for side, border in ax.spines.items():\n        border.set_visible(False)\n\n    ax.tick_params(bottom = True, top = False, left = False, right = False,\n                   labelbottom = True, labeltop = False, labelleft = False, labelright = False)\n    \n    return\n\n\nWe can do the same for each principal component and show the trait-wise PC-score for each component, as shown in the top panel of Figure 3. In such representation, we lose the information of the total variance by the component, but it gives an idea of the ability of the component to distinguish different phenotypes. In the bottom panel, we retain the information of the total variance explained (by avoiding the scaling to 1.0) but it is difficult to see the utility of the components with lower variance.\n\n\nCode\nfig = plt.figure(figsize = (12, 10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\nplot_traitwise_pc_scores(ax1, U, S, unique_labels, trait_colors, max_idx = 25)\nplot_traitwise_pc_scores(ax2, U, S, unique_labels, trait_colors, max_idx = 25, use_proportion = False)\n\nax2.set_xlabel(\"Principal Components\")\nax1.set_ylabel(\"Trait-wise scores for each PC (scaled)\")\nax2.set_ylabel(\"Trait-wise scores for each PC (scaled)\")\nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\nFigure 3: Trait-wise PVE by the first principal component of the input matrix\n\n\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\nsvd_pc1 = pcomp[:, idx1]\nsvd_pc2 = pcomp[:, idx2]\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nplt.show()\n\n\n\n\n\nFigure 4: Comparison of the first two principal components of input matrix\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 50, alpha = 0.7, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pcomp)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 5: Comparison of the first 6 principal components of the input matrix\n\n\n\n\n\n\nExperiment\nHere, we perform the low-rank approximation using two methods: - Robust PCA - Nuclear Norm Matrix Factorization using Frank-Wolfe Algorithm\n\n\nCode\nrpca = RobustPCA(lmb=0.0095, max_iter=1000)\nL_rpca, M_rpca = rpca.fit(Xcent)\nnp.linalg.matrix_rank(L_rpca)\n\n\n24\n\n\n\n\nCode\nr_opt = 4096.\nL_cvopt, _, _ = frank_wolfe_minimize(Xcent, np.ones(Xcent.shape), r_opt)\nM_cvopt = Xcent - L_cvopt\n\n\n\n\nCode\nX_rpca = L_rpca + M_rpca\n\nL_rpca_cent = L_rpca - np.mean(L_rpca, axis = 0, keepdims = True)\nM_rpca_cent = M_rpca - np.mean(M_rpca, axis = 0, keepdims = True)\nX_rpca_cent = X_rpca - np.mean(X_rpca, axis = 0, keepdims = True)\n\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(L_rpca_cent, full_matrices = False)\n\nL_cvopt_cent = L_cvopt - np.mean(L_cvopt, axis = 0, keepdims = True)\nM_cvopt_cent = M_cvopt - np.mean(M_cvopt, axis = 0, keepdims = True)\n\nU_cvopt, S_cvopt, Vt_cvopt = np.linalg.svd(L_cvopt_cent, full_matrices = False)\n\npc_cvopt = U_cvopt @ np.diag(S_cvopt)\npc_rpca  = U_rpca  @ np.diag(S_rpca)\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 50, alpha = 0.7, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pc_cvopt)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 6: Comparison of the first 6 principal components of the low rank approximation\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pc_rpca)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 7: Comparison of the first 6 principal components of the Robust PCA low rank\n\n\n\n\nIn Figure 8, we compare the trait-wise PC scores obtained from the two methods. Note that, I am not showing the first component because the first component is very similar for the input data and the low-rank approximations. Neglecting the first component also allows me to look at the unscaled component-wise PC scores without losing too much information about the components with lower eigenvalues.\n\n\nCode\nfig = plt.figure(figsize = (12, 10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\n\nplot_traitwise_pc_scores(ax1, U_cvopt, S_cvopt, unique_labels, trait_colors, min_idx = 0, max_idx = 9, use_proportion = True)\n#plot_traitwise_pc_scores(ax2, U_cvopt2, S_cvopt2, unique_labels, trait_colors, max_idx = 10, use_proportion = False)\nplot_traitwise_pc_scores(ax2, U_rpca, S_rpca, unique_labels, trait_colors, min_idx = 0, max_idx = 9, use_proportion = True)\n\nax1.set_title(\"NNWMF\")\nax2.set_title(\"Robust PCA\")\n\nax2.set_xlabel(\"Principal Components\")\nax1.set_ylabel(\"Trait-wise PC score\")\nax2.set_ylabel(\"Trait-wise PC score\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\nFigure 8: Trait-wise PC scores for the low rank approximation of the input matric\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\n\nplot_traitwise_pc_scores(ax1, U_cvopt, S_cvopt, unique_labels, trait_colors, min_idx = 1, max_idx = 9, use_proportion = False)\n#plot_traitwise_pc_scores(ax2, U_cvopt2, S_cvopt2, unique_labels, trait_colors, max_idx = 10, use_proportion = False)\nplot_traitwise_pc_scores(ax2, U_rpca, S_rpca, unique_labels, trait_colors, min_idx = 1, max_idx = 9, use_proportion = False)\n\nax1.set_title(\"NNWMF\")\nax2.set_title(\"Robust PCA\")\n\nax2.set_xlabel(\"Principal Components\")\nax1.set_ylabel(\"Trait-wise PC score\")\nax2.set_ylabel(\"Trait-wise PC score\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\nFigure 9: Trait-wise PC scores for the low rank approximation of the input matric\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(Xcent)\n\n\n68\n\n\n\n\nCode\nnp.linalg.matrix_rank(L_rpca)\n\n\n18\n\n\n\n\nCode\nnp.linalg.matrix_rank(L_cvopt)\n\n\n66"
  },
  {
    "objectID": "notebooks/explore/2023-07-01-how-to-simulate.html",
    "href": "notebooks/explore/2023-07-01-how-to-simulate.html",
    "title": "How to simulate ground truth for multi-phenotype z-scores?",
    "section": "",
    "text": "About\nHere, I try to develop a generative model to create a feature matrix with realistic ground truth in order to benchmark different methods.\nMatrix factorization methods represent an observed N \\times P data matrix \\mathbf{Y} as:\n\n\\mathbf{Y} = \\mathbf{L}\\mathbf{F}^{\\intercal} + \\mathbf{E}\n\nwhere \\mathbf{L} is an N \\times K matrix, \\mathbf{F} is a P \\times K matrix, and \\mathbf{E} is an N \\times P matrix of residuals. For consistency, we adopt the notation and terminology of factor analysis, and refer to \\mathbf{L} as the loadings and \\mathbf{F} as the factors.\nDifferent matrix factorization methods assume different constraints on \\mathbf{L} and \\mathbf{F}. For example, PCA assumes that the columns of \\mathbf{L} are orthogonal and the columns of \\mathbf{F} are orthonormal. For the purpose of generating a ground truth, we will use a generative model,\n\n\\mathbf{Y} = \\mathbf{M} + \\mathbf{L}\\mathbf{F}^{\\intercal} + \\mathbf{E}\\,,\n where every row of \\mathbf{M} is equal to the mean value for each feature. The noise \\mathbf{E} is sampled from \\mathcal{N}(0, \\sigma^2 \\mathbf{I}). Equivalently, for sample (phenotype) i and feature (SNP) j, \ny_{ij} = m_j + \\sum_{k = 1}^K l_{ik}f_{jk} + e_j\n\nWe will make some realistic assumptions of \\mathbf{M}, \\mathbf{L}, \\mathbf{F} and \\mathbf{E}, as discussed below.\n\n\nGetting setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps as mpl_cmaps\nimport matplotlib.colors as mpl_colors\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\n\n\n\n\nInput Dimensions\n\nntrait = 4 # categories / class\nngwas  = 100 # N\nnsnp   = 1000 # P\nnfctr  = 20 # K\n\n\n\nGenerate latent factors\nThe matrix \\mathbf{F} of latent factors, size P \\times K. Properties: - Semi-orthogonal matrix - Columns are K orthonormal vectors (all orthogonal to each other: ortho; all of unit length: **normal”)\n\n\nCode\nfrom scipy import stats as spstats\nF = spstats.ortho_group.rvs(nsnp)[:, :nfctr]\n\n# Quickly check if vectors are orthonormal\nnp.testing.assert_array_almost_equal(F.T @ F, np.identity(nfctr))\n\n\n\n\nCode\ndef do_standardize(Z, axis = 0, center = True, scale = True):\n    '''\n    Standardize (divide by standard deviation)\n    and/or center (subtract mean) of a given numpy array Z\n    \n    axis: the direction along which the std / mean is aggregated.\n        In other words, this axis is collapsed. For example,\n        axis = 0, means the rows will aggregated (collapsed).\n        In the output, the mean will be zero and std will be 1\n        along the remaining axes.\n        For a 2D array (matrix), use axis = 0 for column standardization\n        (with mean = 0 and std = 1 along the columns, axis = 1).\n        Simularly, use axis = 1 for row standardization\n        (with mean = 0 and std = 1 along the rows, axis = 0).\n        \n    center: whether or not to subtract mean.\n    \n    scale: whether or not to divide by std.\n    '''\n    dim = Z.ndim\n    \n    if scale:\n        Znew = Z / np.std(Z, axis = axis, keepdims = True)\n    else:\n        Znew = Z.copy()\n        \n    if center:\n        Znew = Znew - np.mean(Znew, axis = axis, keepdims = True)\n\n    return Znew\n\ndef get_equicorr_feature(n, p, rho = 0.8, seed = None, standardize = True):\n    '''\n    Return a matrix X of size n x p with correlated features.\n    The matrix S = X^T X has unit diagonal entries and constant off-diagonal entries rho.\n    \n    '''\n    if seed is not None: np.random.seed(seed)\n    iidx = np.random.normal(size = (n , p))\n    comR = np.random.normal(size = (n , 1))\n    x    = comR * np.sqrt(rho) + iidx * np.sqrt(1 - rho)\n\n    # standardize if required\n    if standardize:\n        x = do_standardize(x)\n\n    return x\n\ndef get_blockdiag_features(n, p, rholist, groups, rho_bg = 0.0, seed = None, standardize = True):\n    '''\n    Return a matrix X of size n x p with correlated features.\n    The matrix S = X^T X has unit diagonal entries and \n    k blocks of matrices, whose off-diagonal entries \n    are specified by elements of `rholist`.\n    \n    rholist: list of floats, specifying the correlation within each block\n    groups: list of integer arrays, each array contains the indices of the blocks.\n    '''\n    np.testing.assert_equal(len(rholist), len(groups))\n    \n    if seed is not None: np.random.seed(seed)\n    iidx = get_equicorr_feature(n, p, rho = rho_bg)\n\n    # number of blocks\n    k = len(rholist)\n    \n    # zero initialize\n    x = iidx.copy() #np.zeros_like(iidx)\n    \n    for rho, grp in zip(rholist, groups):\n        comR = np.random.normal(size = (n, 1))\n        x[:, grp] = np.sqrt(rho) * comR + np.sqrt(1 - rho) * iidx[:, grp]\n\n    # standardize if required\n    if standardize:\n        x = do_standardize(x)\n\n    return x\n\n\ndef get_blockdiag_matrix(n, rholist, groups):\n    R = np.ones((n, n))\n\n    for i, (idx, rho) in enumerate(zip(groups, rholist)):\n        nblock = idx.shape[0]\n        xblock = np.ones((nblock, nblock)) * rho\n        R[np.ix_(idx, idx)] = xblock\n        \n    return R\n\n# def get_correlated_features (n, p, R, seed = None, standardize = True, method = 'cholesky'):\n#     '''\n#     method: Choice of method, cholesky or eigenvector or blockdiag.\n#     '''\n\n#     # Generate samples from independent normally distributed random\n#     # variables (with mean 0 and std. dev. 1).\n#     x = norm.rvs(size=(p, n))\n\n#     # We need a matrix `c` for which `c*c^T = r`.  We can use, for example,\n#     # the Cholesky decomposition, or the we can construct `c` from the\n#     # eigenvectors and eigenvalues.\n\n#     if method == 'cholesky':\n#         # Compute the Cholesky decomposition.\n#         c = cholesky(r, lower=True)\n#     else:\n#         # Compute the eigenvalues and eigenvectors.\n#         evals, evecs = eigh(r)\n#         # Construct c, so c*c^T = r.\n#         c = np.dot(evecs, np.diag(np.sqrt(evals)))\n#     # Convert the data to correlated random variables. \n#     y = np.dot(c, x)\n#     return y\n\ndef get_sample_indices(ntrait, ngwas, shuffle = True):\n    '''\n    Distribute the samples in the categories (classes)\n    '''\n    rs = 0.6 * np.random.rand(ntrait) + 0.2 # random sample from [0.2, 0.8)\n    z = np.array(np.round((rs / np.sum(rs)) * ngwas), dtype = int)\n    z[-1] = ngwas - np.sum(z[:-1])\n    tidx = np.arange(ngwas)\n    if shuffle:\n        np.random.shuffle(tidx)\n    bins = np.zeros(ntrait + 1, dtype = int)\n    bins[1:] = np.cumsum(z)\n    sdict = {i : np.sort(tidx[bins[i]:bins[i+1]]) for i in range(ntrait)}\n    return sdict\n\ndef plot_covariance_heatmap(ax, X):\n    return plot_heatmap(ax, np.cov(X))\n\ndef plot_heatmap(ax, X):\n    '''\n    Helps to plot a heatmap\n    '''\n    cmap1 = mpl_cmaps.get_cmap(\"YlOrRd\").copy()\n    cmap1.set_bad(\"w\")\n    norm1 = mpl_colors.TwoSlopeNorm(vmin=0., vcenter=0.5, vmax=1.)\n    im1 = ax.imshow(X.T, cmap = cmap1, norm = norm1, interpolation='nearest', origin = 'upper')\n\n    ax.tick_params(bottom = False, top = True, left = True, right = False,\n                    labelbottom = False, labeltop = True, labelleft = True, labelright = False)\n\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n    cbar = plt.colorbar(im1, cax=cax, fraction = 0.1)\n    return\n\ndef reduce_dimension_svd(X, nfeature = None):\n    if k is None: k = int(X.shape[1] / 10)\n    k = max(1, k)\n    U, S, Vt = np.linalg.svd(X)\n    Uk = U[:, :k]\n    Sk = S[:k]\n    return Uk @ Sk\n\n# L_qr_ortho, L_qr_eig = orthogonalize_qr(do_standardize(L_full))\n# #idsort = np.argsort(L_eig)[::-1]\n# #idselect = idsort[:nfctr]\n# idselect = np.arange(nfctr)\n# L_qr =  L_qr_ortho[:, idselect] @ np.diag(L_qr_eig[idselect])\n\n# L_svd_ortho, L_svd_eig = orthogonalize_svd(do_standardize(L_full), k = nfctr)\n# L_svd = L_svd_ortho @ np.diag(L_svd_eig)\n\n\n\n\nGenerate loadings of each factor\nThe matrix \\mathbf{L} of loadings, size N \\times K. It encapsulates the similarity and distinctness of the samples in the latent space. We design \\mathbf{L} to contain the a covariance structure similar to the realistic data. The covariance of \\mathbf{L} is shown in Figure 1\nTo-Do: Is there a way to enforce the columns of \\mathbf{L} to be orthogonal? This can be both good and bad. Good, because many matrix factorization methods like PCA, etc assume that W is orthogonal. Bad, because real data may not be orthogonal.\n\n\nCode\nsample_dict = get_sample_indices(ntrait, ngwas, shuffle = False)\nsample_indices = [x for k, x in sample_dict.items()]\nrholist = [0.7 for x in sample_indices]\nrholist = [0.9, 0.5, 0.6, 0.9]\n\n\n\n\nCode\nL_full = get_blockdiag_features(1000, ngwas, rholist, sample_indices, rho_bg = 0.0, seed = 2000).T\nL = L_full[:, :nfctr]\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nplot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 1: Covariance structure of the loadings\n\n\n\n\nThe covariance of \\mathbf{Y}_{\\mathrm{true}} = \\mathbf{L}\\mathbf{F}^{\\intercal} is shown in the left panel of ?@fig-Ytrue-visual. In the right panel, we show the covariance of \\mathbf{Y}_{\\mathrm{true}}\\mathbf{F}.\n\n\nCode\nfig = plt.figure(figsize = (16, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nY_true = L @ F.T\nY_true_proj = Y_true @ F\n\nplot_covariance_heatmap(ax1, Y_true)\nplot_covariance_heatmap(ax2, Y_true_proj)\n\nax1.set_title(\"Covariance of Y in feature space\", pad = 50)\nax2.set_title(\"Covariance of Y after projection to latent space\", pad = 50)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 2: Visualization of the true input matrix\n\n\n\n\n\nApproximate methods to obtain loadings decomposition\nEnforcing orthogonality on the loading will naturally overshrink the similarity and enhance the distinctness. Still, we can reduce the dimension of L_full using truncated SVD and use the projection of L_full on the first K components of the right singular vector.\nAnother option is to use a QR decomposition to obtain orthogonal columns, scaled by some approximate eigenvalues: 1. Transpose L_full and use QR decomposition. 2. \\mathbf{s}_{qr} \\leftarrow eigenvalues (diagonal of \\mathbf{R}). 3. Define \\mathbf{s}' such that s'_i = 1 if s_{qr} &gt; 0 and s'_i = -1 if s_{qr} &lt; 0. 4. Choose K orthogonal vectors from \\mathbf{Q} multiplied by \\mathrm{diag}(\\mathbf{s}'). This does not work numerically, as shown below in Figure 3\n\n\nCode\ndef orthogonalize_qr(X):\n    Q, R = np.linalg.qr(X)\n    eigv = np.diag(R).copy()\n    eigv[eigv &gt; 0] = 1.\n    eigv[eigv &lt; 0] = -1.\n    U = Q @ np.diag(eigv)\n    #return U, np.diag(R)\n    return Q, np.abs(np.diag(R))\n\ndef orthogonalize_svd(X, k = None):\n    if k is None:  k = X.shape[1]\n    U, S, Vt = np.linalg.svd(X)\n    Uk = U[:, :k]\n    Sk = S[:k]\n    return Uk, Sk\n\nL_qr_ortho, L_qr_eig = orthogonalize_qr(do_standardize(L_full))\n#idsort = np.argsort(L_eig)[::-1]\n#idselect = idsort[:nfctr]\nidselect = np.arange(nfctr)\nL_qr =  L_qr_ortho[:, idselect] @ np.diag(L_qr_eig[idselect])\n\nL_svd_ortho, L_svd_eig = orthogonalize_svd(do_standardize(L_full), k = nfctr)\nL_svd = L_svd_ortho @ np.diag(L_svd_eig)\n\n\nfig = plt.figure(figsize = (16, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nplot_covariance_heatmap(ax1, L_qr)\nplot_covariance_heatmap(ax2, L_svd)\n\nax1.set_title(\"Covariance of loadings after QR decomposition\", pad = 50)\nax2.set_title(\"Covariance of loadings after SVD decomposition\", pad = 50)\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 3: Approximate methods for loading decomposition\n\n\n\n\n\n\n\nGenerate Data\n\n\n\n\n\n\n\nvariable\ndata\n\n\n\n\nY_true\nGround truth\n\n\nY_true_proj\nProjection of ground truth on the latent factors\n\n\nY\nObserved data\n\n\nY_std\nObserved data, standardized to mean 0 and std 1 for each feature (column)\n\n\nY_std_proj\nProjection of Y_std on the latent factors\n\n\n\n\n\nCode\n# fixed noise for every SNP\nsigma2 = np.random.uniform(1e-2, 5.0, nsnp)\nnoise = np.random.multivariate_normal(np.zeros(nsnp), np.diag(sigma2), size = ngwas)\nmeanshift = np.random.normal(0, 10, size = (1, nsnp))\n\n# Generate data and its projections on true F\nY_true = L @ F.T\nY_true_proj = Y_true @ F\nY = Y_true + meanshift + noise\nY_std = do_standardize(Y)\nY_std_proj = Y_std @ F\n\n\nWe obtain the observed \\mathbf{Y} by adding noise and mean to \\mathbf{Y}_{\\mathrm{true}}. The covariance of \\mathbf{Y} is shown in the left panel of ?@fig-Y-visual. In the right panel, we show the covariance of \\mathbf{Y}\\mathbf{F}\n\n\nCode\nfig = plt.figure(figsize = (16, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nplot_covariance_heatmap(ax1, Y_std)\nplot_covariance_heatmap(ax2, Y_std_proj)\n\nax1.set_title(\"Covariance of Y in feature space\", pad = 50)\nax2.set_title(\"Covariance of Y after projection to latent space\", pad = 50)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 4: Visualization of the observed input matrix\n\n\n\n\n\n\nPlot distribution of each feature\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor i in range(Y.shape[1]):\n    x = Y_std[:, i]\n    outlier_mask = mpy_histogram.iqr_outlier(x, axis = 0, bar = 5)\n    data = x[~outlier_mask]\n    xmin, xmax, bins, xbin = mpy_histogram.get_bins(data, 100, None, None)\n    curve = mpy_histogram.get_density(xbin, data)\n    ax1.plot(xbin, curve)\nplt.show()\n\n\n\n\n\nFigure 5: Distribution of z-scores for all phenotypes\n\n\n\n\n\n\nCan SVD decomposition capture the signal?\nIn Figure 6, we look at the separation of the underlying features using - projection of \\mathbf{Y}_{\\mathrm{true}} on \\mathbf{F} (left panel). - projection of \\mathbf{Y} on \\mathbf{F} (center panel). - principal components obtained from SVD of \\mathbf{Y} (right panel).\n\n\nCode\nU, S, Vt = np.linalg.svd(Y_std, full_matrices=False)\npcomps_svd = U[:, :nfctr] @ np.diag(S[:nfctr])\n\n\n\n\nCode\nfig = plt.figure(figsize = (18, 6))\n\nidx1 = 1\nidx2 = 2\n\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nmcolors = mpl_stylesheet.kelly_colors()\nfor i, grp in enumerate(sample_indices):\n    ax1.scatter(Y_true_proj[grp, idx1], Y_true_proj[grp, idx2], color = mcolors[i])\n    ax2.scatter(Y_std_proj[grp, idx1], Y_std_proj[grp, idx2], color = mcolors[i])\n    ax3.scatter(pcomps_svd[grp, idx1], pcomps_svd[grp, idx2], color = mcolors[i])\n    \nax1.set_title (\"Projection of ground truth on latent factors\", pad = 20)\nax2.set_title (\"Projection of observed data on latent factors\", pad = 20)\nax3.set_title (\"Principal components of observed data\", pad = 20)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 6: Separation of components in different spaces\n\n\n\n\n\n\nCode\nYpred_svd = pcomps_svd @ Vt[:nfctr, :]\nY_true_std = do_standardize(Y_true)\n\nnp.sqrt(np.sum(np.square(Ypred_svd - Y_true) / np.square(Y_true)))\n\n\n230973.68978816015"
  },
  {
    "objectID": "notebooks/explore/2023-06-23-external-validation-metrics.html",
    "href": "notebooks/explore/2023-06-23-external-validation-metrics.html",
    "title": "Metrices for evaluating clusters given true labels",
    "section": "",
    "text": "About\nThere are many aspects of “rightness” for clustering. Broadly, there are two kinds of validity indices to measure the quality of clustering results: external indices and internal indices. An external index is a measure of agreement between two partitions where the first partition is the a priori known clustering structure, and the second results from the clustering procedure. Internal indices are used to measure the goodness of a clustering structure without external information. For external indices, we evaluate the results of a clustering algorithm based on a known cluster structure of a data set (or cluster labels). Here, we look at several possible external validation metrics.\n\n\nGetting set up\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import model_selection as skmodel\nfrom sklearn import metrics as skmetrics\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n'''\nX matrix (n_samples x n_features) -- obtain from Z-scores\n'''\nselect_ids = beta_df.columns\nX = np.array(zscore_df[select_ids]).T # contain NaN values\ncolmeans = np.nanmean(X, axis = 0, keepdims = True)\nXcent = X - colmeans # contain NaN values\nXcent = np.nan_to_num(X, nan=0) # remove NaN values\n\n'''\nY vector (n_samples) -- contain class labels\n'''\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\nencoding = {x:i for i, x in enumerate(unique_labels)}\nYlabels = np.array([encoding[x] for x in labels])\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\n\n\nSample counts of input data\n\n\nCode\nsample_counts = {label : (Ylabels == idx).sum() for label, idx in encoding.items()}\nprint (f\"Count   Phenotype\")\nprint (f\"-----   ---------\")\nfor phenotype, count in sample_counts.items():\n    print (f\"{count}\\t{phenotype}\")\n\n\nCount   Phenotype\n-----   ---------\n7   Sleep\n3   SZ\n2   ASD\n2   Migraine\n2   ADHD\n1   OCD\n8   Depression\n2   Intel/education\n11  Epilepsy\n10  Other psych\n7   Cognition\n6   BD\n8   Neurodegenerative\n\n\n\n\nSplit into training and test data\n\n\nCode\nfrom sklearn import model_selection as skmodel\n\n'''\nOne-liner to split:\n# X_train, X_test, y_train, y_test = skmodel.train_test_split(X, Ylabels, test_size = 0.33)\nbut it does not return the index for the training and test data,\nso I use a little more verbose solution\n'''\nitrain, itest = skmodel.train_test_split(np.arange(Ylabels.shape[0]), test_size = 0.33)\nX_train = X[itrain, :]\nX_test  = X[itest, :]\ny_train = Ylabels[itrain]\ny_test  = Ylabels[itest]\n\nprint (f\"Train   Test    Phenotype\")\nprint (f\"-----   ----    ---------\")\nfor phenotype, idx in encoding.items():\n    train_count = np.sum(y_train == idx)\n    test_count  = np.sum(y_test  == idx)\n    print (f\"{train_count}\\t{test_count}\\t{phenotype}\")\n\n\nTrain   Test    Phenotype\n-----   ----    ---------\n7   0   Sleep\n2   1   SZ\n0   2   ASD\n1   1   Migraine\n1   1   ADHD\n0   1   OCD\n5   3   Depression\n1   1   Intel/education\n9   2   Epilepsy\n5   5   Other psych\n4   3   Cognition\n6   0   BD\n5   3   Neurodegenerative\n\n\n\n\nClustering from distance matrix\nWe want to cluster the samples based on the Euclidean distance between them, obtained from the feature matrix. There are hundreds of algorithms to choose from, for example: - Hierarchical clustering in it’s myriad of variants. Cut the dendrogram as desired, e.g., to get k clusters - PAM, the closest match to k-means on a distance matrix (minimizes the average distance from the cluster center) - Spectral clustering - DBSCAN - OPTICS - HDBSCAN* - Affinity Propagation\nAvailable Software in Python: - pyclustering for fast Python implementation of different algorithms. They have nice documentation and examples. - sklearn.cluster - HDBSCAN* provides a very nice documentation for comparing different algorithms (albeit a bit biased, highlighting their own strength). - scipy.cluster provides the hierarchy module which has functions for hierarchical and agglomerative clustering.\n\n\nCode\ndistance_matrix = skmetrics.pairwise.pairwise_distances(Xcent, metric='euclidean')\n\n\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\n\nmodel = AgglomerativeClustering(n_clusters = len(unique_labels), linkage = 'average', metric = 'precomputed')\nY_pred = model.fit_predict(distance_matrix)\n#km = KMeans(n_clusters = len(unique_labels), random_state = 0, n_init=\"auto\")\n#km.fit(Xcent)\n#Y_pred = km.labels_\n\n\n\n\nCode\nY_random = np.random.choice(len(unique_labels), size=Ylabels.shape[0], replace=True)\n\n\n\n\nComparison Metrics\nWe can use several external validation techniques to assess the quality or “correctness” of the clusters since we have manually assigned the cluster labels. For example, we can use adjusted rand index, adjusted mutual information, homogeneity/completeness/v-measure, Fowlkes-Mallows score.\n\nAdjusted Rand Index\n\n\nCode\nprint (f\"Random: {skmetrics.adjusted_rand_score(Ylabels, Y_random):.5f}\")\nprint (f\"Predicted: {skmetrics.adjusted_rand_score(Ylabels, Y_pred):.5f}\")\n\n\nRandom: -0.01573\nPredicted: 0.15229\n\n\n\n\nAdjusted Mutual Information\n\n\nCode\nprint (f\"Random: {skmetrics.adjusted_mutual_info_score(Ylabels, Y_random):.5f}\")\nprint (f\"Predicted: {skmetrics.adjusted_mutual_info_score(Ylabels, Y_pred):.5f}\")\n\n\nRandom: -0.02511\nPredicted: 0.36902\n\n\n\n\nHomogeneity and V-measure\nRosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment: - homogeneity: each cluster contains only members of a single class. - completeness: all members of a given class are assigned to the same cluster.\nWe turn those concept as scores homogeneity_score and completeness_score. Both are bounded below by 0.0 and above by 1.0 (higher is better). Their harmonic mean called V-measure is computed by v_measure_score.\nNote. v_measure_score is symmetric: it can be used to evaluate the agreement of two independent assignments on the same dataset. This is not the case for completeness_score and homogeneity_score: both are bound by the relationship:\nhomogeneity_score(a, b) == completeness_score(b, a)\n\n\nCode\nprint (f\"        Homogeneity \\tCompleteness \\tV-Measure\")\n\nhcv_random = skmetrics.homogeneity_completeness_v_measure(Ylabels, Y_random)\nhcv_pred   = skmetrics.homogeneity_completeness_v_measure(Ylabels, Y_pred)\n\nprint (\"Random:    \" + ' \\t'.join([f\"{x:.5f}\" for x in hcv_random]))\nprint (\"Predicted: \" + ' \\t'.join([f\"{x:.5f}\" for x in hcv_pred]))\n\n\n        Homogeneity     Completeness    V-Measure\nRandom:    0.36793  0.35036     0.35893\nPredicted: 0.47942  0.70042     0.56922\n\n\n\n\nFowlkes-Mallows scores\nFMI is defined as the geometric mean of the pairwise precision and recall.\n\n\nCode\nprint (f\"Random: {skmetrics.fowlkes_mallows_score(Ylabels, Y_random):.5f}\")\nprint (f\"Random: {skmetrics.fowlkes_mallows_score(Ylabels, Y_pred):.5f}\")\n\n\nRandom: 0.07035\nRandom: 0.34056\n\n\n\n\n\nDoes distance matrix from truncated SVD improve score?\n\n\nCode\nK = 20\n\nU, S, Vt = np.linalg.svd(Xcent, full_matrices=False)\npcomp_tsvd = U[:, :K] @ np.diag(S[:K])\n\ndistance_matrix_tsvd = skmetrics.pairwise.pairwise_distances(pcomp_tsvd, metric='euclidean')\n\nmodel = AgglomerativeClustering(n_clusters = len(unique_labels), linkage = 'average', metric = 'precomputed')\nY_pred_tsvd = model.fit_predict(distance_matrix_tsvd)\n\nskmetrics.adjusted_mutual_info_score(Ylabels, Y_pred_tsvd)\n\n\n0.3878603292380521\n\n\n\n\nFurther Reading\n\nScikit: Clustering Performance Evaluation\nHow to compare a clustering algorithm partition to a “ground truth”?"
  },
  {
    "objectID": "notebooks/explore/2023-05-16-robustpca.html",
    "href": "notebooks/explore/2023-05-16-robustpca.html",
    "title": "Robust PCA implementation",
    "section": "",
    "text": "About\nOur input matrix has missing data. Here, I try to use Robust PCA by Candes et. al., 2011 to obtain a low rank matrix \\mathbf{L} by stripping out some noise \\mathbf{M} and then apply PCA on \\mathbf{L} \n\\mathbf{X} = \\mathbf{L} + \\mathbf{M}\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nfrom sklearn.decomposition import PCA\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename = f\"{data_dir}/prec_df.pkl\"\nbeta_df = pd.read_pickle(beta_df_filename)\nprec_df = pd.read_pickle(prec_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n\nCode\nmean_se = prec_df.apply(lambda x : 1 / np.sqrt(x)).replace([np.inf, -np.inf], np.nan).mean(axis = 0, skipna = True)\nmean_se = pd.DataFrame(mean_se).set_axis([\"mean_se\"], axis = 1)\nbeta_std = beta_df.std(axis = 0, skipna = True)\nbeta_std = pd.DataFrame(beta_std).set_axis([\"beta_std\"], axis = 1)\nerror_df = pd.concat([mean_se, beta_std], axis = 1)\n\nselect_ids = error_df.query(\"mean_se &lt;= 0.2 and beta_std &lt;= 0.2\").index\nse_df = prec_df.apply(lambda x : 1 / np.sqrt(x)).replace([np.inf, -np.inf], np.nan)\n\nzscore_df = beta_df / se_df\nzscore_df = zscore_df.replace(np.nan, 0)\nX = np.array(zscore_df[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"After filtering, we have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\n\nAfter filtering, we have 69 samples (phenotypes) and 8403 features (variants)\n\n\n\n\nRobust PCA\n\n\nCode\ndef soft_thresholding(y: np.ndarray, mu: float):\n    \"\"\"\n    Soft thresholding operator as explained in Section 6.5.2 of https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf\n    Solves the following problem:\n    argmin_x (1/2)*||x-y||_F^2 + lmb*||x||_1\n\n    Parameters\n    ----------\n        y : np.ndarray\n            Target vector/matrix\n        lmb : float\n            Penalty parameter\n    Returns\n    -------\n        x : np.ndarray\n            argmin solution\n    \"\"\"\n    return np.sign(y) * np.clip(np.abs(y) - mu, a_min=0, a_max=None)\n\ndef svd_shrinkage(y: np.ndarray, tau: float):\n    \"\"\"\n    SVD shrinakge operator as explained in Theorem 2.1 of https://statweb.stanford.edu/~candes/papers/SVT.pdf\n    Solves the following problem:\n    argmin_x (1/2)*||x-y||_F^2 + tau*||x||_*\n    \n    Parameters\n    ----------\n        y : np.ndarray\n            Target vector/matrix\n        tau : float\n            Penalty parameter\n    Returns\n    -------\n        x : np.ndarray\n            argmin solution\n    \n    \"\"\"\n    U, s, Vh = np.linalg.svd(y, full_matrices=False)\n    s_t = soft_thresholding(s, tau)\n    return U.dot(np.diag(s_t)).dot(Vh)\n\nclass RobustPCA:\n    \"\"\"\n    Solves robust PCA using Inexact ALM as explained in Algorithm 5 of https://arxiv.org/pdf/1009.5055.pdf\n    Parameters\n    ----------\n        lmb: \n            penalty on sparse errors\n        mu_0: \n            initial lagrangian penalty\n        rho: \n            learning rate\n        tau:\n            mu update criterion parameter\n        max_iter:\n            max number of iterations for the algorithm to run\n        tol_rel:\n            relative tolerance\n        \n    \"\"\"\n    def __init__(self, lmb: float, mu_0: float=1e-5, rho: float=2, tau: float=10, \n                 max_iter: int=10, tol_rel: float=1e-3):\n        assert mu_0 &gt; 0\n        assert lmb &gt; 0\n        assert rho &gt; 1\n        assert tau &gt; 1\n        assert max_iter &gt; 0\n        assert tol_rel &gt; 0\n        self.mu_0_ = mu_0\n        self.lmb_ = lmb\n        self.rho_ = rho\n        self.tau_ = tau\n        self.max_iter_ = max_iter\n        self.tol_rel_ = tol_rel\n        \n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Fits robust PCA to X and returns the low-rank and sparse components\n        Parameters\n        ----------\n            X:\n                Original data matrix\n\n        Returns\n        -------\n            L:\n                Low rank component of X\n            S:\n                Sparse error component of X\n        \"\"\"\n        assert X.ndim == 2\n        mu = self.mu_0_\n        Y = X / self._J(X, mu)\n        S = np.zeros_like(X)\n        S_last = np.empty_like(S)\n        for k in range(self.max_iter_):\n            # Solve argmin_L ||X - (L + S) + Y/mu||_F^2 + (lmb/mu)*||L||_*\n            L = svd_shrinkage(X - S + Y/mu, 1/mu)\n            \n            # Solve argmin_S ||X - (L + S) + Y/mu||_F^2 + (lmb/mu)*||S||_1\n            S_last = S.copy()\n            S = soft_thresholding(X - L + Y/mu, self.lmb_/mu)\n            \n            # Update dual variables Y &lt;- Y + mu * (X - S - L)\n            Y += mu*(X - S - L)\n            r, h = self._get_residuals(X, S, L, S_last, mu)\n            \n            # Check stopping cirteria\n            tol_r, tol_h = self._update_tols(X, L, S, Y)\n            if r &lt; tol_r and h &lt; tol_h:\n                break\n                \n            # Update mu\n            mu = self._update_mu(mu, r, h)\n            \n        return L, S\n            \n    def _J(self, X: np.ndarray, lmb: float):\n        \"\"\"\n        The function J() required for initialization of dual variables as advised in Section 3.1 of \n        https://people.eecs.berkeley.edu/~yima/matrix-rank/Files/rpca_algorithms.pdf            \n        \"\"\"\n        return max(np.linalg.norm(X), np.max(np.abs(X))/lmb)\n    \n    @staticmethod\n    def _get_residuals(X: np.ndarray, S: np.ndarray, L: np.ndarray, S_last: np.ndarray, mu: float):\n        primal_residual = np.linalg.norm(X - S - L, ord=\"fro\")\n        dual_residual = mu * np.linalg.norm(S - S_last, ord=\"fro\")\n        return primal_residual, dual_residual\n    \n    def _update_mu(self, mu: float, r: float, h: float):\n        if r &gt; self.tau_ * h:\n            return mu * self.rho_\n        elif h &gt; self.tau_ * r:\n            return mu / self.rho_\n        else:\n            return mu\n        \n    def _update_tols(self, X, S, L, Y):\n        tol_primal = self.tol_rel_ * max(np.linalg.norm(X), np.linalg.norm(S), np.linalg.norm(L))\n        tol_dual = self.tol_rel_ * np.linalg.norm(Y)\n        return tol_primal, tol_dual\n\n\n\n\nCode\nrpca = RobustPCA(lmb=0.0085, max_iter=1000)\nL, M = rpca.fit(Xcent)\n\n\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices=False)\npca_proj = U @ np.diag(S)\n\n\n\n\nCode\nLcent = L - np.mean(L, axis = 0, keepdims = True)\nMcent = M - np.mean(M, axis = 0, keepdims = True)\n\n\n\n\nCode\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Lcent, full_matrices = False)\nrpca_proj = U_rpca @ np.diag(S_rpca)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\n\nsvd_pc1 = pca_proj[:, idx1]\nsvd_pc2 = pca_proj[:, idx2]\nrpca_pc1 = rpca_proj[:, idx1]\nrpca_pc2 = rpca_proj[:, idx2]\n\nfig = plt.figure(figsize = (16, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    ax2.scatter(rpca_pc1[idx], rpca_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \nax2.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nax2.set_xlabel(f\"Component {idx1}\")\nax2.set_ylabel(f\"Component {idx2}\")\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.8, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, rpca_proj)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nnp.linalg.matrix_rank(L)\n\n14\n\n\n\nnp.linalg.matrix_rank(M)\n\n68"
  },
  {
    "objectID": "notebooks/explore/comparison-frankwolfe-models-nnm-sparse.html",
    "href": "notebooks/explore/comparison-frankwolfe-models-nnm-sparse.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import FrankWolfe\n\n\n\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\n\n\n\n\nCode\nunique_labels  = list(range(len(sample_indices)))\nclass_labels = [None for x in range(ngwas)]\nfor k, idxs in enumerate(sample_indices):\n    for i in idxs:\n        class_labels[i] = k\n\n\n\n\nCode\nnnm = FrankWolfe(show_progress = True, debug = True, benchmark = True, svd_max_iter = 50, model = 'nnm')\nnnm.fit(Y_cent, 64.0, Ytrue = Y_cent)\n\n\n2023-08-01 18:22:22,410 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 4483.21\n2023-08-01 18:22:22,637 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-01 18:22:22,741 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-01 18:22:22,764 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-01 18:22:22,828 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-01 18:22:22,851 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-01 18:22:22,873 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n\n\n\n\nCode\nnnm_sparse = FrankWolfe(show_progress = True, debug = True, benchmark = True, svd_max_iter = 50, model = 'nnm-sparse')\nnnm_sparse.fit(Y_cent, (64.0, 20.0), Ytrue = Y_cent)\n\n\n2023-08-01 18:22:15,116 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 4606.81\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(np.arange(1, len(nnm.steps)), nnm.cpu_time_[1:], 'o-', label = 'NNM')\nax1.plot(np.arange(1, len(nnm_sparse.steps)), nnm_sparse.cpu_time_[1:], 'o-', label = 'NNM-Sparse')\nax1.legend()\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"CPU Time for iteration\")\n\nax2.plot(np.cumsum(nnm.cpu_time_), nnm.rmse_, 'o-', label = 'NNM')\nax2.plot(np.cumsum(nnm_sparse.cpu_time_), nnm_sparse.rmse_, 'o-', label = 'NNM-Sparse')\nax2.legend()\nax2.set_xlabel(\"CPU Time\")\nax2.set_ylabel(\"RMSE\")\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\nCode\nnp.cumsum(nnm.cpu_time_)\n\n\narray([0.        , 0.36776924, 0.82912604, 1.1407996 , 1.44675399,\n       1.73415578, 2.01139777, 2.28452002, 2.56100667, 2.83585069,\n       3.11413158, 3.38283034, 3.65911238, 3.93231661, 4.2186038 ])"
  },
  {
    "objectID": "notebooks/explore/index.html",
    "href": "notebooks/explore/index.html",
    "title": "Explore",
    "section": "",
    "text": "This is a daily workbench, mostly fot prototyping ideas, exploring data, implementing methods, debugging stuff and everything else.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\n23-08-02\n\n\nComparison of different noise models\n\n\n\n\n23-07-28\n\n\nChoosing step size for Inexact ALM algorithm\n\n\n\n\n23-07-05\n\n\nSimulation setup for benchmarking matrix factorization methods\n\n\n\n\n23-07-01\n\n\nHow to simulate ground truth for multi-phenotype z-scores?\n\n\n\n\n23-06-23\n\n\nMetrices for evaluating clusters given true labels\n\n\n\n\n23-06-05\n\n\nCan the low rank approximation capture the distinct GWAS phenotypes?\n\n\n\n\n23-05-23\n\n\nNuclear norm regularization using Frank-Wolfe algorithm\n\n\n\n\n23-05-16\n\n\nPCA of NPD summary statistics\n\n\n\n\n23-05-16\n\n\nRobust PCA implementation\n\n\n\n\n23-05-12\n\n\nPreprocess NPD summary statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/explore/2023-05-12-preprocess.html",
    "href": "notebooks/explore/2023-05-12-preprocess.html",
    "title": "Preprocess NPD summary statistics",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as sc_stats\nimport collections\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120)\n\n\n\nRead\nSummary statistics for association of selected SNPs with multiple diseases. SNPs which had significant association with at least one phenotype were selected in a previous preprocessing step, details in this repository. Next, the summary statistics of those SNPs for all available phenotypes were included from the respective GWAS.\n\n\nCode\ndata_dir = \"../data\"\nstudies  = ['gtex', 'pgc', 'ieu']\nassoc_file = {}\nfor s in studies:\n    assoc_file[s] = f\"{data_dir}/{s}_assoc_sb.txt.gz\"\ntrait_df_filename = f\"{data_dir}/trait_meta.csv\"\n\n\n\n\nCode\nassoc = {}\n\nreq_cols = {\n    'SNP' : 'string',\n    'CHR' : 'string',\n    'BP'  : 'Int64', # use Pandas Int64 to handle NA values\n    'A1'  : 'string',\n    'A2'  : 'string',\n    'Z'   : np.float64,\n    'P'   : np.float64,\n    'BETA': np.float64,\n    'SE'  : np.float64,\n    'ID'  : 'string',\n    'TRAIT': 'string'\n}\n\nfor s in studies:\n    print (f\"Read summary statistics for {s}\")\n    header = pd.read_csv(assoc_file[s], nrows=0, sep='\\t')\n    use_cols   = [x for x in req_cols.keys() if x in header.columns]\n    use_dtypes = {k:v for k, v in req_cols.items() if k in use_cols}\n    assoc[s] = pd.read_csv(assoc_file[s], sep = '\\t',\n                           usecols = use_cols,\n                           dtype = use_dtypes\n                          )\n\n\nRead summary statistics for gtex\nRead summary statistics for pgc\nRead summary statistics for ieu\n\n\n\n\nCode\ntrait_df = pd.read_csv(trait_df_filename)\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n\nClean\n\nCombine all summary statistics to a single DataFrame.\n\n\nCode\nassoc_df = pd.concat([v for k,v in assoc.items()])\nassoc_df['A1'] = assoc_df['A1'].str.upper()\nassoc_df['A2'] = assoc_df['A2'].str.upper()\nassoc_df['TRAIT'] = assoc_df['TRAIT'].fillna(assoc_df['ID'])\n\n\n\n\nCode\nassoc_df = assoc_df.drop_duplicates()\n\n\n\nassoc_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 680512 entries, 0 to 258531\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     680512 non-null  string \n 1   CHR     664837 non-null  string \n 2   BP      664837 non-null  Int64  \n 3   A2      680512 non-null  string \n 4   A1      680512 non-null  string \n 5   Z       260187 non-null  float64\n 6   P       680512 non-null  float64\n 7   BETA    640083 non-null  float64\n 8   SE      640068 non-null  float64\n 9   ID      680512 non-null  string \n 10  TRAIT   680512 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 63.0 MB\n\n\n\nassoc_df\n\n\n\n\n\n\n\n\nSNP\nCHR\nBP\nA2\nA1\nZ\nP\nBETA\nSE\nID\nTRAIT\n\n\n\n\n0\nrs147538909\nchr1\n115746\nT\nC\n-1.197695\n0.231036\nNaN\nNaN\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n1\nrs2977608\nchr1\n832873\nC\nA\n0.205174\n0.837436\n0.000454\n0.002213\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n2\nrs9442391\nchr1\n1048922\nC\nT\n1.415529\n0.156913\n0.002705\n0.001911\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n3\nrs6685064\nchr1\n1275912\nT\nC\n-1.869576\n0.061543\n-0.007038\n0.003764\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n4\nrs79113395\nchr1\n1659060\nA\nG\n-0.462598\n0.643653\n-0.000987\n0.002134\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n258527\nrs7599488\n2\n60718347\nT\nC\nNaN\n0.730000\n0.000530\n0.001563\nieu-b-13\nchildhood absence epilepsy\n\n\n258528\nrs75010292\n2\n55172536\nG\nA\nNaN\n0.091000\n-0.004222\n0.002496\nieu-b-13\nchildhood absence epilepsy\n\n\n258529\nrs10168513\n2\n55452375\nA\nG\nNaN\n0.860000\n-0.000413\n0.002291\nieu-b-13\nchildhood absence epilepsy\n\n\n258530\nrs17428810\n2\n32736043\nC\nT\nNaN\n0.840000\n-0.000342\n0.001684\nieu-b-13\nchildhood absence epilepsy\n\n\n258531\nrs615449\n2\n40469480\nA\nC\nNaN\n0.420000\n-0.002027\n0.002493\nieu-b-13\nchildhood absence epilepsy\n\n\n\n\n680512 rows × 11 columns\n\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10262\nNumber of unique studies: 92\n\n\n\n\nKeep only Biallelic SNPs\nassoc_df_fa: fa or ‘filter_alleles’ removes alleles which consists of more than two nucleotides.\n\n\nCode\ndef count_nucleotides(row, df):\n    '''\n    df must have 3 columns: SNP, A1, A2\n    '''\n    snp = row['SNP']\n    snp_dict = df[df['SNP'] == snp][['A1', 'A2']].to_dict('records')\n    return len(set([v for x in snp_dict for k, v in x.items()]))\n\n\n'''\nWhether to count the alleles.\nIf a SNP appears twice, most likely it is biallelic with (A1, A2) and (A2, A1) in the two rows.\nHowever, if the second row has (A2, A3), then it is not biallelic.\nTherefore, we count the unique nucleotides for each SNP.\n\n!!! This is slow. I have checked there are no (A2, A3) for the current data. Hence, avoid.\n'''\ndo_count_nucleotides = False\n\nalleles       = assoc_df[['SNP', 'A1', 'A2']].drop_duplicates()\ncount_colname = 'count'\nalleles_count = alleles['SNP'].value_counts().reset_index(name=count_colname)\nif do_count_nucleotides:\n    count_colname = 'nucleotide_count'\n    alleles_count[count_colname] = alleles_count.apply(count_nucleotides, axis = 1, args = (alleles,))\n    \nbiallelic = alleles_count[alleles_count[count_colname] &lt;= 2].merge(alleles, on='SNP', how='inner')\n\n# Arbitrary choice of alleles\none_arrange = biallelic.drop_duplicates('SNP', keep='first').filter(regex = '^(?!.*count)', axis = 1)\n\n\n\n\nCode\nassoc_df_allele1 = one_arrange.merge(assoc_df, on=['SNP', 'A1', 'A2'], how='inner')\nassoc_df_allele2 = one_arrange.merge(assoc_df, \n                        left_on=['SNP',  'A1', 'A2'], \n                        right_on=['SNP', 'A2', 'A1'],\n                        suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\nassoc_df_allele2['BETA'] = assoc_df_allele2['BETA'] * -1\nassoc_df_allele2['Z']    = assoc_df_allele2['Z'] * -1\n\nassoc_df_fa = pd.concat([assoc_df_allele1, assoc_df_allele2]) # fa: fixed alleles\n\n\n\nassoc_df_fa.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 677779 entries, 0 to 105160\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     677779 non-null  string \n 1   A1      677779 non-null  string \n 2   A2      677779 non-null  string \n 3   CHR     662150 non-null  string \n 4   BP      662150 non-null  Int64  \n 5   Z       259246 non-null  float64\n 6   P       677779 non-null  float64\n 7   BETA    637545 non-null  float64\n 8   SE      637530 non-null  float64\n 9   ID      677779 non-null  string \n 10  TRAIT   677779 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 62.7 MB\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fa['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fa['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10224\nNumber of unique studies: 92\n\n\n\n\nRemove SNPs found in fewer studies\nSNPs which are available in only a few studies are less powerful ‘features’. For a distribution of SNPs, see Figure 1.\n\n\nCode\nsnps_count = assoc_df_fa['SNP'].value_counts().reset_index(name='count')\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.hist(snps_count['count'], density = True)\nax1.set_xlabel('Number of studies available for each SNP')\nax1.set_ylabel('Density')\nplt.show()\n\n\n\n\n\nFigure 1: Distribution of SNPs in different studies\n\n\n\n\n\n\nCode\nassoc_df_fa_nsnp = snps_count[snps_count['count'] &gt;=20 ].merge(assoc_df_fa, on=['SNP'], how='inner').filter(regex = '^(?!.*count)', axis = 1)\n\n\n\nassoc_df_fa_nsnp.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 676261 entries, 0 to 676260\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     676261 non-null  string \n 1   A1      676261 non-null  string \n 2   A2      676261 non-null  string \n 3   CHR     660634 non-null  string \n 4   BP      660634 non-null  Int64  \n 5   Z       259054 non-null  float64\n 6   P       676261 non-null  float64\n 7   BETA    636027 non-null  float64\n 8   SE      636012 non-null  float64\n 9   ID      676261 non-null  string \n 10  TRAIT   676261 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 57.4 MB\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fa_nsnp['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fa_nsnp['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10068\nNumber of unique studies: 92\n\n\n\n\nRemove duplicate (SNP, ID) combinations\n\n\nCode\nassoc_df_fa_nsnp_nodup = assoc_df_fa_nsnp.drop_duplicates(subset=['SNP', 'ID'], keep = False)\n\n\n\nassoc_df_fa_nsnp_nodup.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 676229 entries, 0 to 676260\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     676229 non-null  string \n 1   A1      676229 non-null  string \n 2   A2      676229 non-null  string \n 3   CHR     660602 non-null  string \n 4   BP      660602 non-null  Int64  \n 5   Z       259054 non-null  float64\n 6   P       676229 non-null  float64\n 7   BETA    635995 non-null  float64\n 8   SE      635980 non-null  float64\n 9   ID      676229 non-null  string \n 10  TRAIT   676229 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 62.6 MB\n\n\n\nassoc_df_fa_nsnp_nodup\n\n\n\n\n\n\n\n\nSNP\nA1\nA2\nCHR\nBP\nZ\nP\nBETA\nSE\nID\nTRAIT\n\n\n\n\n0\nrs10486722\nC\nT\nchr7\n41772310\n0.881222\n0.378198\n0.001757\n0.001994\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n1\nrs10486722\nC\nT\nchr7\n41772310\n1.369354\n0.170889\n0.003476\n0.002538\nUKB_1180_Morning_or_evening_person_chronotype\nUKB_1180_Morning_or_evening_person_chronotype\n\n\n2\nrs10486722\nC\nT\nchr7\n41772310\n-0.138782\n0.889622\n-0.000257\n0.001850\nUKB_1200_Sleeplessness_or_insomnia\nUKB_1200_Sleeplessness_or_insomnia\n\n\n3\nrs10486722\nC\nT\nchr7\n41772310\n0.442336\n0.658246\n0.000035\n0.000080\nUKB_20002_1243_self_reported_psychological_or_...\nUKB_20002_1243_self_reported_psychological_or_...\n\n\n4\nrs10486722\nC\nT\nchr7\n41772310\n0.275442\n0.782977\n0.000030\n0.000110\nUKB_20002_1262_self_reported_parkinsons_disease\nUKB_20002_1262_self_reported_parkinsons_disease\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n676256\nrs2322\nC\nG\n4\n9701603\nNaN\n0.396100\n-0.077600\n0.091500\nieu-b-7\nParkinson's disease\n\n\n676257\nrs2322\nC\nG\n4\n9701603\nNaN\n0.107599\n0.024041\n0.014941\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\n\n\n676258\nrs2322\nC\nG\n4\n9701603\nNaN\n0.430338\n0.016141\n0.020468\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\n\n\n676259\nrs2322\nC\nG\n4\n9701603\nNaN\n0.068770\n0.049301\n0.027090\nMHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_F...\nMHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_F...\n\n\n676260\nrs2322\nC\nG\n4\n9701603\nNaN\n0.721340\n0.006329\n0.017746\nMHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filter...\nMHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filter...\n\n\n\n\n676229 rows × 11 columns\n\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fa_nsnp_nodup['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fa_nsnp_nodup['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10068\nNumber of unique studies: 92\n\n\n\n\n\nData Matrix\n\n\nCode\nbeta_df   = assoc_df_fa_nsnp_nodup[['SNP', 'ID', 'BETA']].pivot(index = 'SNP', columns = 'ID', values = 'BETA').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nse_df     = assoc_df_fa_nsnp_nodup[['SNP', 'ID', 'SE']].pivot(index = 'SNP', columns = 'ID', values = 'SE').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nzscore_df = assoc_df_fa_nsnp_nodup[['SNP', 'ID', 'Z']].pivot(index = 'SNP', columns = 'ID', values = 'Z').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\n\nse_df     = se_df.fillna(beta_df / zscore_df).replace(0, np.nan)\nzscore_df = zscore_df.fillna(beta_df / se_df)\nprec_df   = se_df.apply(lambda x: 1 / x / x)\n\n\n\nzscore_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10068 entries, rs1000031 to rs999494\nData columns (total 92 columns):\n #   Column                                                                                            Non-Null Count  Dtype  \n---  ------                                                                                            --------------  -----  \n 0   AD_sumstats_Jansenetal_2019sept.txt.gz                                                            10020 non-null  float64\n 1   CNCR_Insomnia_all                                                                                 9792 non-null   float64\n 2   ENIGMA_Intracraneal_Volume                                                                        9706 non-null   float64\n 3   GPC-NEO-NEUROTICISM                                                                               9430 non-null   float64\n 4   IGAP_Alzheimer                                                                                    9651 non-null   float64\n 5   ILAE_Genetic_generalised_epilepsy                                                                 9484 non-null   float64\n 6   Jones_et_al_2016_Chronotype                                                                       9782 non-null   float64\n 7   Jones_et_al_2016_SleepDuration                                                                    9782 non-null   float64\n 8   MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                       7801 non-null   float64\n 9   MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                           7826 non-null   float64\n 10  MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz            7977 non-null   float64\n 11  MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz  7971 non-null   float64\n 12  MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz     7972 non-null   float64\n 13  MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz          7973 non-null   float64\n 14  PGC3_SCZ_wave3_public.v2.txt.gz                                                                   9707 non-null   float64\n 15  PGC_ADHD_EUR_2017                                                                                 9688 non-null   float64\n 16  PGC_ASD_2017_CEU                                                                                  9626 non-null   float64\n 17  SSGAC_Depressive_Symptoms                                                                         9672 non-null   float64\n 18  SSGAC_Education_Years_Pooled                                                                      9740 non-null   float64\n 19  UKB_1160_Sleep_duration                                                                           9776 non-null   float64\n 20  UKB_1180_Morning_or_evening_person_chronotype                                                     9776 non-null   float64\n 21  UKB_1200_Sleeplessness_or_insomnia                                                                9776 non-null   float64\n 22  UKB_20002_1243_self_reported_psychological_or_psychiatric_problem                                 9776 non-null   float64\n 23  UKB_20002_1262_self_reported_parkinsons_disease                                                   9776 non-null   float64\n 24  UKB_20002_1265_self_reported_migraine                                                             9776 non-null   float64\n 25  UKB_20002_1289_self_reported_schizophrenia                                                        9776 non-null   float64\n 26  UKB_20002_1616_self_reported_insomnia                                                             9776 non-null   float64\n 27  UKB_20016_Fluid_intelligence_score                                                                9776 non-null   float64\n 28  UKB_20127_Neuroticism_score                                                                       9776 non-null   float64\n 29  UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy                                                         9776 non-null   float64\n 30  UKB_G43_Diagnoses_main_ICD10_G43_Migraine                                                         9776 non-null   float64\n 31  anxiety.meta.full.cc.txt.gz                                                                       8449 non-null   float64\n 32  anxiety.meta.full.fs.txt.gz                                                                       8418 non-null   float64\n 33  daner_PGC_BIP32b_mds7a_0416a.txt.gz                                                               9988 non-null   float64\n 34  daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz                                       8108 non-null   float64\n 35  daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz                                       8096 non-null   float64\n 36  daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz                8795 non-null   float64\n 37  iPSYCH-PGC_ASD_Nov2017.txt.gz                                                                     9654 non-null   float64\n 38  ieu-a-1000                                                                                        8792 non-null   float64\n 39  ieu-a-1009                                                                                        4022 non-null   float64\n 40  ieu-a-1018                                                                                        4009 non-null   float64\n 41  ieu-a-1019                                                                                        1439 non-null   float64\n 42  ieu-a-1029                                                                                        4180 non-null   float64\n 43  ieu-a-1041                                                                                        8494 non-null   float64\n 44  ieu-a-1042                                                                                        8492 non-null   float64\n 45  ieu-a-1043                                                                                        8496 non-null   float64\n 46  ieu-a-1044                                                                                        8495 non-null   float64\n 47  ieu-a-1045                                                                                        8496 non-null   float64\n 48  ieu-a-1046                                                                                        8491 non-null   float64\n 49  ieu-a-1047                                                                                        8496 non-null   float64\n 50  ieu-a-1048                                                                                        8498 non-null   float64\n 51  ieu-a-1061                                                                                        4117 non-null   float64\n 52  ieu-a-1062                                                                                        3940 non-null   float64\n 53  ieu-a-1063                                                                                        4059 non-null   float64\n 54  ieu-a-1064                                                                                        4120 non-null   float64\n 55  ieu-a-1065                                                                                        4117 non-null   float64\n 56  ieu-a-1066                                                                                        4120 non-null   float64\n 57  ieu-a-1067                                                                                        4116 non-null   float64\n 58  ieu-a-1068                                                                                        4117 non-null   float64\n 59  ieu-a-1085                                                                                        9043 non-null   float64\n 60  ieu-a-118                                                                                         8939 non-null   float64\n 61  ieu-a-297                                                                                         8870 non-null   float64\n 62  ieu-a-298                                                                                         242 non-null    float64\n 63  ieu-a-45                                                                                          2539 non-null   float64\n 64  ieu-a-808                                                                                         789 non-null    float64\n 65  ieu-a-810                                                                                         702 non-null    float64\n 66  ieu-a-812                                                                                         1206 non-null   float64\n 67  ieu-a-818                                                                                         377 non-null    float64\n 68  ieu-a-824                                                                                         719 non-null    float64\n 69  ieu-a-990                                                                                         7676 non-null   float64\n 70  ieu-b-10                                                                                          5586 non-null   float64\n 71  ieu-b-11                                                                                          5817 non-null   float64\n 72  ieu-b-12                                                                                          5828 non-null   float64\n 73  ieu-b-13                                                                                          5820 non-null   float64\n 74  ieu-b-14                                                                                          5780 non-null   float64\n 75  ieu-b-15                                                                                          5821 non-null   float64\n 76  ieu-b-16                                                                                          5829 non-null   float64\n 77  ieu-b-17                                                                                          5814 non-null   float64\n 78  ieu-b-18                                                                                          7781 non-null   float64\n 79  ieu-b-2                                                                                           9360 non-null   float64\n 80  ieu-b-43                                                                                          964 non-null    float64\n 81  ieu-b-5070                                                                                        7585 non-null   float64\n 82  ieu-b-7                                                                                           9227 non-null   float64\n 83  ieu-b-8                                                                                           5508 non-null   float64\n 84  ieu-b-9                                                                                           5631 non-null   float64\n 85  ocd_aug2017.txt.gz                                                                                9774 non-null   float64\n 86  pgc-bip2021-BDI.vcf.txt.gz                                                                        9551 non-null   float64\n 87  pgc-bip2021-BDII.vcf.txt.gz                                                                       9418 non-null   float64\n 88  pgc-bip2021-all.vcf.txt.gz                                                                        9718 non-null   float64\n 89  pgc.scz2                                                                                          9742 non-null   float64\n 90  pgcAN2.2019-07.vcf.txt.gz                                                                         9057 non-null   float64\n 91  pts_all_freeze2_overall.txt.gz                                                                    9988 non-null   float64\ndtypes: float64(92)\nmemory usage: 7.1 MB\n\n\n\npd.testing.assert_index_equal(beta_df.index, prec_df.index)\npd.testing.assert_index_equal(beta_df.index, zscore_df.index)\npd.testing.assert_index_equal(beta_df.columns, prec_df.columns)\npd.testing.assert_index_equal(beta_df.columns, zscore_df.columns)\n\n\nRemove studies with missingness\nStudies with high missingness are not good samples, hence they are removed. See histogram of missingness in Figure 2\n\n\nCode\nzscore_df\n\n\n\n\n\n\n\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz\nCNCR_Insomnia_all\nENIGMA_Intracraneal_Volume\nGPC-NEO-NEUROTICISM\nIGAP_Alzheimer\nILAE_Genetic_generalised_epilepsy\nJones_et_al_2016_Chronotype\nJones_et_al_2016_SleepDuration\nMDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_...\nMDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUK...\n...\nieu-b-7\nieu-b-8\nieu-b-9\nocd_aug2017.txt.gz\npgc-bip2021-BDI.vcf.txt.gz\npgc-bip2021-BDII.vcf.txt.gz\npgc-bip2021-all.vcf.txt.gz\npgc.scz2\npgcAN2.2019-07.vcf.txt.gz\npts_all_freeze2_overall.txt.gz\n\n\n\n\nrs1000031\n0.999531\n-0.327477\n2.184712\n1.241557\n0.441709\n-1.041816\n0.163658\n-0.163658\n0.336654\n0.793129\n...\n0.532189\nNaN\nNaN\n-0.198735\n1.057089\n-0.269020\n1.279776\n-0.433158\n-1.573766\n-1.674269\n\n\nrs1000269\n1.212805\n-1.046310\n0.001880\n0.741814\n-1.844296\n0.771000\n2.673787\n1.126391\n-0.092067\n-0.163246\n...\n1.665179\n-0.732000\n-0.699000\n0.100883\n-0.226381\n0.338368\n-0.924392\n0.832016\n0.681645\n-0.701776\n\n\nrs10003281\n0.813444\n2.034345\n-2.023031\n-1.750164\n-0.076778\n1.448634\n0.954165\n-1.805477\nNaN\nNaN\n...\n-0.475795\n4.437998\n2.366001\n0.967399\n0.286699\n-1.162661\n-0.199299\n0.014539\nNaN\n-1.379710\n\n\nrs10004866\n-0.011252\n1.327108\n1.004786\n1.442363\n-1.215173\n0.139000\n0.050154\n1.439531\n-2.458370\n-2.407460\n...\n-1.234375\n-2.520001\n-0.593997\n-0.685110\n0.902252\n1.106939\n1.776456\n-1.654677\n-0.964630\n0.851608\n\n\nrs10005235\n-0.612540\n-0.410609\n1.526040\n0.653087\n0.344062\n-1.868000\n2.183486\n-1.514102\n0.460191\n0.393006\n...\n0.387805\n-0.345000\n-0.960998\n0.177317\n-1.339598\n1.795867\n-1.249969\n2.349671\n0.996305\n-0.333356\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrs9989571\n-0.028306\n-0.208891\n1.883150\n0.366470\n0.821257\n0.712000\n-0.453762\n1.895698\n-0.218149\n-0.920789\n...\n-1.231511\n-0.820996\n0.712000\n-2.150176\n-0.877410\n-1.938969\n-2.729983\n3.207917\n1.469194\n-1.293122\n\n\nrs9991694\n0.679790\n-1.005571\n-1.004740\n0.753472\n-0.539271\n0.368971\n-1.674665\n2.862736\n3.744820\n3.583060\n...\n0.064417\nNaN\nNaN\n-2.884911\n-1.000231\n0.031860\n-1.248222\n2.309425\nNaN\n1.048454\n\n\nrs9992763\n-0.691405\n-0.010299\n2.067894\n-0.140010\n-0.419843\n1.320000\n0.138304\n-0.568052\n-0.019684\n0.194404\n...\n0.191860\n-0.074000\n1.030997\n-0.228287\n-0.051297\n0.781766\n0.010638\n0.456681\n-0.503370\n-1.435277\n\n\nrs9993607\n1.625392\n-0.391585\n-0.113291\n0.514268\n0.027576\n-0.512000\n-0.150969\n0.113039\n4.638940\n4.631950\n...\n-0.685106\n0.194000\n0.240001\n-0.790290\n-0.876804\n-0.577696\n-0.785670\n-0.062707\n0.240834\n-0.199740\n\n\nrs999494\n0.303642\n0.872613\n1.522435\n-0.227674\n1.390424\n-1.022395\n-0.138304\n-1.281552\n-1.873050\n-1.645590\n...\n-0.437500\n0.253000\n-0.926997\n-1.449674\n0.910515\n0.783853\n1.376043\n-5.195746\n-1.151316\n0.660120\n\n\n\n\n10068 rows × 92 columns\n\n\n\n\n\nCode\npd.DataFrame(zscore_df.isnull()).mean(axis=0)\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz    0.004768\nCNCR_Insomnia_all                         0.027414\nENIGMA_Intracraneal_Volume                0.035956\nGPC-NEO-NEUROTICISM                       0.063369\nIGAP_Alzheimer                            0.041418\n                                            ...   \npgc-bip2021-BDII.vcf.txt.gz               0.064561\npgc-bip2021-all.vcf.txt.gz                0.034764\npgc.scz2                                  0.032380\npgcAN2.2019-07.vcf.txt.gz                 0.100417\npts_all_freeze2_overall.txt.gz            0.007946\nLength: 92, dtype: float64\n\n\n\n\nCode\nmissing_df = pd.DataFrame(zscore_df.isnull()).mean(axis = 0).reset_index(name = 'missingness').rename(columns = {'index': 'ID'})\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.hist(missing_df['missingness'], density = True, bins = 20)\nax1.set_xlabel('Missingness')\nax1.set_ylabel('Density')\nplt.show()\n\n\n\n\n\nFigure 2: Missingness of SNPs in different studies\n\n\n\n\n\n\nCode\nkeep_columns = missing_df[missing_df['missingness'] &lt;= 0.6]['ID'].to_list()\nprint(f\"Keeping {len(keep_columns)} phenotypes with low missingness\")\n\nbeta_df_sub   = beta_df[keep_columns]\nprec_df_sub   = prec_df[keep_columns]\nzscore_df_sub = zscore_df[keep_columns]\nse_df_sub     = se_df[keep_columns]\n\nremove_columns = [x for x in missing_df['ID'].to_list() if x not in keep_columns]\nprint(\"Phenotypes removed:\")\nfor x in remove_columns:\n    missingness = missing_df.loc[missing_df['ID'] == x]['missingness'].iloc[0]\n    print(f\"\\t{missingness:.3f}\\t{x}\\t{phenotype_dict[x]}\")\n\n\nKeeping 80 phenotypes with low missingness\nPhenotypes removed:\n    0.601   ieu-a-1009  Other psych\n    0.602   ieu-a-1018  Other psych\n    0.857   ieu-a-1019  BD\n    0.609   ieu-a-1062  Cognition\n    0.976   ieu-a-298   Neurodegenerative\n    0.748   ieu-a-45    Other psych\n    0.922   ieu-a-808   BD\n    0.930   ieu-a-810   SZ\n    0.880   ieu-a-812   Neurodegenerative\n    0.963   ieu-a-818   Neurodegenerative\n    0.929   ieu-a-824   Neurodegenerative\n    0.904   ieu-b-43    Other psych\n\n\n\nbeta_df_sub.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10068 entries, rs1000031 to rs999494\nData columns (total 80 columns):\n #   Column                                                                                            Non-Null Count  Dtype  \n---  ------                                                                                            --------------  -----  \n 0   AD_sumstats_Jansenetal_2019sept.txt.gz                                                            10020 non-null  float64\n 1   CNCR_Insomnia_all                                                                                 8950 non-null   float64\n 2   ENIGMA_Intracraneal_Volume                                                                        8351 non-null   float64\n 3   GPC-NEO-NEUROTICISM                                                                               3623 non-null   float64\n 4   IGAP_Alzheimer                                                                                    8033 non-null   float64\n 5   ILAE_Genetic_generalised_epilepsy                                                                 0 non-null      float64\n 6   Jones_et_al_2016_Chronotype                                                                       8807 non-null   float64\n 7   Jones_et_al_2016_SleepDuration                                                                    8807 non-null   float64\n 8   MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                       7801 non-null   float64\n 9   MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                           7826 non-null   float64\n 10  MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz            7977 non-null   float64\n 11  MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz  7971 non-null   float64\n 12  MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz     7972 non-null   float64\n 13  MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz          7973 non-null   float64\n 14  PGC3_SCZ_wave3_public.v2.txt.gz                                                                   9707 non-null   float64\n 15  PGC_ADHD_EUR_2017                                                                                 7880 non-null   float64\n 16  PGC_ASD_2017_CEU                                                                                  7287 non-null   float64\n 17  SSGAC_Depressive_Symptoms                                                                         7888 non-null   float64\n 18  SSGAC_Education_Years_Pooled                                                                      8824 non-null   float64\n 19  UKB_1160_Sleep_duration                                                                           8829 non-null   float64\n 20  UKB_1180_Morning_or_evening_person_chronotype                                                     8829 non-null   float64\n 21  UKB_1200_Sleeplessness_or_insomnia                                                                8829 non-null   float64\n 22  UKB_20002_1243_self_reported_psychological_or_psychiatric_problem                                 8829 non-null   float64\n 23  UKB_20002_1262_self_reported_parkinsons_disease                                                   8829 non-null   float64\n 24  UKB_20002_1265_self_reported_migraine                                                             8829 non-null   float64\n 25  UKB_20002_1289_self_reported_schizophrenia                                                        8829 non-null   float64\n 26  UKB_20002_1616_self_reported_insomnia                                                             8829 non-null   float64\n 27  UKB_20016_Fluid_intelligence_score                                                                8829 non-null   float64\n 28  UKB_20127_Neuroticism_score                                                                       8829 non-null   float64\n 29  UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy                                                         8829 non-null   float64\n 30  UKB_G43_Diagnoses_main_ICD10_G43_Migraine                                                         8829 non-null   float64\n 31  anxiety.meta.full.cc.txt.gz                                                                       8449 non-null   float64\n 32  anxiety.meta.full.fs.txt.gz                                                                       8418 non-null   float64\n 33  daner_PGC_BIP32b_mds7a_0416a.txt.gz                                                               9988 non-null   float64\n 34  daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz                                       8108 non-null   float64\n 35  daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz                                       8096 non-null   float64\n 36  daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz                8795 non-null   float64\n 37  iPSYCH-PGC_ASD_Nov2017.txt.gz                                                                     9654 non-null   float64\n 38  ieu-a-1000                                                                                        8792 non-null   float64\n 39  ieu-a-1029                                                                                        4180 non-null   float64\n 40  ieu-a-1041                                                                                        8494 non-null   float64\n 41  ieu-a-1042                                                                                        8492 non-null   float64\n 42  ieu-a-1043                                                                                        8496 non-null   float64\n 43  ieu-a-1044                                                                                        8495 non-null   float64\n 44  ieu-a-1045                                                                                        8496 non-null   float64\n 45  ieu-a-1046                                                                                        8491 non-null   float64\n 46  ieu-a-1047                                                                                        8496 non-null   float64\n 47  ieu-a-1048                                                                                        8498 non-null   float64\n 48  ieu-a-1061                                                                                        4117 non-null   float64\n 49  ieu-a-1063                                                                                        4059 non-null   float64\n 50  ieu-a-1064                                                                                        4120 non-null   float64\n 51  ieu-a-1065                                                                                        4117 non-null   float64\n 52  ieu-a-1066                                                                                        4120 non-null   float64\n 53  ieu-a-1067                                                                                        4116 non-null   float64\n 54  ieu-a-1068                                                                                        4117 non-null   float64\n 55  ieu-a-1085                                                                                        9043 non-null   float64\n 56  ieu-a-118                                                                                         8939 non-null   float64\n 57  ieu-a-297                                                                                         8870 non-null   float64\n 58  ieu-a-990                                                                                         7676 non-null   float64\n 59  ieu-b-10                                                                                          5586 non-null   float64\n 60  ieu-b-11                                                                                          5817 non-null   float64\n 61  ieu-b-12                                                                                          5828 non-null   float64\n 62  ieu-b-13                                                                                          5820 non-null   float64\n 63  ieu-b-14                                                                                          5780 non-null   float64\n 64  ieu-b-15                                                                                          5821 non-null   float64\n 65  ieu-b-16                                                                                          5829 non-null   float64\n 66  ieu-b-17                                                                                          5814 non-null   float64\n 67  ieu-b-18                                                                                          7781 non-null   float64\n 68  ieu-b-2                                                                                           9360 non-null   float64\n 69  ieu-b-5070                                                                                        7585 non-null   float64\n 70  ieu-b-7                                                                                           9227 non-null   float64\n 71  ieu-b-8                                                                                           5508 non-null   float64\n 72  ieu-b-9                                                                                           5631 non-null   float64\n 73  ocd_aug2017.txt.gz                                                                                9774 non-null   float64\n 74  pgc-bip2021-BDI.vcf.txt.gz                                                                        9551 non-null   float64\n 75  pgc-bip2021-BDII.vcf.txt.gz                                                                       9418 non-null   float64\n 76  pgc-bip2021-all.vcf.txt.gz                                                                        9718 non-null   float64\n 77  pgc.scz2                                                                                          8775 non-null   float64\n 78  pgcAN2.2019-07.vcf.txt.gz                                                                         9057 non-null   float64\n 79  pts_all_freeze2_overall.txt.gz                                                                    9988 non-null   float64\ndtypes: float64(80)\nmemory usage: 6.2 MB\n\n\n\nprec_df_sub.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10068 entries, rs1000031 to rs999494\nData columns (total 80 columns):\n #   Column                                                                                            Non-Null Count  Dtype  \n---  ------                                                                                            --------------  -----  \n 0   AD_sumstats_Jansenetal_2019sept.txt.gz                                                            10020 non-null  float64\n 1   CNCR_Insomnia_all                                                                                 8950 non-null   float64\n 2   ENIGMA_Intracraneal_Volume                                                                        8351 non-null   float64\n 3   GPC-NEO-NEUROTICISM                                                                               3623 non-null   float64\n 4   IGAP_Alzheimer                                                                                    8033 non-null   float64\n 5   ILAE_Genetic_generalised_epilepsy                                                                 0 non-null      float64\n 6   Jones_et_al_2016_Chronotype                                                                       8807 non-null   float64\n 7   Jones_et_al_2016_SleepDuration                                                                    8807 non-null   float64\n 8   MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                       7801 non-null   float64\n 9   MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                           7826 non-null   float64\n 10  MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz            7977 non-null   float64\n 11  MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz  7971 non-null   float64\n 12  MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz     7972 non-null   float64\n 13  MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz          7973 non-null   float64\n 14  PGC3_SCZ_wave3_public.v2.txt.gz                                                                   9707 non-null   float64\n 15  PGC_ADHD_EUR_2017                                                                                 7880 non-null   float64\n 16  PGC_ASD_2017_CEU                                                                                  7287 non-null   float64\n 17  SSGAC_Depressive_Symptoms                                                                         7888 non-null   float64\n 18  SSGAC_Education_Years_Pooled                                                                      8824 non-null   float64\n 19  UKB_1160_Sleep_duration                                                                           8829 non-null   float64\n 20  UKB_1180_Morning_or_evening_person_chronotype                                                     8829 non-null   float64\n 21  UKB_1200_Sleeplessness_or_insomnia                                                                8829 non-null   float64\n 22  UKB_20002_1243_self_reported_psychological_or_psychiatric_problem                                 8829 non-null   float64\n 23  UKB_20002_1262_self_reported_parkinsons_disease                                                   8829 non-null   float64\n 24  UKB_20002_1265_self_reported_migraine                                                             8829 non-null   float64\n 25  UKB_20002_1289_self_reported_schizophrenia                                                        8829 non-null   float64\n 26  UKB_20002_1616_self_reported_insomnia                                                             8829 non-null   float64\n 27  UKB_20016_Fluid_intelligence_score                                                                8829 non-null   float64\n 28  UKB_20127_Neuroticism_score                                                                       8829 non-null   float64\n 29  UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy                                                         8829 non-null   float64\n 30  UKB_G43_Diagnoses_main_ICD10_G43_Migraine                                                         8829 non-null   float64\n 31  anxiety.meta.full.cc.txt.gz                                                                       8449 non-null   float64\n 32  anxiety.meta.full.fs.txt.gz                                                                       8418 non-null   float64\n 33  daner_PGC_BIP32b_mds7a_0416a.txt.gz                                                               9988 non-null   float64\n 34  daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz                                       8108 non-null   float64\n 35  daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz                                       8096 non-null   float64\n 36  daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz                8795 non-null   float64\n 37  iPSYCH-PGC_ASD_Nov2017.txt.gz                                                                     9654 non-null   float64\n 38  ieu-a-1000                                                                                        8792 non-null   float64\n 39  ieu-a-1029                                                                                        4180 non-null   float64\n 40  ieu-a-1041                                                                                        8494 non-null   float64\n 41  ieu-a-1042                                                                                        8492 non-null   float64\n 42  ieu-a-1043                                                                                        8496 non-null   float64\n 43  ieu-a-1044                                                                                        8495 non-null   float64\n 44  ieu-a-1045                                                                                        8496 non-null   float64\n 45  ieu-a-1046                                                                                        8491 non-null   float64\n 46  ieu-a-1047                                                                                        8496 non-null   float64\n 47  ieu-a-1048                                                                                        8498 non-null   float64\n 48  ieu-a-1061                                                                                        4117 non-null   float64\n 49  ieu-a-1063                                                                                        4059 non-null   float64\n 50  ieu-a-1064                                                                                        4120 non-null   float64\n 51  ieu-a-1065                                                                                        4117 non-null   float64\n 52  ieu-a-1066                                                                                        4120 non-null   float64\n 53  ieu-a-1067                                                                                        4116 non-null   float64\n 54  ieu-a-1068                                                                                        4117 non-null   float64\n 55  ieu-a-1085                                                                                        9043 non-null   float64\n 56  ieu-a-118                                                                                         8939 non-null   float64\n 57  ieu-a-297                                                                                         8870 non-null   float64\n 58  ieu-a-990                                                                                         7676 non-null   float64\n 59  ieu-b-10                                                                                          5586 non-null   float64\n 60  ieu-b-11                                                                                          5817 non-null   float64\n 61  ieu-b-12                                                                                          5828 non-null   float64\n 62  ieu-b-13                                                                                          5820 non-null   float64\n 63  ieu-b-14                                                                                          5780 non-null   float64\n 64  ieu-b-15                                                                                          5821 non-null   float64\n 65  ieu-b-16                                                                                          5829 non-null   float64\n 66  ieu-b-17                                                                                          5814 non-null   float64\n 67  ieu-b-18                                                                                          7781 non-null   float64\n 68  ieu-b-2                                                                                           9360 non-null   float64\n 69  ieu-b-5070                                                                                        7585 non-null   float64\n 70  ieu-b-7                                                                                           9227 non-null   float64\n 71  ieu-b-8                                                                                           5508 non-null   float64\n 72  ieu-b-9                                                                                           5631 non-null   float64\n 73  ocd_aug2017.txt.gz                                                                                9774 non-null   float64\n 74  pgc-bip2021-BDI.vcf.txt.gz                                                                        9551 non-null   float64\n 75  pgc-bip2021-BDII.vcf.txt.gz                                                                       9418 non-null   float64\n 76  pgc-bip2021-all.vcf.txt.gz                                                                        9718 non-null   float64\n 77  pgc.scz2                                                                                          8760 non-null   float64\n 78  pgcAN2.2019-07.vcf.txt.gz                                                                         9057 non-null   float64\n 79  pts_all_freeze2_overall.txt.gz                                                                    9988 non-null   float64\ndtypes: float64(80)\nmemory usage: 6.2 MB\n\n\n\n\n\nRemove studies with unreliable standard errors\nTo eliminate unreliable estimates of genetic associations, we remove samples (phenotypes) whose median standard error (SE) of beta value or log odds ratio is more than 0.2. The median SE for all the phenotypes are shown in Figure 3\nNote: For SE, the mean is not optimal because, in some studies, there are few SNPs with very high (~10000) SE, which inflates the mean.\n\n\nCode\nmean_se  = se_df_sub.median(axis = 0, skipna = True)\nmean_se  = pd.DataFrame(mean_se).set_axis([\"mean_se\"], axis = 1)\nbeta_std = beta_df_sub.std(axis = 0, skipna = True)\nbeta_std = pd.DataFrame(beta_std).set_axis([\"beta_std\"], axis = 1)\nerror_df = pd.concat([mean_se, beta_std], axis = 1)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(np.log10(error_df['beta_std']), np.log10(error_df['mean_se']), alpha = 0.5, s = 100)\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.set_yticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.plot_diag(ax1)\n\nkeep_columns = error_df.query(\"mean_se &lt;= 0.2\").index\nfor pid in error_df.index.to_list():\n    if pid not in keep_columns:\n        pid_text = f\"{pid} / {phenotype_dict[pid]}\"\n        xval = np.log10(error_df.loc[pid]['beta_std'])\n        yval = np.log10(error_df.loc[pid]['mean_se'])\n        ax1.annotate(pid_text, (xval, yval))\n\nax1.set_xlabel(r\"Standard Deviation of mean $\\beta$\")\nax1.set_ylabel(r\"Median of SE\")\nplt.show()\n\n\n\n\n\nFigure 3: Calibration of SE against std of beta\n\n\n\n\n\n\nCode\nprint(f\"Keeping {len(keep_columns)} phenotypes with low standard error\")\n\nbeta_df_sub_lowse   = beta_df_sub[keep_columns]\nprec_df_sub_lowse   = prec_df_sub[keep_columns]\nzscore_df_sub_lowse = zscore_df_sub[keep_columns]\nse_df_sub_lowse     = se_df_sub[keep_columns]\n\nremove_columns = [x for x in error_df.index.to_list() if x not in keep_columns]\nprint(\"Phenotypes removed:\")\nfor x in remove_columns:\n    se = error_df.loc[x]['mean_se']\n    bstd = error_df.loc[x]['beta_std']\n    print(f\"\\t{se:.3f}\\t{bstd:.3f}\\t{x}\\t{phenotype_dict[x]}\")\n\n\nKeeping 70 phenotypes with low standard error\nPhenotypes removed:\n    2322.222    4272.766    ENIGMA_Intracraneal_Volume  Volume\n    nan nan ILAE_Genetic_generalised_epilepsy   Epilepsy\n    2330.240    4287.398    ieu-a-1041  Volume\n    1.326   2.479   ieu-a-1042  Volume\n    2.855   5.450   ieu-a-1043  Volume\n    5.930   11.025  ieu-a-1044  Volume\n    5.691   10.837  ieu-a-1045  Volume\n    2.319   4.306   ieu-a-1046  Volume\n    7.255   13.720  ieu-a-1047  Volume\n    7.590   14.253  ieu-a-1048  Volume\n\n\n\n\nRemove studies with unreliable z-scores\nTo eliminate unreliable z-scores, we make sure that the median of z-scores is less than 0.2\n\n\nCode\nmedian_zscore = zscore_df_sub_lowse.median(axis = 0, skipna = True).abs()\nmedian_zscore  = pd.DataFrame(median_zscore).set_axis([\"median_zscore\"], axis = 1)\n\nkeep_columns = median_zscore.query(\"median_zscore &lt;= 0.2\").index\nremove_columns = median_zscore.query(\"median_zscore &gt; 0.2\").index\n\nprint(f\"Keeping {len(keep_columns)} phenotypes with reasonable z-scores.\")\n\nbeta_df_sub_lowse_zchi2   = beta_df_sub_lowse[keep_columns]\nprec_df_sub_lowse_zchi2   = prec_df_sub_lowse[keep_columns]\nzscore_df_sub_lowse_zchi2 = zscore_df_sub_lowse[keep_columns]\nse_df_sub_lowse_zchi2     = se_df_sub_lowse[keep_columns]\n\nprint(\"Phenotypes removed:\")\nfor x in remove_columns:\n    print(f\"\\t{x}\\t{phenotype_dict[x]}\")\n\n\nKeeping 69 phenotypes with reasonable z-scores.\nPhenotypes removed:\n    ieu-b-5070  SZ\n\n\n\n\nCode\ndef q1(x, axis = None):\n    return np.percentile(x, 25, axis = axis)\n\ndef q3(x, axis = None):\n    return np.percentile(x, 75, axis = axis)\n\ndef iqr_outlier(x, axis = None, bar = 1.5, side = 'both'):\n    assert side in ['gt', 'lt', 'both'], 'Side should be `gt`, `lt` or `both`.'\n\n    d_iqr = sc_stats.iqr(x, axis = axis)\n    d_q1 = q1(x, axis = axis)\n    d_q3 = q3(x, axis = axis)\n    iqr_distance = np.multiply(d_iqr, bar)\n\n    stat_shape = list(x.shape)\n\n    if isinstance(axis, collections.abc.Iterable):\n        for single_axis in axis:\n            stat_shape[single_axis] = 1\n    else:\n        stat_shape[axis] = 1\n\n    if side in ['gt', 'both']:\n        upper_range = d_q3 + iqr_distance\n        upper_outlier = np.greater(x - upper_range.reshape(stat_shape), 0)\n    if side in ['lt', 'both']:\n        lower_range = d_q1 - iqr_distance\n        lower_outlier = np.less(x - lower_range.reshape(stat_shape), 0)\n\n    if side == 'gt':\n        return upper_outlier\n    if side == 'lt':\n        return lower_outlier\n    if side == 'both':\n        return np.logical_or(upper_outlier, lower_outlier)\n\ndef get_density(x, data):\n    density = sc_stats.gaussian_kde(data)\n    return density.pdf(x)\n\ndef get_bins(data, nbin, xmin, xmax):\n    xdelta = (np.max(data) - np.min(data)) / 10\n    if not xmin: xmin = np.min(data) - xdelta\n    if not xmax: xmax = np.max(data) + xdelta\n    bins = np.linspace(xmin, xmax, nbin)\n    xbin = [(bins[i] + bins[i+1]) / 2 for i in range(bins.shape[0] - 1)] # centers of the bins\n    return xmin, xmax, bins, xbin\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nremove_columns = []\nfor pid in zscore_df_sub_lowse_zchi2.columns.to_list():\n    x = zscore_df_sub_lowse_zchi2[pid].values\n    x_dropna = x[~np.isnan(x)]\n    outlier_mask = iqr_outlier(x_dropna, axis = 0, bar = 5)\n    #data = np.square(x_dropna[~outlier_mask])\n    data = x_dropna[~outlier_mask]\n    xmin, xmax, bins, xbin = get_bins(data, 100, None, None)\n    curve = get_density(xbin, data)\n    #print (pid, xmax)\n    if xmax &gt;= 1000:\n        print (pid, xmax)\n        remove_columns.append(pid)\n    else:\n        ax1.plot(xbin, curve, label = pid)\n        \n#ax1.legend()\nplt.show()\n\n\n\n\n\nFigure 4: Distribution of z-scores for all phenotypes\n\n\n\n\n\n\nFlip Signs\n\n\nCode\nto_flip = [\n    \"AD_sumstats_Jansenetal_2019sept.txt.gz\",\n    \"daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz\",\n    \"Jones_et_al_2016_Chronotype\",\n    \"Jones_et_al_2016_SleepDuration\",\n    \"UKB_1160_Sleep_duration\",\n    #\"UKB_1180_Morning_or_evening_person_chronotype\",\n    \"MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\",\n    \"MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\",\n    \"PGC3_SCZ_wave3_public.v2.txt.gz\",\n]\n\nremaining_columns = beta_df_sub_lowse_zchi2.columns.to_list()\nfor flip_id in to_flip:\n    if flip_id in remaining_columns:\n        beta_df_sub_lowse_zchi2.loc[:, (flip_id)] = - beta_df_sub_lowse_zchi2[flip_id]\n        zscore_df_sub_lowse_zchi2.loc[:, (flip_id)] = - zscore_df_sub_lowse_zchi2[flip_id]\n        print(flip_id)\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz\ndaner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz\nJones_et_al_2016_Chronotype\nJones_et_al_2016_SleepDuration\nUKB_1160_Sleep_duration\nMDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\nMDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\nPGC3_SCZ_wave3_public.v2.txt.gz\n\n\n\n\nSave Pickle\n\nbeta_df_filename = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename = f\"{data_dir}/prec_df.pkl\"\nse_df_filename   = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\nbeta_df_sub_lowse_zchi2.to_pickle(beta_df_filename)\nprec_df_sub_lowse_zchi2.to_pickle(prec_df_filename)\nse_df_sub_lowse_zchi2.to_pickle(se_df_filename)\nzscore_df_sub_lowse_zchi2.to_pickle(zscore_df_filename)\n\n\n\nCode\nzscore_df_sub_lowse_zchi2.columns.to_list()\n\n\n['AD_sumstats_Jansenetal_2019sept.txt.gz',\n 'CNCR_Insomnia_all',\n 'GPC-NEO-NEUROTICISM',\n 'IGAP_Alzheimer',\n 'Jones_et_al_2016_Chronotype',\n 'Jones_et_al_2016_SleepDuration',\n 'MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz',\n 'MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz',\n 'MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'PGC3_SCZ_wave3_public.v2.txt.gz',\n 'PGC_ADHD_EUR_2017',\n 'PGC_ASD_2017_CEU',\n 'SSGAC_Depressive_Symptoms',\n 'SSGAC_Education_Years_Pooled',\n 'UKB_1160_Sleep_duration',\n 'UKB_1180_Morning_or_evening_person_chronotype',\n 'UKB_1200_Sleeplessness_or_insomnia',\n 'UKB_20002_1243_self_reported_psychological_or_psychiatric_problem',\n 'UKB_20002_1262_self_reported_parkinsons_disease',\n 'UKB_20002_1265_self_reported_migraine',\n 'UKB_20002_1289_self_reported_schizophrenia',\n 'UKB_20002_1616_self_reported_insomnia',\n 'UKB_20016_Fluid_intelligence_score',\n 'UKB_20127_Neuroticism_score',\n 'UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy',\n 'UKB_G43_Diagnoses_main_ICD10_G43_Migraine',\n 'anxiety.meta.full.cc.txt.gz',\n 'anxiety.meta.full.fs.txt.gz',\n 'daner_PGC_BIP32b_mds7a_0416a.txt.gz',\n 'daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz',\n 'daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz',\n 'daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz',\n 'iPSYCH-PGC_ASD_Nov2017.txt.gz',\n 'ieu-a-1000',\n 'ieu-a-1029',\n 'ieu-a-1061',\n 'ieu-a-1063',\n 'ieu-a-1064',\n 'ieu-a-1065',\n 'ieu-a-1066',\n 'ieu-a-1067',\n 'ieu-a-1068',\n 'ieu-a-1085',\n 'ieu-a-118',\n 'ieu-a-297',\n 'ieu-a-990',\n 'ieu-b-10',\n 'ieu-b-11',\n 'ieu-b-12',\n 'ieu-b-13',\n 'ieu-b-14',\n 'ieu-b-15',\n 'ieu-b-16',\n 'ieu-b-17',\n 'ieu-b-18',\n 'ieu-b-2',\n 'ieu-b-7',\n 'ieu-b-8',\n 'ieu-b-9',\n 'ocd_aug2017.txt.gz',\n 'pgc-bip2021-BDI.vcf.txt.gz',\n 'pgc-bip2021-BDII.vcf.txt.gz',\n 'pgc-bip2021-all.vcf.txt.gz',\n 'pgc.scz2',\n 'pgcAN2.2019-07.vcf.txt.gz',\n 'pts_all_freeze2_overall.txt.gz']"
  },
  {
    "objectID": "notebooks/explore/2023-05-16-pca-sumstat.html",
    "href": "notebooks/explore/2023-05-16-pca-sumstat.html",
    "title": "PCA of NPD summary statistics",
    "section": "",
    "text": "About\nFor multi-trait analysis of GWAS summary statistics, we look at one of the simplest models - the principal component analysis (PCA). The goal is to characterize the latent components of genetic associations. We apply PCA to the matrix of summary statistics derived from GWAS across 80 NPD phenotypes from various sources – namely, GTEx, OpenGWAS and PGC. For a similar comprehensive study with the UK Biobank data, see Tanigawa et al., Nat. Comm. 2019.\nThe \\mathbf{X} matrix of size N \\times P for the PCA is characterized by N phenotypes (samples) and P variants (features).\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nfrom sklearn.decomposition import PCA\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nRead summary statistics\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\nAfter filtering, there are 100068 variants for 69 phenotypes, summarized below.\n\nbeta_df\n\n\n\n\n\n\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz\nCNCR_Insomnia_all\nGPC-NEO-NEUROTICISM\nIGAP_Alzheimer\nJones_et_al_2016_Chronotype\nJones_et_al_2016_SleepDuration\nMDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_...\nMDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUK...\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\n...\nieu-b-7\nieu-b-8\nieu-b-9\nocd_aug2017.txt.gz\npgc-bip2021-BDI.vcf.txt.gz\npgc-bip2021-BDII.vcf.txt.gz\npgc-bip2021-all.vcf.txt.gz\npgc.scz2\npgcAN2.2019-07.vcf.txt.gz\npts_all_freeze2_overall.txt.gz\n\n\n\n\nrs1000031\n-0.002240\n-0.003273\n0.1289\n0.0072\n-0.000698\n0.000712\n0.001170\n0.005629\n-0.016636\n-0.046173\n...\n0.0124\nNaN\nNaN\n-0.006995\n0.012896\n-0.005596\n0.012798\n-0.004802\n-0.022505\n-0.022603\n\n\nrs1000269\n-0.002631\n-0.009901\n0.0708\n-0.0286\n-0.010576\n-0.004509\n0.001181\n0.000982\n0.025008\n0.044356\n...\n0.0373\n-0.005293\n-0.008693\n0.003400\n-0.002603\n0.006598\n-0.008597\n0.008801\n0.009202\n-0.008702\n\n\nrs10003281\n-0.005380\n0.060423\n-0.4361\n-0.0037\n-0.011744\n0.022551\nNaN\nNaN\nNaN\nNaN\n...\n-0.0344\n0.111759\n0.103478\n0.097997\n0.011296\n-0.091501\n-0.006099\n0.000500\nNaN\n-0.036700\n\n\nrs10004866\n0.000037\n0.019230\n0.2035\n-0.0294\n-0.000277\n-0.008780\n-0.021749\n-0.023538\n-0.000016\n-0.034849\n...\n-0.0316\n-0.027825\n-0.011386\n-0.035900\n0.016601\n0.036197\n0.027002\n-0.027703\n-0.021801\n0.015499\n\n\nrs10005235\n0.002221\n-0.007645\n0.1255\n0.0108\n-0.016947\n0.011787\n0.003163\n0.002522\n0.016984\n0.004132\n...\n0.0159\n-0.004489\n-0.021064\n0.010604\n-0.028399\n0.068602\n-0.021499\n0.045102\n0.025605\n-0.006400\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrs9989571\n0.000085\n-0.002726\nNaN\n0.0184\n0.002450\n-0.010379\n-0.001487\n-0.009087\n0.024389\n0.023753\n...\n-0.0383\n-0.008719\n0.012676\n-0.100198\n-0.014302\n-0.054097\n-0.037401\n0.047999\n0.028502\n-0.021595\n\n\nrs9991694\n-0.004170\n-0.011018\nNaN\n-0.0150\n0.007666\n-0.013183\n0.031620\n0.034180\n-0.062177\n-0.058765\n...\n0.0021\nNaN\nNaN\n-0.185500\n-0.015103\n0.000800\n-0.015103\n0.033102\nNaN\n0.017299\n\n\nrs9992763\n0.001502\n-0.000098\nNaN\n-0.0067\n-0.000568\n0.002275\n0.000704\n0.002954\n0.013290\n0.001284\n...\n0.0033\n-0.000539\n0.012932\n-0.007899\n-0.000600\n0.015401\n0.000100\n0.004898\n-0.006896\n-0.018802\n\n\nrs9993607\n-0.003670\n-0.003879\nNaN\n0.0005\n0.000622\n-0.000486\n0.031370\n0.036475\n-0.044404\n-0.045986\n...\n-0.0161\n0.001483\n0.003172\n-0.027897\n-0.010697\n-0.011901\n-0.007700\n-0.000700\n0.003396\n-0.002597\n\n\nrs999494\n-0.000829\n0.010356\n-0.0274\n0.0274\n0.000675\n0.006377\n-0.014707\n-0.014568\n0.021090\n-0.000227\n...\n-0.0126\n0.002342\n-0.014567\n-0.060596\n0.013202\n0.019204\n0.016100\n-0.070304\n-0.019803\n0.010100\n\n\n\n\n10068 rows × 69 columns\n\n\n\n\nselect_ids = beta_df.columns\n\n\n\nPCA\nWe use the z-score as the observation statistic for each phenotype and variant. For PCA, we center the columns of the X matrix.\n\n\nCode\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n#phenotype_dict\n\n\n\n\nCode\nX = np.array(zscore_df.replace(np.nan, 0)[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\n\n\nCode\nncomp = min(Xcent.shape)\npca = PCA(n_components = ncomp)\npca.fit(Xcent)\nbeta_pcs = pca.fit_transform(Xcent)\nbeta_eig  = pca.components_\n\n\nThe variance explained by each principal component is shown in Figure 1\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\n#ax1.scatter(np.arange(ncomp), pca.explained_variance_ratio_, s = 100, alpha = 0.7)\nax1.plot(np.arange(ncomp), np.cumsum(pca.explained_variance_ratio_), marker = 'o')\nax1.set_ylabel(\"Fraction of variance explained\")\nax1.set_xlabel(\"Principal Components\")\n\nax2.plot(np.arange(ncomp), pca.explained_variance_ratio_, marker = 'o')\nax2.set_ylabel(\"Variance explained by each component\")\nax2.set_xlabel(\"Principal Components\")\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\nFigure 1: Variance explained by the Principal Components\n\n\n\n\nWe plot the first 6 principal components against each other in Figure 2. Each dot is a sample (phenotype) colored by their broad label of NPD.\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.8, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, beta_pcs)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 2: First 6 principal components compared against each other.\n\n\n\n\n\n\nPCA using SVD\nJust to make sure that I understand things correctly, I am redoing the PCA using SVD. It must yield the same results as sklearn PCA. Indeed, it does as shown in Figure 3.\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices=False)\nsvd_pcs = U @ np.diag(S)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\nsvd_pc1 = svd_pcs[:, idx1]\nsvd_pc2 = svd_pcs[:, idx2]\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \n# idxs = np.array([i for i, x in enumerate(labels) if x == 'Sleep'])\n# for idx in idxs:\n#     pid = select_ids[idx]\n#     ax1.annotate(pid, (svd_pc1[idx], svd_pc2[idx]))\n\n    \nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nplt.show()\n\n\n\n\n\nFigure 3: Check PCA using SVD.\n\n\n\n\n\n\nWeighted PCA\nHere, I implement the weighted covariance eigendecomposition approach algorithm proposed by L. Delchambre (2014). I used 3 different versions of the weight matrix:\n\nthe identity matrix (to check that the implementation is correct).\nthe precision matrix (as suggested by David in the project proposal).\nsquare root of the precision (ad-hoc idea to reduce the weights on each observation)\n\n\n# W = np.ones(X.shape)\n# W = np.array(prec_df[select_ids]).T\nW = np.sqrt(np.array(prec_df.replace(np.nan, 0)[select_ids]).T)\n\n\n\nCode\ndef weighted_mean(x, w=None, axis=None):\n    \"\"\"Compute the weighted mean along the given axis\n\n    The result is equivalent to (x * w).sum(axis) / w.sum(axis),\n    but large temporary arrays are not created.\n\n    Parameters\n    ----------\n    x : array_like\n        data for which mean is computed\n    w : array_like (optional)\n        weights corresponding to each data point. If supplied, it must be the\n        same shape as x\n    axis : int or None (optional)\n        axis along which mean should be computed\n\n    Returns\n    -------\n    mean : np.ndarray\n        array representing the weighted mean along the given axis\n    \"\"\"\n    if w is None:\n        return np.mean(x, axis)\n\n    x = np.asarray(x)\n    w = np.asarray(w)\n\n    if x.shape != w.shape:\n        raise NotImplementedError(\"Broadcasting is not implemented: \"\n                                  \"x and w must be the same shape.\")\n\n    if axis is None:\n        wx_sum = np.einsum('i,i', np.ravel(x), np.ravel(w))\n    else:\n        try:\n            axis = tuple(axis)\n        except TypeError:\n            axis = (axis,)\n\n        if len(axis) != len(set(axis)):\n            raise ValueError(\"duplicate value in 'axis'\")\n\n        trans = sorted(set(range(x.ndim)).difference(axis)) + list(axis)\n        operand = \"...{0},...{0}\".format(''.join(chr(ord('i') + i)\n                                                 for i in range(len(axis))))\n        wx_sum = np.einsum(operand,\n                           np.transpose(x, trans),\n                           np.transpose(w, trans))\n\n    return wx_sum / np.sum(w, axis)\n\ndef weighted_pca_delchambre(X, W, n_components = None, regularization = None):\n    \n    import scipy as sp\n    \n    weights = weighted_mean(X, W, axis = 0).reshape(1, -1)\n    _X = (X - weights) *  weights\n    _covar = np.dot(_X.T, _X)\n    _covar /= np.dot(weights.T, weights)\n    _covar[np.isnan(_covar)] = 0\n\n    n_components = 20\n    eigvals = (_X.shape[1] - n_components, _X.shape[1] - 1)\n    evals, evecs = sp.linalg.eigh(_covar, subset_by_index = eigvals)\n\n    components = evecs[:, ::-1].T\n    explained_variance = evals[::-1]\n    Y = np.zeros((_X.shape[0], components.shape[0]))\n    for i in range(_X.shape[0]):\n        cW = components * weights[0, i]\n        cWX = np.dot(cW, _X[i])\n        cWc = np.dot(cW, cW.T)\n        if regularization is not None:\n            cWc += np.diag(regularization / explained_variance)\n        Y[i] = np.linalg.solve(cWc, cWX)\n    return Y, explained_variance\n\n\n\n\nCode\nweighted_pcs, explained_variance = weighted_pca_delchambre(Xcent, W, n_components = 20, regularization = None)\n\n\nAs before, the variance explained by each principal component is shown in Figure 4\n\n\nCode\nwpca_tot_explained_variance = np.sum(explained_variance)\nwpca_explained_variance_ratio = explained_variance / wpca_tot_explained_variance\n\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(np.arange(20), np.cumsum(wpca_explained_variance_ratio), marker = 'o')\nax1.set_ylabel(\"Fraction of variance explained\")\nax1.set_xlabel(\"Principal Components\")\n\nax2.plot(np.arange(20), wpca_explained_variance_ratio, marker = 'o')\nax2.set_ylabel(\"Variance explained by each component\")\nax2.set_xlabel(\"Principal Components\")\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\nFigure 4: Variance explained by the weighted Principal Components\n\n\n\n\nWe plot the first 6 principal components against each other in Figure 5. Each dot is a sample (phenotype) colored by their broad label of NPD.\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.8, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, weighted_pcs)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 5: First 6 weighted principal components compared against each other.\n\n\n\n\n\n\nFurther Reading\n\nComments on PCA by Alex Williams"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Network effects of neuropsychiatric disorders",
    "section": "",
    "text": "The project aims to use convex optimization methods to understand the network effects of neuropsychiatric disorders.\nMy daily researach journal at the NYGC is also available online. To learn more about my work and research interests, please visit my main portfolio website."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "References",
    "section": "",
    "text": "Grotzinger, Andrew D., Mijke Rhemtulla, Ronald de Vlaming, Stuart J. Ritchie, Travis T. Mallard, W. David Hill, Hill F. Ip, et al. 2019. “Genomic Structural Equation Modelling Provides Insights into the Multivariate Genetic Architecture of Complex Traits.” Nature Human Behaviour 3 (5): 513–25. https://doi.org/10.1038/s41562-019-0566-x.\n\n\nTanigawa, Yosuke, Jiehan Li, Johanne M. Justesen, Heiko Horn, Matthew Aguirre, Christopher DeBoever, Chris Chang, et al. 2019. “Components of Genetic Associations Across 2,138 Phenotypes in the UK Biobank Highlight Adipocyte Biology.” Nature Communications 10 (1): 4064. https://doi.org/10.1038/s41467-019-11953-9."
  },
  {
    "objectID": "reference/index.html#methods",
    "href": "reference/index.html#methods",
    "title": "References",
    "section": "",
    "text": "Grotzinger, Andrew D., Mijke Rhemtulla, Ronald de Vlaming, Stuart J. Ritchie, Travis T. Mallard, W. David Hill, Hill F. Ip, et al. 2019. “Genomic Structural Equation Modelling Provides Insights into the Multivariate Genetic Architecture of Complex Traits.” Nature Human Behaviour 3 (5): 513–25. https://doi.org/10.1038/s41562-019-0566-x.\n\n\nTanigawa, Yosuke, Jiehan Li, Johanne M. Justesen, Heiko Horn, Matthew Aguirre, Christopher DeBoever, Chris Chang, et al. 2019. “Components of Genetic Associations Across 2,138 Phenotypes in the UK Biobank Highlight Adipocyte Biology.” Nature Communications 10 (1): 4064. https://doi.org/10.1038/s41467-019-11953-9."
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html",
    "title": "Comparison of different noise models",
    "section": "",
    "text": "Our goal is to obtain a denoised, low-rank approximation of the input matrix. Such low-rank matrix can be obtained using different noise models:\n\nGaussian noise \n\\min_{\\mathbf{X}} \\frac{1}{2}\\left\\lVert \\mathbf{Y} - \\mathbf{X} \\right\\rVert_{F}^2 \\quad \\textrm{s.t.} \\quad \\left\\lVert \\mathbf{X} \\right\\rVert_{*} \\le r\n\nSparse noise \n\\min_{\\mathbf{X}, \\mathbf{M}} \\left\\lVert \\mathbf{X} \\right\\rVert_{*} + \\lambda \\left\\lVert \\mathbf{M} \\right\\rVert_{1} \\quad \\textrm{s.t.} \\quad \\mathbf{L} + \\mathbf{M} = \\mathbf{Y}\n\nGaussian + sparse \n\\min_{\\mathbf{X}} \\frac{1}{2}\\left\\lVert \\mathbf{Y} - \\mathbf{X} - \\mathbf{M} \\right\\rVert_{F}^2 \\quad \\textrm{s.t.} \\quad \\left\\lVert \\mathbf{X} \\right\\rVert_{*} \\le r_L\\,, \\left\\lVert \\mathbf{M} \\right\\rVert_{1} \\le r_M\n\n\nFor models (1) and (3), we use the Frank-Wolfe method, and for model (2), we use the inexact augmented Lagrangian method (IALM) with a slight modification of the step-size update (borrowing the ADMM updates of Boyd et. al.). We use a simple simulation (as described previously) with additive Gaussian noise to realistically mimic the z-scores obtained from multiple GWAS."
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#ialm---rpca",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#ialm---rpca",
    "title": "Comparison of different noise models",
    "section": "IALM - RPCA",
    "text": "IALM - RPCA\n\n\nCode\nrpca = IALM(benchmark = True, max_iter = 1000, mu_update_method='admm', show_progress = True)\nrpca.fit(Y_cent, Xtrue = Y_cent)\n\n\n2023-08-02 14:46:55,016 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 0. Primal residual 0.807755. Dual residual 0.00319521\n2023-08-02 14:47:16,524 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 100. Primal residual 6.9408e-06. Dual residual 1.68576e-06"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnm",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnm",
    "title": "Comparison of different noise models",
    "section": "FW - NNM",
    "text": "FW - NNM\nWe do a little cheating here. Instead of cross-validation, we directly use the known rank of the underlying true data. However, based on my previous work, I know that cross-validation does provide the correct rank.\n\n\nCode\nnnm = FrankWolfe(model = 'nnm', svd_max_iter = 50, \n                 show_progress = True, debug = True, benchmark = True)\nnnm.fit(Y_cent, 40.0, Ytrue = Y_cent)\n\n\n2023-08-02 14:47:37,144 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 5383.02"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnml1",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnml1",
    "title": "Comparison of different noise models",
    "section": "FW - NNML1",
    "text": "FW - NNML1\nFor the third model, we use the sparse noise threshold from the RPCA result and the rank from the true data. I have not checked cross-validation results for this method yet.\n\nnp.sum(np.abs(rpca.E_)) / (ngwas * nsnp)\n\n0.7012888342395619\n\n\n\n\nCode\nnnm_sparse = FrankWolfe(model = 'nnm-sparse', max_iter = 1000, svd_max_iter = 50, \n                        tol = 1e-3, step_tol = 1e-5, simplex_method = 'sort',\n                        show_progress = True, debug = True, benchmark = True, print_skip = 100)\nnnm_sparse.fit(Y_cent, (40.0, 0.7), Ytrue = Y_cent)\n\n\n2023-08-02 14:47:49,393 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 6.42438e+06\n2023-08-02 14:48:03,988 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.004. Duality Gap 6.28097"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#cpu-time-and-accuracy",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#cpu-time-and-accuracy",
    "title": "Comparison of different noise models",
    "section": "CPU time and accuracy",
    "text": "CPU time and accuracy\nIn Figure 2, we look at the RMSE (w.r.t the observed input data) and CPU time.\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nkp = 0\nax1.plot(np.log10(np.cumsum(rpca.cpu_time_[kp:])), rpca.rmse_[kp:], 'o-', label = 'IALM - RPCA')\nax1.plot(np.log10(np.cumsum(nnm.cpu_time_[kp:])), nnm.rmse_[kp:], 'o-', label = 'FW - NNM')\nax1.plot(np.log10(np.cumsum(nnm_sparse.cpu_time_[kp:])), nnm_sparse.rmse_[kp:], 'o-', label = 'FW - NNML1')\nax1.legend()\n\nax1.set_xlabel(\"CPU Time\")\nax1.set_ylabel(\"RMSE from input matrix\")\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\nFigure 2: Comparison of time and accuracy of different methods"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#principal-components",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#principal-components",
    "title": "Comparison of different noise models",
    "section": "Principal Components",
    "text": "Principal Components\nCan the principal components of the low rank matrix help in classification?\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    return pcomps\n\npcomps_rpca = get_principal_components(rpca.L_)\npcomps_nnm = get_principal_components(nnm.X)\npcomps_nnm_sparse = get_principal_components(nnm_sparse.X)\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\nFigure 3: First 6 principal components compared against each other (FW-NNM).\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\nFigure 4: First 6 principal components compared against each other (IALM - RPCA).\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm_sparse, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\nFigure 5: First 6 principal components compared against each other (FW-NNML1)."
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#benchmark-stats",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#benchmark-stats",
    "title": "Comparison of different noise models",
    "section": "Benchmark stats",
    "text": "Benchmark stats\nWe look at different statistics to determine the efficiency of the methods. In particular:\n\nRMSE (root mean square error) with respect to the ground truth\nPSNR (peak signal-to-noise-ratio) with respect to the ground truth (frequently used in image denoising literature)\nAdjusted MI score measures how good the methods are in the classification task.\n\n\n\nCode\nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\nY_rpca_cent = mpy_simulate.do_standardize(rpca.L_, scale = False)\nY_nnm_cent  = mpy_simulate.do_standardize(nnm.X, scale = False)\nY_nnm_sparse_cent = mpy_simulate.do_standardize(nnm_sparse.X, scale = False)"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#rmse",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#rmse",
    "title": "Comparison of different noise models",
    "section": "RMSE",
    "text": "RMSE\n\n\nCode\nrmse_rpca = merr.get(Y_true_cent, Y_rpca_cent, method = 'rmse')\nrmse_nnm = merr.get(Y_true_cent, Y_nnm_cent, method = 'rmse')\nrmse_nnm_sparse = merr.get(Y_true_cent, Y_nnm_sparse_cent, method = 'rmse')\n\nprint (f\"{rmse_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{rmse_nnm_sparse:.4f}\\tSparse Nuclear Norm Minimization\")\nprint (f\"{rmse_rpca:.4f}\\tRobust PCA\")\n\n\n0.1602  Nuclear Norm Minimization\n0.1371  Sparse Nuclear Norm Minimization\n0.3972  Robust PCA"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#psnr",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#psnr",
    "title": "Comparison of different noise models",
    "section": "PSNR",
    "text": "PSNR\n\n\nCode\npsnr_rpca = merr.get(Y_true_cent, Y_rpca_cent, method = 'psnr')\npsnr_nnm = merr.get(Y_true_cent, Y_nnm_cent, method = 'psnr')\npsnr_nnm_sparse = merr.get(Y_true_cent, Y_nnm_sparse_cent, method = 'psnr')\n\nprint (f\"{psnr_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{psnr_nnm_sparse:.4f}\\tSparse Nuclear Norm Minimization\")\nprint (f\"{psnr_rpca:.4f}\\tRobust PCA\")\n\n\n19.6846 Nuclear Norm Minimization\n21.0394 Sparse Nuclear Norm Minimization\n11.7991 Robust PCA"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#adjusted-mi-score",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#adjusted-mi-score",
    "title": "Comparison of different noise models",
    "section": "Adjusted MI Score",
    "text": "Adjusted MI Score\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn import metrics as skmetrics\n\ndef get_adjusted_MI_score(pcomp, class_labels):\n    distance_matrix = skmetrics.pairwise.pairwise_distances(pcomp, metric='euclidean')\n    model = AgglomerativeClustering(n_clusters = 4, linkage = 'average', metric = 'precomputed')\n    class_pred = model.fit_predict(distance_matrix)\n    return skmetrics.adjusted_mutual_info_score(class_labels, class_pred)\n\nadjusted_mi_nnm = get_adjusted_MI_score(pcomps_nnm,   class_labels)\nadjusted_mi_nnm_sparse = get_adjusted_MI_score(pcomps_nnm_sparse, class_labels)\nadjusted_mi_rpca = get_adjusted_MI_score(pcomps_rpca, class_labels)\n\nprint (f\"{adjusted_mi_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{adjusted_mi_nnm_sparse:.4f}\\tSparse Nuclear Norm Minimization\")\nprint (f\"{adjusted_mi_rpca:.4f}\\tRobust PCA\")\n\n\n-0.0011 Nuclear Norm Minimization\n1.0000  Sparse Nuclear Norm Minimization\n0.0099  Robust PCA"
  },
  {
    "objectID": "notebooks/explore/robust_pca_cross_validation.html",
    "href": "notebooks/explore/robust_pca_cross_validation.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nfrom sklearn.decomposition import PCA\n\nfrom nnwmf.functions.robustpca import RobustPCA\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename  = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename  = f\"{data_dir}/prec_df.pkl\"\ntrait_df_filename = f\"{data_dir}/trait_meta.csv\"\n\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\nbeta_df = pd.read_pickle(beta_df_filename)\nprec_df = pd.read_pickle(prec_df_filename)\nse_df = prec_df.apply(lambda x : 1 / np.sqrt(x)).replace([np.inf, -np.inf], np.nan)\nzscore_df = beta_df / se_df\nzscore_df = zscore_df.replace(np.nan, 0)\n\ntrait_df = pd.read_csv(trait_df_filename)\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n\nCode\n\n'''\nFilter samples using mean of SE and std of beta\n'''\n\nmean_se = prec_df.apply(lambda x : 1 / np.sqrt(x)).replace([np.inf, -np.inf], np.nan).mean(axis = 0, skipna = True)\nmean_se = pd.DataFrame(mean_se).set_axis([\"mean_se\"], axis = 1)\nbeta_std = beta_df.std(axis = 0, skipna = True)\nbeta_std = pd.DataFrame(beta_std).set_axis([\"beta_std\"], axis = 1)\nerror_df = pd.concat([mean_se, beta_std], axis = 1)\nselect_ids = error_df.query(\"mean_se &lt;= 0.2 and beta_std &lt;= 0.2\").index\n\nX = np.array(zscore_df[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"After filtering, we have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\n\nAfter filtering, we have 69 samples (phenotypes) and 8403 features (variants)\n\n\n\n\nCode\nn, p = Xcent.shape\ntest_size = 0.33\nO = np.ones(n * p)\nntest = int(test_size * n * p)\nO[:ntest] = 0\nnp.random.shuffle(O)\nO = O.reshape(n, p)\n\nXmiss = X * O\n\n\n\n\nCode\ntrain_mask = O\ntest_mask = 1 - O\nXtrain = X * train_mask\nXtest = X * test_mask\n\n\n\n\nCode\ndef f_rmse(X, Y, mask = None):\n    '''\n    RMSE for masked CV\n    '''\n    Xmask = X if mask is None else X * mask\n    return np.sqrt(np.mean(np.square(Xmask - Y)))\n\n\n\n\nCode\nlmb_seq = np.logspace(-5, 5, 10)\nrpca = dict()\nL = dict()\nM = dict()\nerr_train = dict()\nerr_test = dict()\nfor i, lmb in enumerate(lmb_seq):\n    rpca[i] = RobustPCA(lmb=lmb, max_iter=1000)\n    L[i], M[i] = rpca[i].fit(Xmiss)\n    err_test[i] = f_rmse(X, M[i], mask = test_mask)\n    err_train[i] = f_rmse(X, L[i], mask = train_mask)\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(np.log(lmb_seq), err_train.values(), 'o-', label = \"Train_error\")\nax1.plot(np.log(lmb_seq), err_test.values(), 'o-', label = \"Test error\")\nax1.legend()\nplt.show()\n\n\n\n\n\n\n\nCode\nmin(err_test.items(), key=lambda x: x[1]) \n\n\n(5, 0.8008069891518402)\n\n\n\n\nCode\nlmb_seq[1]\n\n\n0.0001291549665014884"
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "",
    "text": "David proposed a convex optimization algorithm for nuclear norm minimization of weighted matrix factorization. This is a pilot implementation of the algorithm, following his implementation see here. I was not sure if the weighted matrix factorization was working. Therefore, I implemented the matrix completion to make sure that the algorithm is working to find the missing data.\nIn this proof-of-concept, we show that the NNWMF (we need a better acronym) indeed can find the missing data despite the noise in real data using an unit weight matrix.\nHere are some references I used for the Frank-Wolfe algorithm: - Martin Jaggi, “Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization”, PMLR 28(1):427-435, 2013 - Martin Jaggi and Marek Sulovská, “A Simple Algorithm for Nuclear Norm Regularized Problems”, ICML 2010 - Fabian Pedregosa, “Notes on the Frank-Wolfe Algorithm, Part I”, Personal Blog, 2018 - Moritz Hardt, Lecture Notes on “Convex Optimization and Approximation”, 2018\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.extmath import randomized_svd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')"
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html#simplex-projection",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html#simplex-projection",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "Simplex projection",
    "text": "Simplex projection\nProject the input matrix on nuclear norm ball, using an algorithm proposed by Duchi et. al. in “Efficient projections onto the l1-ball for learning in high dimensions”, Proc. 25th ICML, pages 272–279. ACM, 2008. This is computationally expensive but will help to compare the results from Frank-Wolfe algorithm.\n\n\nCode\nXproj = nuclear_projection(Xcent, r = 1000)\nXproj = Xproj - np.mean(Xproj, axis = 0, keepdims = True)\nU, S, Vt = np.linalg.svd(Xproj)\nprint(f\"Nuclear norm of projected input matrix is {np.sum(S)}\")\n\n\nNuclear norm of projected input matrix is 1000.0000000000019"
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html#frank-wolfe",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html#frank-wolfe",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "Frank-Wolfe",
    "text": "Frank-Wolfe\n\n\nCode\nX_opt, gs, fx = frank_wolfe_minimize(Xcent, weight, 1000.0, max_iter = 500, debug = False)\n\n\nIn Figure 2, we show the progress of the optimization. On the left hand side, we show the duality gap and on the right hand side, we show the objective function.\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nkp = 100\nax1.plot(np.arange(kp), gs[:kp])\nax2.plot(np.arange(kp), fx[:kp])\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\nFigure 2: Frank-Wolfe optimization statistic\n\n\n\n\n\n\nCode\nX_opt_cent = X_opt - np.mean(X_opt, axis = 0, keepdims = True)\nU_fw, S_fw, Vt_fw = np.linalg.svd(X_opt_cent)\n\n\nIn Figure 3, we compare the singular values of the output signal from the nuclear norm projection and the Frank-Wolfe algorithm. This gives us confidence with the implementation.\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nkp = 22\nax1.plot(np.arange(kp), S[:kp], 'o-', label = 'Simplex Projection')\nax1.plot(np.arange(kp), S_fw[:kp], 'o-', label = 'Frank-Wolfe')\n\nax1.legend()\nax1.set_xlabel(\"Component\")\nax1.set_ylabel(\"Eigenvalue (S)\")\nplt.show()\n\n\n\n\n\nFigure 3: Eigenvalues obtained using SVD of the projected input matrix"
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html#compare-the-principal-components-of-the-recovered-matrix",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html#compare-the-principal-components-of-the-recovered-matrix",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "Compare the principal components of the recovered matrix",
    "text": "Compare the principal components of the recovered matrix\n\n\nCode\nU_input, S_input, Vt_input = np.linalg.svd(Xcent, full_matrices=False)\npca_input = U_input @ np.diag(S_input)\n\n\n\n\nCode\nX_cvopt_cent = X_cvopt - np.mean(X_cvopt, axis = 0, keepdims = True)\nU_cvopt, S_cvopt, Vt_cvopt = np.linalg.svd(X_cvopt_cent, full_matrices = False)\npca_cvopt = U_cvopt @ np.diag(S_cvopt)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\n\nsvd_pc1 = pca_input[:, idx1]\nsvd_pc2 = pca_input[:, idx2]\nwmf_pc1 = pca_cvopt[:, idx1]\nwmf_pc2 = pca_cvopt[:, idx2]\n\nfig = plt.figure(figsize = (16, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.5, label = label)\n    ax2.scatter(wmf_pc1[idx], wmf_pc2[idx], s = 100, alpha = 0.5, label = label)\n    \nax2.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nax2.set_xlabel(f\"Component {idx1}\")\nax2.set_ylabel(f\"Component {idx2}\")\nax1.set_title(\"Raw input\", pad = 20.0)\nax2.set_title(\"Low rank approximation\", pad = 20.)\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\nFigure 8: Comparison of the first two principal components of input matrix and low rank approximation\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.7, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pca_cvopt)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\nFigure 9: Comparison of the first 6 principal components of the low rank approximation\n\n\n\n\n\n\nCode\ntrain_error\n\n\n{8.0: 1.0445314519131719,\n 16.0: 1.041037978310444,\n 32.0: 1.0342448061985816,\n 64.0: 1.0212031961118844,\n 128.0: 0.9980512728715728,\n 256.0: 0.9592552598273123,\n 512.0: 0.8958869142145245,\n 1024.0: 0.7931029154998555,\n 2048.0: 0.6284386790184624,\n 4096.0: 0.3992465647184987,\n 5000.0: 0.36700702863471724,\n 6000.0: 0.37273215971181584,\n 7000.0: 0.39743960536609935,\n 8000.0: 0.46060258805130466}\n\n\n\n\nCode\ntest_error\n\n\n{8.0: 0.7379283863527464,\n 16.0: 0.7360553648577052,\n 32.0: 0.7323992229046496,\n 64.0: 0.7252923894633748,\n 128.0: 0.7126499267407608,\n 256.0: 0.6929325616806638,\n 512.0: 0.6650417778754745,\n 1024.0: 0.6294795062102471,\n 2048.0: 0.5984243261174017,\n 4096.0: 0.5813444546875715,\n 5000.0: 0.5978681673912616,\n 6000.0: 0.6282368213911593,\n 7000.0: 0.6693131025636734,\n 8000.0: 0.6992722149861493}"
  },
  {
    "objectID": "notebooks/explore/weighted_nuclear_norm_minimization.html",
    "href": "notebooks/explore/weighted_nuclear_norm_minimization.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.extmath import randomized_svd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\nselect_ids = beta_df.columns\n\nX = np.array(zscore_df.replace(np.nan, 0)[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices = False)\nprint (f\"Nuclear Norm of input matrix: {np.sum(S)}\")\n\n\nNuclear Norm of input matrix: 7292.701186600059\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(np.arange(S.shape[0]), S)\nax1.set_xlabel(\"Component\")\nax1.set_ylabel(\"Eigenvalue (S)\")\nplt.show()\n\n\n\n\n\nFigure 1: Eigenvalues obtained using SVD of the input matrix\n\n\n\n\n\n# weight = np.array(prec_df[select_ids]).T * np.array(mean_se.loc[select_ids]) * np.array(mean_se.loc[select_ids])\n# weight = np.array(prec_df[select_ids]).T\nweight = np.ones(X.shape)\n\n\ndef nuclear_norm(X):\n    '''\n    Nuclear norm of input matrix\n    '''\n    return np.sum(np.linalg.svd(X)[1])\n\ndef f_objective(X, Y, W = None, mask = None):\n    '''\n    Objective function\n    Y is observed, X is estimated\n    W is the weight of each observation.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    # The * operator can be used as a shorthand for np.multiply on ndarrays.\n    if Wmask is None:\n        f_obj = 0.5 * np.linalg.norm(Y - Xmask, 'fro')**2\n    else:\n        f_obj = 0.5 * np.linalg.norm(Wmask * (Y - Xmask), 'fro')**2\n    return f_obj\n\n\ndef f_gradient(X, Y, W = None, mask = None):\n    '''\n    Gradient of the objective function.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    if Wmask is None:\n        f_grad = Xmask - Y\n    else:\n        f_grad = np.square(Wmask) * (Xmask - Y)\n    \n    return f_grad\n\n\ndef linopt_oracle(grad, r = 1.0, max_iter = 10):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a nuclear norm ball for some r\n    '''\n    U1, V1_T = singular_vectors_power_method(grad, max_iter = max_iter)\n    S = - r * U1 @ V1_T\n    return S\n\n\ndef singular_vectors_randomized_method(X, max_iter = 10):\n    u, s, vh = randomized_svd(X, n_components = 1, n_iter = max_iter,\n                              power_iteration_normalizer = 'none',\n                              random_state = 0)\n    return u, vh\n\n\ndef singular_vectors_power_method(X, max_iter = 10):\n    '''\n    Power method.\n        \n        Computes approximate top left and right singular vector.\n        \n    Parameters:\n    -----------\n        X : array {m, n},\n            input matrix\n        max_iter : integer, optional\n            number of steps\n            \n    Returns:\n    --------\n        u, v : (n, 1), (p, 1)\n            two arrays representing approximate top left and right\n            singular vectors.\n    '''\n    n, p = X.shape\n    u = np.random.normal(0, 1, n)\n    u /= np.linalg.norm(u)\n    v = X.T.dot(u)\n    v /= np.linalg.norm(v)\n    for _ in range(max_iter):      \n        u = X.dot(v)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)       \n    return u.reshape(-1, 1), v.reshape(1, -1)\n\n\ndef do_step_size(dg, D, W = None):\n    if W is None:\n        denom = np.linalg.norm(D, 'fro')**2\n    else:\n        denom = np.linalg.norm(W * D, 'fro')**2\n    step_size = dg / denom\n    step_size = min(step_size, 1.0)\n    if step_size &lt; 0:\n        print (\"Warning: Step Size is less than 0\")\n        step_size = 1.0\n    return step_size\n\n\ndef frank_wolfe_minimize_step(X, Y, r, istep, W = None, mask = None):\n    \n    # 1. Gradient for X_(t-1)\n    G = f_gradient(X, Y, W = W, mask = mask)\n    # 2. Linear optimization subproblem\n    power_iter = 10 + int(istep / 50)\n    S = linopt_oracle(G, r, max_iter = power_iter)\n    # 3. Define D\n    D = X - S\n    # 4. Duality gap\n    dg = np.trace(D.T @ G)\n    # 5. Step size\n    step = do_step_size(dg, D, W = W)\n    # 6. Update\n    Xnew = X - step * D\n    return Xnew, G, dg, step\n\n\ndef frank_wolfe_minimize(Y, r, X0 = None,\n                         weight = None,\n                         mask = None,\n                         max_iter = 1000, tol = 1e-8,\n                         return_all = True,\n                         debug = False, debug_step = 10):\n    \n    # Step 0\n    old_X = np.zeros_like(Y) if X0 is None else X0.copy()\n    dg = np.inf\n\n    if return_all:\n        dg_list = [dg]\n        fx_list = [f_objective(old_X, Y, W = weight, mask = mask)]\n        st_list = [1]\n        \n    # Steps 1, ..., max_iter\n    for istep in range(max_iter):\n        X, G, dg, step = \\\n            frank_wolfe_minimize_step(old_X, Y, r, istep, W = weight, mask = mask)\n        f_obj = f_objective(X, Y, W = weight, mask = mask)\n\n        if return_all:\n            dg_list.append(dg)\n            fx_list.append(f_obj)\n            st_list.append(step)\n        \n        if debug:\n            if (istep % debug_step == 0):\n                print (f\"Iteration {istep}. Step size {step:.3f}. Duality Gap {dg:g}\")\n        if np.abs(dg) &lt;= tol:\n            break\n            \n        old_X = X.copy()\n        \n    if return_all:\n        return X, dg_list, fx_list, st_list\n    else:\n        return X\n\n\n\nCode\nX_opt, dg_list, fx_list, step_list = frank_wolfe_minimize(Xcent, 100.0, max_iter = 10, debug = True, debug_step = 1)\n\n\nIteration 0. Step size 1.000. Duality Gap 47280.8\nWarning: Step Size is less than 0\nIteration 1. Step size 1.000. Duality Gap -0.17212\nWarning: Step Size is less than 0\nIteration 2. Step size 1.000. Duality Gap -9.51455\nWarning: Step Size is less than 0\nIteration 3. Step size 1.000. Duality Gap -11.2205\nIteration 4. Step size 0.386. Duality Gap 582.621\nIteration 5. Step size 1.000. Duality Gap 143.495\nIteration 6. Step size 0.671. Duality Gap 47.3667\nWarning: Step Size is less than 0\nIteration 7. Step size 1.000. Duality Gap -454.516\nIteration 8. Step size 0.529. Duality Gap 1893.78\nIteration 9. Step size 1.000. Duality Gap 419.05\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nkp = 10\nax1.plot(np.arange(kp - 2), dg_list[2:kp])\nax2.plot(np.arange(kp - 1), fx_list[1:kp])\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(X_opt)\n\n\n3\n\n\n\n\nCode\nnuclear_norm(X_opt)\n\n\n99.99956475737909\n\n\n\n\nCode\nX_opt_cent = X_opt - np.mean(X_opt, axis = 0, keepdims = True)\nU_fw, S_fw, Vt_fw = np.linalg.svd(X_opt_cent)\npca_fw = U_fw @ np.diag(S_fw)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\nsvd_pc1 = pca_fw[:, idx1]\nsvd_pc2 = pca_fw[:, idx2]\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/simplex-projection-methods.html",
    "href": "notebooks/explore/simplex-projection-methods.html",
    "title": "About",
    "section": "",
    "text": "Comparison of different algorithms for projection on \\ell_1 ball.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\nv_orig = np.random.normal(0, 1, 10)\n\n\n\n\nCode\n# def proj_l1ball_sort(y, a):\n#     if np.sum(y) == a and np.alltrue(y &gt;= 0):\n#         return y\n#     yabs = np.abs(y)\n#     u = np.sort(yabs)[::-1]\n#     ukvals = (np.cumsum(u) - a) / np.arange(1, y.shape[0] + 1)\n#     K = np.max(np.where(ukvals &lt; u))\n#     tau = ukvals[K]\n#     x = np.sign(y) * np.clip(yabs - tau, a_min=0, a_max=None)\n#     return x\n\ndef proj_simplex_sort(y, a = 1.0):\n    if np.sum(y) == a and np.alltrue(y &gt;= 0):\n        return y\n    u = np.sort(y)[::-1]\n    ukvals = (np.cumsum(u) - a) / np.arange(1, y.shape[0] + 1)\n    K = np.nonzero(ukvals &lt; u)[0][-1]\n    tau = ukvals[K]\n    x = np.clip(y - tau, a_min=0, a_max=None)\n    return x\n\ndef proj_l1ball_sort(y, a = 1.0):\n    return np.sign(y) * proj_simplex_sort(np.abs(y), a = a)\n\ndef l1_norm(x):\n    return np.sum(np.abs(x))\n\n\n\n\nCode\nl1_norm(proj_l1ball_sort(v_orig, 1.0))\n\n\n1.0\n\n\n\n\nCode\nl1_norm(v_orig)\n\n\n7.817970379144475\n\n\n\n\nCode\nproj_l1ball_sort(v_orig, 1.0)\n\n\narray([-0.00168506, -0.        ,  0.        ,  0.        ,  0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.99831494])\n\n\n\n\nCode\ndef proj_simplex_michelot(y, a = 1.0):\n    auxv = y.copy()\n    N = y.shape[0]\n    rho = (np.sum(y) - a) / N\n    istep = 0\n    vnorm_last = l1_norm(auxv)\n    while True:\n        istep += 1\n        allowed = auxv &gt; rho\n        auxv = auxv[allowed]\n        nv = np.sum(allowed)\n        vnorm = l1_norm(auxv)\n        if vnorm == vnorm_last:\n            break\n        rho = (np.sum(auxv) - a) / nv\n        vnorm_last = vnorm\n    x = np.clip(y - rho, a_min = 0, a_max = None)\n    return x\n    \ndef proj_l1_michelot(y, a = 1.0):\n    return np.sign(y) * proj_simplex_michelot(np.abs(y), a)\n\nproj_l1_michelot(v_orig)\n\n\narray([-0.00168506, -0.        ,  0.        ,  0.        ,  0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.99831494])\n\n\n\n\nCode\ndef proj_simplex_condat(y, a = 1.0):\n    auxv = np.array([y[0]])\n    vtilde = np.array([])\n    rho = y[0] - a\n    N = y.shape[0]\n    # Step 2\n    for i in range(1, N):\n        if y[i] &gt; rho:\n            rho += (y[i] - rho) / (auxv.shape[0] + 1)\n            if rho &gt; (y[i] - a):\n                auxv = np.append(auxv, y[i])\n            else:\n                vtilde = np.append(vtilde, auxv)\n                auxv = np.array([y[i]])\n                rho = y[i] - a\n    # Step 3\n    if vtilde.shape[0] &gt; 0:\n        for v in vtilde:\n            if v &gt; rho:\n                auxv = np.append(auxv, v)\n                rho += (v - rho) / (auxv.shape[0])                \n    # Step 4\n    nv_last = auxv.shape[0]\n    istep = 0\n    while True:\n        istep += 1\n        to_remove = list()\n        nv_ = auxv.shape[0]\n        for i, v in enumerate(auxv):\n            if v &lt;= rho:\n                to_remove.append(i)\n                nv_ = nv_ - 1\n                rho += (rho - v) / nv_\n        auxv = np.delete(auxv, to_remove)\n        nv = auxv.shape[0]\n        assert nv == nv_\n        if nv == nv_last:\n            break\n        nv_last = nv\n    # Step 5\n    x = np.clip(y - rho, a_min=0, a_max=None)\n    return x\n\ndef proj_l1_condat(y, a = 1.0):\n    return np.sign(y) * proj_simplex_condat(np.abs(y), a)\n\nproj_l1_condat(v_orig)\n\n\narray([-0.00168506, -0.        ,  0.        ,  0.        ,  0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.99831494])\n\n\n\n\nCode\n%%timeit\n\nproj_l1_condat(v_orig)\n\n\n84.8 µs ± 2.24 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nCode\n%%timeit\nproj_l1ball_sort(v_orig)\n\n\n56.4 µs ± 1.21 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nCode\nv_orig2 = np.random.normal(0, 1, 100000)\n\n\n\n\nCode\n%%timeit -n 1 -r 1\nproj_l1_condat(v_orig2)\n\n\n29.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\nCode\n%%timeit -n 1 -r 1\nproj_l1ball_sort(v_orig2)\n\n\n12.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)"
  },
  {
    "objectID": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html",
    "href": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html",
    "title": "Choosing step size for Inexact ALM algorithm",
    "section": "",
    "text": "There are two alternate proposed methods for updating the penalty \\mu on the residual. The penalty is equivalent to the step size. Here, we compare the two methods on a simulated dataset. The proposed methods are:\n\nEq. 25 of Lin et. al.\nEq. 3.13 of Boyd et. al.\n\nTo-Do: Check stopping criterion of Boyd et. al. using Eq. 3.12."
  },
  {
    "objectID": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#default",
    "href": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#default",
    "title": "Choosing step size for Inexact ALM algorithm",
    "section": "Default",
    "text": "Default\n\n\nCode\nY_rpca_cent = mpy_simulate.do_standardize(rpca.L_, scale = False)\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Y_rpca_cent, full_matrices = False)\npcomps_rpca = U_rpca @ np.diag(S_rpca)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#admm",
    "href": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#admm",
    "title": "Choosing step size for Inexact ALM algorithm",
    "section": "ADMM",
    "text": "ADMM\n\n\nCode\nY_rpca_cent = mpy_simulate.do_standardize(rpca_admm.L_, scale = False)\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Y_rpca_cent, full_matrices = False)\npcomps_rpca = U_rpca @ np.diag(S_rpca)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-07-05-simulation-setup.html",
    "href": "notebooks/explore/2023-07-05-simulation-setup.html",
    "title": "Simulation setup for benchmarking matrix factorization methods",
    "section": "",
    "text": "About\nHere, I check if the simulation benchmarking makes sense using simple examples. The idea is to run large scale simulations using pipelines. Before that, I want to look at simple examples.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe_CV\nfrom nnwmf.optimize import FrankWolfe\n\n\n\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\nCode\nsample_dict = mpy_simulate.get_sample_indices(ntrait, ngwas, shuffle = False)\nsample_indices = [x for _, x in sample_dict.items()]\nunique_labels  = [k for k, _ in sample_dict.items()]\nclass_labels = [None for x in range(ngwas)]\nfor k, x in sample_dict.items():\n    for i in x:\n        class_labels[i] = k\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, sample_groups = sample_indices, std = 1.0)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\n\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nTrue components\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(L, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\nTruncated SVD\n\n\nCode\ndef truncated_SVD(X, thres = 0.9):\n    U, S, Vt = np.linalg.svd(X, full_matrices = False)\n    k = np.where(np.cumsum(S / np.sum(S)) &gt;= thres)[0][0]\n    pcomps = U[:, :k] @ np.diag(S[:k])\n    return U, S, Vt, pcomps\n\n_, S_tsvd, _, pcomps_tsvd = truncated_SVD(Y_cent)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_tsvd, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\nCode\nS2 = np.square(S_tsvd)\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(np.arange(S2.shape[0]), np.cumsum(S2 / np.sum(S2)), 'o-')\nplt.show()\n\n\n\n\n\n\n\nNuclear Norm Minimization using Frank-Wolfe algorithm\n\n\nCode\nnnmcv = FrankWolfe_CV(chain_init = True, reverse_path = False, kfolds = 2)\nnnmcv.fit(Y_cent)\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(2):\n    #ax1.plot(np.log10(list(nnmcv.training_error.keys())), [x[k] for x in nnmcv.training_error.values()], 'o-')\n    ax1.plot(np.log10(list(nnmcv.test_error.keys())), [x[k] for x in nnmcv.test_error.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\nCode\nr_opt = 32.0\n\nnnm = FrankWolfe(show_progress = True, svd_max_iter = 50, debug = True, suppress_warnings = True)\nnnm.fit(Y_cent, r_opt)\n\nY_nnm_cent = mpy_simulate.do_standardize(nnm.X, scale = False)\nU_nnm, S_nnm, Vt_nnm = np.linalg.svd(Y_nnm_cent, full_matrices = False)\npcomps_nnm = U_nnm @ np.diag(S_nnm)\n\n\n2023-07-28 11:16:31,147 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 2742.83\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\nWeighted Nuclear Norm Minimization\n\n\nCode\nsnp_weights = 1 / np.sqrt(noise_var)\nweight = np.column_stack([snp_weights for _ in range(ngwas)]).T\n\nwnnmcv = FrankWolfe_CV(chain_init = True, reverse_path = False, kfolds = 5, debug = True)\nwnnmcv.fit(Y_cent, weight = weight)\n\n\n2023-07-28 11:16:55,580 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Cross-validation over 15 ranks.\n2023-07-28 11:16:55,604 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 1 ...\n2023-07-28 11:17:13,426 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 2 ...\n2023-07-28 11:17:35,075 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 3 ...\n2023-07-28 11:17:57,035 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 4 ...\n2023-07-28 11:18:16,387 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 5 ...\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(5):\n    #ax1.plot(np.log10(list(nnmcv.training_error.keys())), [x[k] for x in nnmcv.training_error.values()], 'o-')\n    ax1.plot(np.log10(list(wnnmcv.test_error.keys())), [x[k] for x in wnnmcv.test_error.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\nCode\nr_opt = 32.0\n\nwnnm = FrankWolfe(show_progress = True, svd_max_iter = 50, debug = True)\nwnnm.fit(Y_cent, r_opt, weight = weight)\n\nY_wnnm_cent = mpy_simulate.do_standardize(wnnm.X, scale = False)\nU_wnnm, S_wnnm, Vt_wnnm = np.linalg.svd(Y_wnnm_cent, full_matrices = False)\npcomps_wnnm = U_wnnm @ np.diag(S_wnnm)\n\n\n2023-07-28 11:18:52,909 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 0.154. Duality Gap 7.2392e+06\n2023-07-28 11:18:55,519 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.003. Duality Gap 100806\n2023-07-28 11:18:58,015 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.003. Duality Gap 42452.7\n2023-07-28 11:19:00,500 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.002. Duality Gap 31460.9\n2023-07-28 11:19:02,997 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 400. Step size 0.005. Duality Gap 20861.9\n2023-07-28 11:19:05,500 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 500. Step size 0.004. Duality Gap 16232.8\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_wnnm, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\nRobust PCA\n\n\nCode\nrpca = IALM()\nrpca.fit(Y_cent)\nnp.linalg.matrix_rank(rpca.L_)\n\n\nNameError: name 'lmb' is not defined\n\n\n\n\nCode\nY_rpca_cent = mpy_simulate.do_standardize(L_rpca, scale = False)\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Y_rpca_cent, full_matrices = False)\npcomps_rpca = U_rpca @ np.diag(S_rpca)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\nCode\ndef get_rmse(original, recovered, mask = None):\n    if mask is None:\n        mask = np.ones(original.shape)\n    n = np.sum(mask)\n    mse = np.sum(np.square((original - recovered) * mask)) / n\n    return np.sqrt(mse)\n    \nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\nY_rpca_cent = mpy_simulate.do_standardize(L_rpca, scale = False)\n\nrmse_nnm = get_rmse(Y_true_cent, Y_nnm_cent)\nrmse_wnnm = get_rmse(Y_true_cent, Y_wnnm_cent)\nrmse_rpca = get_rmse(Y_true_cent, Y_rpca_cent)\n\nprint (f\"{rmse_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{rmse_wnnm:.4f}\\tWeighted Nuclear Norm Minimization\")\nprint (f\"{rmse_rpca:.4f}\\tRobust PCA\")\n\n\n0.1448  Nuclear Norm Minimization\n0.1427  Weighted Nuclear Norm Minimization\n0.1054  Robust PCA\n\n\n\n\nCode\ndef get_psnr(original, recovered):\n    n, p = original.shape\n    maxsig2 = np.square(np.max(original) - np.min(original))\n    mse = np.sum(np.square(recovered - original)) / (n * p)\n    res = 10 * np.log10(maxsig2 / mse)\n    return res\n\npsnr_nnm = get_psnr(Y_true_cent, Y_nnm_cent)\npsnr_wnnm = get_psnr(Y_true_cent, Y_wnnm_cent)\npsnr_rpca = get_psnr(Y_true_cent, Y_rpca_cent)\n\nprint (f\"{psnr_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{psnr_wnnm:.4f}\\tWeighted Nuclear Norm Minimization\")\nprint (f\"{psnr_rpca:.4f}\\tRobust PCA\")\n\n\n21.2372 Nuclear Norm Minimization\n21.3642 Weighted Nuclear Norm Minimization\n23.9949 Robust PCA\n\n\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn import metrics as skmetrics\n\ndef get_adjusted_MI_score(pcomp, class_labels):\n    distance_matrix = skmetrics.pairwise.pairwise_distances(pcomp, metric='euclidean')\n    model = AgglomerativeClustering(n_clusters = 4, linkage = 'average', metric = 'precomputed')\n    class_pred = model.fit_predict(distance_matrix)\n    return skmetrics.adjusted_mutual_info_score(class_labels, class_pred)\n\nadjusted_mi_nnm = get_adjusted_MI_score(pcomps_nnm,   class_labels)\nadjusted_mi_wnnm = get_adjusted_MI_score(pcomps_wnnm, class_labels)\nadjusted_mi_rpca = get_adjusted_MI_score(pcomps_rpca, class_labels)\n\nprint (f\"{adjusted_mi_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{adjusted_mi_wnnm:.4f}\\tWeighted Nuclear Norm Minimization\")\nprint (f\"{adjusted_mi_rpca:.4f}\\tRobust PCA\")\n\n\n-0.0006 Nuclear Norm Minimization\n1.0000  Weighted Nuclear Norm Minimization\n1.0000  Robust PCA"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This is a collection of all Jupyter notebooks for this project. It may be easier to navigate through the subcategories of the navigation.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\nundefined\n\n\nundefined\n\n\n\n\n23-08-02\n\n\nComparison of different noise models\n\n\n\n\n23-07-29\n\n\nPython implementation of Frank-Wolfe algorithm for NNM with sparse penalty\n\n\n\n\n23-07-28\n\n\nChoosing step size for Inexact ALM algorithm\n\n\n\n\n23-07-24\n\n\nPython Class for cross-validation of NNM-FW\n\n\n\n\n23-07-19\n\n\nPython Class for NNM using Frank-Wolfe algorithm\n\n\n\n\n23-07-05\n\n\nSimulation setup for benchmarking matrix factorization methods\n\n\n\n\n23-07-01\n\n\nHow to simulate ground truth for multi-phenotype z-scores?\n\n\n\n\n23-06-23\n\n\nMetrices for evaluating clusters given true labels\n\n\n\n\n23-06-05\n\n\nCan the low rank approximation capture the distinct GWAS phenotypes?\n\n\n\n\n23-05-23\n\n\nNuclear norm regularization using Frank-Wolfe algorithm\n\n\n\n\n23-05-16\n\n\nPCA of NPD summary statistics\n\n\n\n\n23-05-16\n\n\nRobust PCA implementation\n\n\n\n\n23-05-12\n\n\nPreprocess NPD summary statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notebooks/develop/2023-07-29-frank-wolfe-nnm-sparse.html",
    "href": "notebooks/develop/2023-07-29-frank-wolfe-nnm-sparse.html",
    "title": "Python implementation of Frank-Wolfe algorithm for NNM with sparse penalty",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_true)\n\n\n40\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_cent)\n\n\n499\n\n\n\n\nCode\nnp.linalg.norm(Y_cent, ord = 'nuc')\n\n\n6932.244652345258\n\n\n\n\nCode\nnp.linalg.norm(Y_std, ord = 'nuc')\n\n\n13368.766978646658\n\n\n\n\nCode\nnp.linalg.norm(Y_true, ord = 'nuc')\n\n\n584.4221920551713\n\n\n\n\nCode\nidx = np.unravel_index(np.argmax(Y_true), Y_true.shape)\n\n\n\n\nCode\nnp.max(Y_true)\n\n\n0.8948086870272697\n\n\n\n\nCode\nY_true[idx]\n\n\n0.8948086870272697\n\n\n\n\nCode\nfrom sklearn.utils.extmath import randomized_svd\n\ndef proj_simplex_sort(y, a = 1.0):\n    if np.sum(y) == a and np.alltrue(y &gt;= 0):\n        return y\n    u = np.sort(y)[::-1]\n    ukvals = (np.cumsum(u) - a) / np.arange(1, y.shape[0] + 1)\n    K = np.nonzero(ukvals &lt; u)[0][-1]\n    tau = ukvals[K]\n    x = np.clip(y - tau, a_min=0, a_max=None)\n    return x\n\ndef proj_l1ball_sort(y, a = 1.0):\n    if y.ndim == 2:\n        n, p = y.shape\n        yflat = y.flatten()\n        yproj = np.sign(y) * proj_simplex_sort(np.abs(yflat), a = a).reshape(n, p)\n    else:\n        yproj = np.sign(y) * proj_simplex_sort(np.abs(yflat), a = a).reshape(n, p)\n    return yproj\n\ndef l1_norm(x):\n    return np.sum(np.abs(x))\n\ndef nuclear_norm(X):\n    '''\n    Nuclear norm of input matrix\n    '''\n    return np.sum(np.linalg.svd(X)[1])\n\ndef f_objective(X, Y, W = None, mask = None):\n    '''\n    Objective function\n    Y is observed, X is estimated\n    W is the weight of each observation.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    # The * operator can be used as a shorthand for np.multiply on ndarrays.\n    if Wmask is None:\n        f_obj = 0.5 * np.linalg.norm(Y - Xmask, 'fro')**2\n    else:\n        f_obj = 0.5 * np.linalg.norm(Wmask * (Y - Xmask), 'fro')**2\n    return f_obj\n\n\ndef f_gradient(X, Y, W = None, mask = None):\n    '''\n    Gradient of the objective function.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    if Wmask is None:\n        f_grad = Xmask - Y\n    else:\n        f_grad = np.square(Wmask) * (Xmask - Y)\n    \n    return f_grad\n\ndef linopt_oracle_l1norm(grad, r = 1.0, max_iter = 10):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a l1 norm ball for some r\n    '''\n    maxidx = np.unravel_index(np.argmax(grad), grad.shape)\n    S = np.zeros_like(grad)\n    S[maxidx] = - r\n    return S\n\n\ndef linopt_oracle_nucnorm(grad, r = 1.0, max_iter = 10, method = 'power'):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a nuclear norm ball for some r\n    '''\n    if method == 'power':\n        U1, V1_T = singular_vectors_power_method(grad, max_iter = max_iter)\n    elif method == 'randomized':\n        U1, V1_T = singular_vectors_randomized_method(grad, max_iter = max_iter)\n    S = - r * U1 @ V1_T\n    return S\n\n\ndef singular_vectors_randomized_method(X, max_iter = 10):\n    u, s, vh = randomized_svd(X, n_components = 1, n_iter = max_iter,\n                              power_iteration_normalizer = 'none',\n                              random_state = 0)\n    return u, vh\n\n\ndef singular_vectors_power_method(X, max_iter = 10):\n    '''\n    Power method.\n        \n        Computes approximate top left and right singular vector.\n        \n    Parameters:\n    -----------\n        X : array {m, n},\n            input matrix\n        max_iter : integer, optional\n            number of steps\n            \n    Returns:\n    --------\n        u, v : (n, 1), (p, 1)\n            two arrays representing approximate top left and right\n            singular vectors.\n    '''\n    n, p = X.shape\n    u = np.random.normal(0, 1, n)\n    u /= np.linalg.norm(u)\n    v = X.T.dot(u)\n    v /= np.linalg.norm(v)\n    for _ in range(max_iter):      \n        u = X.dot(v)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)       \n    return u.reshape(-1, 1), v.reshape(1, -1)\n\n\ndef do_step_size(dg, D, W = None, old_step = None):\n    if W is None:\n        denom = np.linalg.norm(D, 'fro')**2\n    else:\n        denom = np.linalg.norm(W * D, 'fro')**2\n    step_size = dg / denom\n    step_size = min(step_size, 1.0)\n    if step_size &lt; 0:\n        print (\"Warning: Step Size is less than 0\")\n        if old_step is not None and old_step &gt; 0:\n            print (\"Using previous step size\")\n            step_size = old_step\n        else:\n            step_size = 1.0\n    return step_size\n\n\ndef frank_wolfe_minimize_step(L, M, Y, rl, rm, istep, W = None, mask = None, old_step = None, svd_iter = None, svd_method = 'power'):\n    #\n    # 1. Gradient for X_(t-1)\n    #\n    G = f_gradient(L + M, Y, W = W, mask = mask)\n    #\n    # 2. Linear optimization subproblem\n    #\n    if svd_iter is None: \n        svd_iter = 10 + int(istep / 20)\n        svd_iter = min(svd_iter, 25)\n    SL = linopt_oracle_nucnorm(G, rl, max_iter = svd_iter, method = svd_method)\n    SM = linopt_oracle_l1norm(G, rm)\n    #\n    # 3. Define D\n    #\n    DL = L - SL\n    DM = M - SM\n    #\n    # 4. Duality gap\n    #\n    dg = np.trace(DL.T @ G) + np.trace(DM.T @ G)\n    #\n    # 5. Step size\n    #\n    step = do_step_size(dg, DL + DM, W = W, old_step = old_step)\n    #\n    # 6. Update\n    #\n    Lnew = L - step * DL\n    Mnew = M - step * DM\n    #\n    # 7. l1 projection\n    #\n    G_half = f_gradient(Lnew + Mnew, Y, W = W, mask = mask)\n    Mnew = proj_l1ball_sort(Mnew - G_half, rm)\n    return Lnew, Mnew, G, dg, step\n\n\ndef frank_wolfe_minimize(Y, r, X0 = None,\n                         weight = None,\n                         mask = None,\n                         max_iter = 1000,\n                         svd_iter = None,\n                         svd_method = 'power',\n                         tol = 1e-4, step_tol = 1e-3, rel_tol = 1e-8,\n                         return_all = True,\n                         debug = False, debug_step = 10):\n    \n    rl, rm = r\n    \n    # Step 0\n    old_L = np.zeros_like(Y) if X0 is None else X0.copy()\n    old_M = np.zeros_like(Y)\n    dg = np.inf\n    step = 1.0\n\n    if return_all:\n        dg_list = [dg]\n        fx_list = [f_objective(old_L + old_M, Y, W = weight, mask = mask)]\n        fl_list = [f_objective(old_L, Y, W = weight, mask = mask)]\n        fm_list = [f_objective(old_M, Y, W = weight, mask = mask)]\n        st_list = [1]\n        \n    # Steps 1, ..., max_iter\n    for istep in range(max_iter):\n        L, M, G, dg, step = \\\n            frank_wolfe_minimize_step(old_L, old_M, Y, rl, rm, istep, W = weight, mask = mask, old_step = step, svd_iter = svd_iter, svd_method = svd_method)\n        f_obj = f_objective(L + M, Y, W = weight, mask = mask)\n        fl_obj = f_objective(L, Y, W = weight, mask = mask)\n        fm_obj = f_objective(M, Y, W = weight, mask = mask)\n        fx_list.append(f_obj)\n        fl_list.append(fl_obj)\n        fm_list.append(fm_obj)\n\n        if return_all:\n            dg_list.append(dg)\n            st_list.append(step)\n        \n        if debug:\n            if (istep % debug_step == 0):\n                print (f\"Iteration {istep}. Step size {step:.3f}. Duality Gap {dg:g}\")\n                \n        # Stopping criteria\n        # duality gap\n        if np.abs(dg) &lt;= tol:\n            break\n        # step size\n        if step &gt; 0 and step &lt;= step_tol:\n            break\n        # relative tolerance of objective function\n        f_rel = np.abs((f_obj - fx_list[-2]) / f_obj)\n        if f_rel &lt;= rel_tol:\n            break\n            \n        old_L = L.copy()\n        old_M = M.copy()\n        \n    if return_all:\n        return L, M, dg_list, fx_list, st_list, fl_list, fm_list\n    else:\n        return L, M\n\n\n\n\nCode\nL_opt, M_opt, dg_list, fx_list, step_list, fl_list, fm_list = \\\n    frank_wolfe_minimize(\n        Y_cent, (40.0, 10.0), max_iter = 1000, debug = True, debug_step = 100, step_tol = 1e-4, svd_iter=20)\n\n\nIteration 0. Step size 1.000. Duality Gap 3001.78\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\n\n\n\n\nCode\nl1_norm(M_opt)\n\n\n10.000000000000005\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\ndef relative_diff_log10(xlist):\n    x_arr = np.array(xlist)\n    x_ = np.log10(np.abs(np.diff(x_arr) / x_arr[1:]))\n    return x_\n\nkp = len(step_list)\n\nax1.plot(np.arange(kp - 2), dg_list[2:kp])\nax1.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\n\n# ax2.plot(np.arange(kp - 1), np.log10(fx_list[1:kp]))\nax2.plot(np.arange(kp - 1), relative_diff_log10(fx_list), label = \"L + M\")\nax2.plot(np.arange(kp - 1), relative_diff_log10(fl_list), label = \"L\")\nax2.plot(np.arange(kp - 1), relative_diff_log10(fm_list), label = \"M\")\nax2.set_xlabel(\"Number of iterations\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nax2.legend()\n\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\nCode\nU1, S1, Vt1 = np.linalg.svd(mpy_simulate.do_standardize(L_opt, scale = False), full_matrices=False)\nU2, S2, Vt2 = np.linalg.svd(mpy_simulate.do_standardize(M_opt, scale = False), full_matrices=False)\n\nfig = plt.figure(figsize = (18, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, U1 @ np.diag(S1), vmax = 0.01)\nmpy_plotfn.plot_covariance_heatmap(ax2, U2 @ np.diag(S2), vmax = 0.0001)\n\nplt.tight_layout(w_pad=2.0)\nplt.show()"
  },
  {
    "objectID": "notebooks/develop/2023-07-24-nnmfw-cross-validation-python-class.html",
    "href": "notebooks/develop/2023-07-24-nnmfw-cross-validation-python-class.html",
    "title": "Python Class for cross-validation of NNM-FW",
    "section": "",
    "text": "About\nI mask random elements of the input matrix and perform matrix completion using NNMFW. The minimum error for the masked elements in the recovered matrix is used for cross-validation.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import NNMFW_CV\nfrom nnwmf.optimize import NNMFW\n\n\n\n\nSimulate\n\n\nCode\nntrait = 4 # categories / class\nngwas  = 50 # N\nnsnp   = 100 # P\nnfctr  = 40 # K\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\n\n\n\n\nNNMFW\n\n\nCode\nnnm = NNMFW(show_progress = True, svd_max_iter = 50, debug = True)\nnnm.fit(Y_cent, 40.0)\n\n\n2023-07-24 16:02:19,996 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 0.554. Duality Gap 886.279\n2023-07-24 16:02:20,308 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.007. Duality Gap 11.7604\n2023-07-24 16:02:20,597 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-07-24 16:02:20,610 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.004. Duality Gap 6.56283\n2023-07-24 16:02:20,636 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n\n\n\n\nCode\nnnm._convergence_msg\n\n\n'Step size converged below tolerance.'\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nkp = len(nnm._st_list)\nfx_arr = np.array(nnm._fx_list)\nfx_rel_diff_log10 = np.log10(np.abs(np.diff(fx_arr) / fx_arr[1:]))\n\nax1.plot(np.arange(kp - 2), nnm._dg_list[2:kp])\n# ax2.plot(np.arange(kp - 1), np.log10(fx_list[1:kp]))\nax2.plot(np.arange(kp - 1), fx_rel_diff_log10)\nax3.plot(np.arange(kp), nnm._st_list)\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\nFunctions for cross-validation\n\n\nCode\ndef psnr(original, recovered):\n    n, p = original.shape\n    maxsig2 = np.square(np.max(original) - np.min(original))\n    mse = np.sum(np.square(recovered - original)) / (n * p)\n    res = 10 * np.log10(maxsig2 / mse)\n    return res\n\ndef masked_rmse(original, recovered, mask):\n    n = np.sum(mask)\n    mse = np.sum(np.square((original - recovered) * mask)) / n\n    return np.sqrt(mse)\n\ndef generate_rseq(Y):\n    r_min = 1\n    r_max = np.linalg.norm(Y, 'nuc')\n    nseq  = int(np.floor(np.log2(r_max)) + 1) + 1\n    r_seq = np.logspace(0, nseq - 1, num = nseq, base = 2.0)\n    return r_seq\n\ndef generate_mask(Y, folds = 1, test_size = 0.33):\n    n, p = Y.shape\n    O = np.ones(n * p)\n    ntest = int(test_size * n * p)\n    O[:ntest] = 0\n    np.random.shuffle(O)\n    return O.reshape(n, p) == 0\n\ndef generate_fold_labels(Y, folds = 2, test_size = None, shuffle = True):\n    n, p = Y.shape\n    fold_labels = np.ones(n * p)\n    ntest = int ((n * p) / folds) if test_size is None else int(test_size * n * p)\n    for k in range(1, folds):\n        start = k * ntest\n        end = (k + 1) * ntest\n        fold_labels[start: end] = k + 1\n    if shuffle:\n        np.random.shuffle(fold_labels)\n    return fold_labels.reshape(n, p)\n\ndef generate_masked_input(Y, mask):\n    Ymiss_nan = Y.copy()\n    Ymiss_nan[mask] = np.nan\n    Ymiss_nan_cent = Ymiss_nan - np.nanmean(Ymiss_nan, axis = 0, keepdims = True)\n    Ymiss_nan_cent[mask] = 0.0\n    return Ymiss_nan_cent\n\ndef nnmfw_cv(Y, folds = 2, r_seq = None):\n    if r_seq is None:\n        r_seq = generate_rseq(Y)\n    rmse_dict = {r: list() for r in r_seq}\n    fold_labels = generate_fold_labels(Y, folds = folds)\n    for fold in range(folds):\n        mask = fold_labels == fold + 1\n        Ymiss = generate_masked_input(Y, mask)\n        for r in r_seq:\n            nnm_cv = NNMFW(suppress_warnings = True)\n            nnm_cv.fit(Ymiss, r, mask = mask)\n            rmse = masked_rmse(Y, nnm_cv._X, mask)\n            rmse_dict[r].append(rmse)\n    return rmse_dict\n\n\n\n\nCode\nrmse_dict = nnmfw_cv(Y_cent, folds = 5)\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(4):\n    ax1.plot(np.log10(list(rmse_dict.keys())), [x[k] for x in rmse_dict.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\nCode\nmean_err = {k: np.mean(v) for k,v in rmse_dict.items()}\n\n\n\n\nCode\nmin(mean_err, key=mean_err.get)\n\n\n64.0\n\n\n\n\nClass for cross-validation\n\n\nCode\nnnmcv = NNMFW_CV(chain_init = True, reverse_path = False, debug = True, kfolds = 2)\nnnmcv.fit(Y_cent)\n\n\n2023-07-24 16:02:56,005 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Cross-validation over 10 ranks.\n2023-07-24 16:02:56,007 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 1 ...\n2023-07-24 16:02:58,362 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 2 ...\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(2):\n    #ax1.plot(np.log10(list(nnmcv.training_error.keys())), [x[k] for x in nnmcv.training_error.values()], 'o-')\n    ax1.plot(np.log10(list(nnmcv.test_error.keys())), [x[k] for x in nnmcv.test_error.values()], 'o-')\n    ax1.plot(np.log10(list(rmse_dict.keys())), [x[k] for x in rmse_dict.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()"
  },
  {
    "objectID": "meetings/index.html",
    "href": "meetings/index.html",
    "title": "Meetings",
    "section": "",
    "text": "Notes from NPD meetings."
  }
]