[
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "References",
    "section": "",
    "text": "Grotzinger, Andrew D., Mijke Rhemtulla, Ronald de Vlaming, Stuart J. Ritchie, Travis T. Mallard, W. David Hill, Hill F. Ip, et al. 2019. “Genomic Structural Equation Modelling Provides Insights into the Multivariate Genetic Architecture of Complex Traits.” Nature Human Behaviour 3 (5): 513–25. https://doi.org/10.1038/s41562-019-0566-x.\n\n\nTanigawa, Yosuke, Jiehan Li, Johanne M. Justesen, Heiko Horn, Matthew Aguirre, Christopher DeBoever, Chris Chang, et al. 2019. “Components of Genetic Associations Across 2,138 Phenotypes in the UK Biobank Highlight Adipocyte Biology.” Nature Communications 10 (1): 4064. https://doi.org/10.1038/s41467-019-11953-9."
  },
  {
    "objectID": "reference/index.html#methods",
    "href": "reference/index.html#methods",
    "title": "References",
    "section": "",
    "text": "Grotzinger, Andrew D., Mijke Rhemtulla, Ronald de Vlaming, Stuart J. Ritchie, Travis T. Mallard, W. David Hill, Hill F. Ip, et al. 2019. “Genomic Structural Equation Modelling Provides Insights into the Multivariate Genetic Architecture of Complex Traits.” Nature Human Behaviour 3 (5): 513–25. https://doi.org/10.1038/s41562-019-0566-x.\n\n\nTanigawa, Yosuke, Jiehan Li, Johanne M. Justesen, Heiko Horn, Matthew Aguirre, Christopher DeBoever, Chris Chang, et al. 2019. “Components of Genetic Associations Across 2,138 Phenotypes in the UK Biobank Highlight Adipocyte Biology.” Nature Communications 10 (1): 4064. https://doi.org/10.1038/s41467-019-11953-9."
  },
  {
    "objectID": "meetings/index.html",
    "href": "meetings/index.html",
    "title": "Meetings",
    "section": "",
    "text": "Notes from NPD meetings.",
    "crumbs": [
      "Meetings"
    ]
  },
  {
    "objectID": "notebooks/ukbb/2023-10-30-preprocess-ukbb.html",
    "href": "notebooks/ukbb/2023-10-30-preprocess-ukbb.html",
    "title": "Preprocessing UKBB data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as sc_stats\nimport collections\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120)\n\n\n\n\nCode\nphenotype_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/npd_phenotypes.tsv\"\nvariants_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/metadata/significant_variants.tsv\"\ndata_dir = \"/gpfs/commons/home/sbanerjee/npddata/ukbb.imputed_v3.neale/3_all_assoc\"\n\n\n\n\nCode\nphenotype_df = pd.read_csv(phenotype_metafile, sep = '\\t')\nphenotype_ids = phenotype_df['phenotype'].to_list()\nassoc_file = {}\nfor s in phenotype_ids:\n    assoc_file[s] = f\"{data_dir}/{s}.tsv\"\n\n\n\n\nCode\nphenotype_df\n\n\n\n\n\n\n\n\n\nphenotype\ndescription\nvariable_type\nsource\nn_non_missing\nn_missing\nn_controls\nn_cases\nPHESANT_transformation\nnotes\n\n\n\n\n0\n1160\nSleep duration\nordinal\nphesant\n359020\n2174\nNaN\nNaN\n1160_0|| INTEGER || reassignments: -1=NA|-3=NA...\nACE touchscreen question \"About how many hours...\n\n\n1\n1200\nSleeplessness / insomnia\nordinal\nphesant\n360738\n456\nNaN\nNaN\n1200_0|| CAT-SINGLE || Inc(&gt;=10): 3(102157) ||...\nACE touchscreen question \"Do you have trouble ...\n\n\n2\n1220\nDaytime dozing / sleeping (narcolepsy)\nordinal\nphesant\n359752\n1442\nNaN\nNaN\n1220_0|| CAT-SINGLE || reassignments: 3=2 || I...\nACE touchscreen question \"How likely are you t...\n\n\n3\n1920\nMood swings\nbinary\nphesant\n352604\n8590\n193622.0\n158982.0\n1920_0|| CAT-SINGLE || Inc(&gt;=10): 0(193622) ||...\nACE touchscreen question \"Does your mood often...\n\n\n4\n1970\nNervous feelings\nbinary\nphesant\n351829\n9365\n268709.0\n83120.0\n1970_0|| CAT-SINGLE || Inc(&gt;=10): 0(268709) ||...\nACE touchscreen question \"Would you call yours...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n312\nT79\nDiagnoses - main ICD10: T79 Certain early comp...\nbinary\nicd10\n361194\n0\n360989.0\n205.0\nNaN\nNaN\n\n\n313\nTRAUMBRAIN_NONCONCUS\nsevere traumatic brain injury, does not includ...\nbinary\nfinngen\n361194\n0\n360631.0\n563.0\nNaN\nNaN\n\n\n314\nVI_NERVOUS\nDiseases of the nervous system\nbinary\nfinngen\n361194\n0\n339871.0\n21323.0\nNaN\nNaN\n\n\n315\nV_MENTAL_BEHAV\nMental and behavioural disorders\nbinary\nfinngen\n361194\n0\n356892.0\n4302.0\nNaN\nNaN\n\n\n316\nZ43\nDiagnoses - main ICD10: Z43 Attention to artif...\nbinary\nicd10\n361194\n0\n359838.0\n1356.0\nNaN\nNaN\n\n\n\n\n317 rows × 10 columns\n\n\n\n\n\nCode\nassoc_file['1160']\n\n\n'/gpfs/commons/home/sbanerjee/npddata/ukbb.imputed_v3.neale/3_all_assoc/1160.tsv'\n\n\n\n\nCode\npd.read_csv(assoc_file['1160'], sep='\\t', header = None, names = ['variant', 'low_confidence', 'beta', 'se', 'tstat', 'pval'])\n\n\n\n\n\n\n\n\n\nvariant\nlow_confidence\nbeta\nse\ntstat\npval\n\n\n\n\n0\n1:2073742:A:T\nFalse\n-0.002622\n0.007306\n-0.358873\n0.719690\n\n\n1\n1:2094006:A:G\nFalse\n-0.001466\n0.007288\n-0.201181\n0.840557\n\n\n2\n1:2094007:C:G\nFalse\n-0.002688\n0.007319\n-0.367227\n0.713450\n\n\n3\n1:2108792:C:T\nFalse\n-0.002112\n0.007679\n-0.274978\n0.783333\n\n\n4\n1:2111080:C:T\nFalse\n-0.000891\n0.007655\n-0.116456\n0.907291\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n41988\n22:41812084:G:A\nFalse\n0.006255\n0.002175\n2.875620\n0.004033\n\n\n41989\n22:41812439:A:G\nFalse\n0.006235\n0.002175\n2.866260\n0.004154\n\n\n41990\n22:41812755:C:T\nFalse\n0.006234\n0.002175\n2.865640\n0.004162\n\n\n41991\n22:41812819:T:C\nFalse\n0.006261\n0.002175\n2.878240\n0.003999\n\n\n41992\n22:46994296:T:A\nTrue\n0.014798\n0.021522\n0.687576\n0.491720\n\n\n\n\n41993 rows × 6 columns\n\n\n\n\n\nCode\nassoc = {}\n\ncolnames = ['variant', 'low_confidence', 'beta', 'se', 'tstat', 'pval']\n\nfor pid in phenotype_ids:\n    print (f\"Read summary statistics for {pid}\")\n    assoc[pid] = pd.read_csv(assoc_file[pid], sep = '\\t', names = colnames)\n    assoc[pid]['phenotype_id'] = pid\n\n\nRead summary statistics for 1160\nRead summary statistics for 1200\nRead summary statistics for 1220\nRead summary statistics for 1920\nRead summary statistics for 1970\nRead summary statistics for 1980\nRead summary statistics for 20002_1123\nRead summary statistics for 20002_1240\nRead summary statistics for 20002_1243\nRead summary statistics for 20002_1246\nRead summary statistics for 20002_1249\nRead summary statistics for 20002_1250\nRead summary statistics for 20002_1251\nRead summary statistics for 20002_1254\nRead summary statistics for 20002_1255\nRead summary statistics for 20002_1256\nRead summary statistics for 20002_1257\nRead summary statistics for 20002_1258\nRead summary statistics for 20002_1261\nRead summary statistics for 20002_1262\nRead summary statistics for 20002_1264\nRead summary statistics for 20002_1265\nRead summary statistics for 20002_1267\nRead summary statistics for 20002_1279\nRead summary statistics for 20002_1286\nRead summary statistics for 20002_1287\nRead summary statistics for 20002_1288\nRead summary statistics for 20002_1289\nRead summary statistics for 20002_1291\nRead summary statistics for 20002_1384\nRead summary statistics for 20002_1394\nRead summary statistics for 20002_1425\nRead summary statistics for 20002_1433\nRead summary statistics for 20002_1434\nRead summary statistics for 20002_1435\nRead summary statistics for 20002_1436\nRead summary statistics for 20002_1437\nRead summary statistics for 20002_1468\nRead summary statistics for 20002_1469\nRead summary statistics for 20002_1470\nRead summary statistics for 20002_1478\nRead summary statistics for 20002_1482\nRead summary statistics for 20002_1523\nRead summary statistics for 20002_1525\nRead summary statistics for 20002_1526\nRead summary statistics for 20002_1536\nRead summary statistics for 20002_1541\nRead summary statistics for 20002_1550\nRead summary statistics for 20002_1614\nRead summary statistics for 20002_1616\nRead summary statistics for 20002_1683\nRead summary statistics for 20019_irnt\nRead summary statistics for 20021_irnt\nRead summary statistics for 20107_10\nRead summary statistics for 20107_11\nRead summary statistics for 20107_12\nRead summary statistics for 2010\nRead summary statistics for 20110_10\nRead summary statistics for 20110_11\nRead summary statistics for 20110_12\nRead summary statistics for 20111_10\nRead summary statistics for 20111_11\nRead summary statistics for 20111_12\nRead summary statistics for 20112_10\nRead summary statistics for 20113_10\nRead summary statistics for 20113_12\nRead summary statistics for 20114_12\nRead summary statistics for 20122\nRead summary statistics for 20126_0\nRead summary statistics for 20126_1\nRead summary statistics for 20126_2\nRead summary statistics for 20126_3\nRead summary statistics for 20126_4\nRead summary statistics for 20126_5\nRead summary statistics for 20127_irnt\nRead summary statistics for 20417\nRead summary statistics for 20418\nRead summary statistics for 20419\nRead summary statistics for 20421\nRead summary statistics for 20422\nRead summary statistics for 20423\nRead summary statistics for 20426\nRead summary statistics for 20427\nRead summary statistics for 20428\nRead summary statistics for 20429\nRead summary statistics for 20432\nRead summary statistics for 20435\nRead summary statistics for 20436\nRead summary statistics for 20437\nRead summary statistics for 20438\nRead summary statistics for 20439\nRead summary statistics for 20440\nRead summary statistics for 20445\nRead summary statistics for 20446\nRead summary statistics for 20447\nRead summary statistics for 20448\nRead summary statistics for 20449\nRead summary statistics for 20450\nRead summary statistics for 20462\nRead summary statistics for 20466\nRead summary statistics for 20467\nRead summary statistics for 20477\nRead summary statistics for 20488\nRead summary statistics for 20493\nRead summary statistics for 20495\nRead summary statistics for 20497\nRead summary statistics for 20498\nRead summary statistics for 20499\nRead summary statistics for 20500\nRead summary statistics for 20501\nRead summary statistics for 20506\nRead summary statistics for 2050\nRead summary statistics for 20510\nRead summary statistics for 20517\nRead summary statistics for 20532\nRead summary statistics for 20533\nRead summary statistics for 20534\nRead summary statistics for 20536_0\nRead summary statistics for 20536_1\nRead summary statistics for 20536_2\nRead summary statistics for 20536_3\nRead summary statistics for 20537\nRead summary statistics for 20538\nRead summary statistics for 20539\nRead summary statistics for 20540\nRead summary statistics for 20541\nRead summary statistics for 20542\nRead summary statistics for 20543\nRead summary statistics for 20544_10\nRead summary statistics for 20544_11\nRead summary statistics for 20544_12\nRead summary statistics for 20544_13\nRead summary statistics for 20544_14\nRead summary statistics for 20544_15\nRead summary statistics for 20544_16\nRead summary statistics for 20544_17\nRead summary statistics for 20544_1\nRead summary statistics for 20544_2\nRead summary statistics for 20544_3\nRead summary statistics for 20544_4\nRead summary statistics for 20544_5\nRead summary statistics for 20544_6\nRead summary statistics for 20544_7\nRead summary statistics for 20546_1\nRead summary statistics for 20546_3\nRead summary statistics for 20546_4\nRead summary statistics for 20547_1\nRead summary statistics for 20547_3\nRead summary statistics for 20548_1\nRead summary statistics for 20548_2\nRead summary statistics for 20548_3\nRead summary statistics for 20548_5\nRead summary statistics for 20548_6\nRead summary statistics for 20548_7\nRead summary statistics for 20548_8\nRead summary statistics for 20548_9\nRead summary statistics for 20549_1\nRead summary statistics for 20549_3\nRead summary statistics for 20549_4\nRead summary statistics for 20550_1\nRead summary statistics for 20550_3\nRead summary statistics for 20551_1\nRead summary statistics for 20552_1\nRead summary statistics for 20552_2\nRead summary statistics for 20554_1\nRead summary statistics for 2090\nRead summary statistics for 2100\nRead summary statistics for 3799\nRead summary statistics for 40001_C719\nRead summary statistics for 40001_G122\nRead summary statistics for 41215_1\nRead summary statistics for 41218_1\nRead summary statistics for 41218_2\nRead summary statistics for 41248_3001\nRead summary statistics for 41248_5003\nRead summary statistics for 4620\nRead summary statistics for 4642\nRead summary statistics for 4968\nRead summary statistics for 4990\nRead summary statistics for 5012\nRead summary statistics for 5663\nRead summary statistics for 5674\nRead summary statistics for 5699\nRead summary statistics for 6145_1\nRead summary statistics for 6145_2\nRead summary statistics for 6145_3\nRead summary statistics for 6145_4\nRead summary statistics for 6145_5\nRead summary statistics for 6145_6\nRead summary statistics for 6156_11\nRead summary statistics for 6156_12\nRead summary statistics for 6156_13\nRead summary statistics for 6156_14\nRead summary statistics for 6156_15\nRead summary statistics for 6159_1\nRead summary statistics for AB1_VIRAL_CNS\nRead summary statistics for AD\nRead summary statistics for AMN2\nRead summary statistics for C3_BRAIN\nRead summary statistics for C3_EYE_BRAIN_NEURO\nRead summary statistics for C71\nRead summary statistics for C_BRAIN\nRead summary statistics for C_EYE_BRAIN_NEURO\nRead summary statistics for D33\nRead summary statistics for D43\nRead summary statistics for F05\nRead summary statistics for F10\nRead summary statistics for F20\nRead summary statistics for F31\nRead summary statistics for F32\nRead summary statistics for F33\nRead summary statistics for F41\nRead summary statistics for F43\nRead summary statistics for F45\nRead summary statistics for F52\nRead summary statistics for F5_ALCOHOLAC\nRead summary statistics for F5_ALLANXIOUS\nRead summary statistics for F5_ANXIETY\nRead summary statistics for F5_BEHAVE\nRead summary statistics for F5_DEMENTIA\nRead summary statistics for F5_DEPRESSIO\nRead summary statistics for F5_MOOD\nRead summary statistics for F5_PANIC\nRead summary statistics for F5_PERSONALITY\nRead summary statistics for F5_SCHIZO\nRead summary statistics for F99\nRead summary statistics for G12\nRead summary statistics for G20\nRead summary statistics for G24\nRead summary statistics for G35\nRead summary statistics for G37\nRead summary statistics for G40\nRead summary statistics for G43\nRead summary statistics for G44\nRead summary statistics for G45\nRead summary statistics for G47\nRead summary statistics for G50\nRead summary statistics for G51\nRead summary statistics for G54\nRead summary statistics for G56\nRead summary statistics for G57\nRead summary statistics for G58\nRead summary statistics for G61\nRead summary statistics for G62\nRead summary statistics for G6_ALS\nRead summary statistics for G6_BELLPA\nRead summary statistics for G6_CARPTU\nRead summary statistics for G6_CONCUS\nRead summary statistics for G6_CPETAL\nRead summary statistics for G6_DEGENOTH\nRead summary statistics for G6_DEMYEL\nRead summary statistics for G6_DIFBRAININJ\nRead summary statistics for G6_DISBROTHUNS\nRead summary statistics for G6_EPIPAROX\nRead summary statistics for G6_GUILBAR\nRead summary statistics for G6_HYDROCEPH\nRead summary statistics for G6_ICTRAUOTHUNS\nRead summary statistics for G6_INTRACRATRAUMA\nRead summary statistics for G6_MONOLOWOTHUNS\nRead summary statistics for G6_MONOOTHUNS\nRead summary statistics for G6_MYONEU\nRead summary statistics for G6_NERPLEX\nRead summary statistics for G6_NEUATR\nRead summary statistics for G6_NEUINFL\nRead summary statistics for G6_NEURODEG\nRead summary statistics for G6_PLANTAR\nRead summary statistics for G6_POLYNEU\nRead summary statistics for G6_POLYOTHUNS\nRead summary statistics for G6_ROOTPLEXOTHUNS\nRead summary statistics for G6_SLEEPAPNO\nRead summary statistics for G6_TRINEU\nRead summary statistics for G6_ULLNLE\nRead summary statistics for G6_XTRAPYR\nRead summary statistics for G81\nRead summary statistics for G93\nRead summary statistics for G95\nRead summary statistics for H49\nRead summary statistics for H7_PARASTRAB\nRead summary statistics for H8_BPV\nRead summary statistics for I45\nRead summary statistics for I9_ANEURYSM\nRead summary statistics for I9_CONDUCTIO\nRead summary statistics for I9_INTRACRA\nRead summary statistics for KRA_PSY_ALCOH\nRead summary statistics for KRA_PSY_ANXIETY\nRead summary statistics for KRA_PSY_ANYMENTAL\nRead summary statistics for KRA_PSY_ANYMENTAL_SUICID\nRead summary statistics for KRA_PSY_DEMENTIA\nRead summary statistics for KRA_PSY_MOOD\nRead summary statistics for KRA_PSY_PERSON\nRead summary statistics for KRA_PSY_SCHIZODEL\nRead summary statistics for KRA_PSY_SUBSTANCE\nRead summary statistics for M13_ALGONEURO\nRead summary statistics for M13_CERVICALGIA\nRead summary statistics for M13_GANGLION\nRead summary statistics for M13_NEURALGIA\nRead summary statistics for M13_SPINSTENOSIS\nRead summary statistics for M13_THORACISPINEPAIN\nRead summary statistics for M50\nRead summary statistics for NEURODEGOTH\nRead summary statistics for O68\nRead summary statistics for PULM_ANXIETY\nRead summary statistics for R11\nRead summary statistics for R29\nRead summary statistics for R41\nRead summary statistics for R47\nRead summary statistics for R51\nRead summary statistics for R53\nRead summary statistics for R56\nRead summary statistics for R90\nRead summary statistics for SFN\nRead summary statistics for SLEEP\nRead summary statistics for T79\nRead summary statistics for TRAUMBRAIN_NONCONCUS\nRead summary statistics for VI_NERVOUS\nRead summary statistics for V_MENTAL_BEHAV\nRead summary statistics for Z43\n\n\n\n\nCode\nassoc_df = pd.concat([v for k,v in assoc.items()])\nassoc_df.to_pickle(\"../data/ukbb_assoc.pkl\")\nassoc_df\n\n\n\n\n\n\n\n\n\nvariant\nlow_confidence\nbeta\nse\ntstat\npval\nphenotype_id\n\n\n\n\n0\n1:2073742:A:T\nFalse\n-0.002622\n0.007306\n-0.358873\n0.719690\n1160\n\n\n1\n1:2094006:A:G\nFalse\n-0.001466\n0.007288\n-0.201181\n0.840557\n1160\n\n\n2\n1:2094007:C:G\nFalse\n-0.002688\n0.007319\n-0.367227\n0.713450\n1160\n\n\n3\n1:2108792:C:T\nFalse\n-0.002112\n0.007679\n-0.274978\n0.783333\n1160\n\n\n4\n1:2111080:C:T\nFalse\n-0.000891\n0.007655\n-0.116456\n0.907291\n1160\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41988\n22:41812084:G:A\nFalse\n-0.000076\n0.000177\n-0.431360\n0.666207\nZ43\n\n\n41989\n22:41812439:A:G\nFalse\n-0.000076\n0.000177\n-0.431209\n0.666317\nZ43\n\n\n41990\n22:41812755:C:T\nFalse\n-0.000076\n0.000177\n-0.428950\n0.667960\nZ43\n\n\n41991\n22:41812819:T:C\nFalse\n-0.000076\n0.000177\n-0.431143\n0.666365\nZ43\n\n\n41992\n22:46994296:T:A\nTrue\n0.000667\n0.001755\n0.380176\n0.703815\nZ43\n\n\n\n\n13311781 rows × 7 columns\n\n\n\n\n\nCode\nassoc_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 13311781 entries, 0 to 41992\nData columns (total 7 columns):\n #   Column          Dtype  \n---  ------          -----  \n 0   variant         object \n 1   low_confidence  bool   \n 2   beta            float64\n 3   se              float64\n 4   tstat           float64\n 5   pval            float64\n 6   phenotype_id    object \ndtypes: bool(1), float64(4), object(2)\nmemory usage: 723.6+ MB\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df['variant'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df['phenotype_id'].unique())}\")\n\n\nNumber of unique SNPs: 41993\nNumber of unique studies: 317\n\n\n\nFilter SNPs (indel, rare, low info)\n\n\nCode\nvariant_colnames = ['variant', 'chr', 'pos', 'ref', 'alt', 'rsid', 'varid', \n                    'consequence', 'consequence_category', 'info', 'call_rate',\n                    'AC', 'AF', 'minor_allele', 'minor_AF', 'p_hwe', \n                    'n_called', 'n_not_called', 'n_hom_ref', 'n_het', 'n_hom_var', 'n_non_ref',\n                    'r_heterozygosity', 'r_het_hom_var', 'r_expected_het_frequency']\nvariant_df = pd.read_csv(variants_metafile, sep = '\\t', names = variant_colnames)\n\n\n\n\nCode\nvariant_df\n\n\n\n\n\n\n\n\n\nvariant\nchr\npos\nref\nalt\nrsid\nvarid\nconsequence\nconsequence_category\ninfo\n...\np_hwe\nn_called\nn_not_called\nn_hom_ref\nn_het\nn_hom_var\nn_non_ref\nr_heterozygosity\nr_het_hom_var\nr_expected_het_frequency\n\n\n\n\n0\n1:2073742:A:T\n1\n2073742\nA\nT\nrs551766141\n1:2073742_A_T\nnon_coding_transcript_exon_variant\nnon_coding\n0.992179\n...\n0.604866\n361194\n0\n350078\n11025\n91\n11116\n0.030524\n121.154000\n0.030546\n\n\n1\n1:2094006:A:G\n1\n2094006\nA\nG\nrs190347047\nrs190347047\nintron_variant\nnon_coding\n1.000000\n...\n0.263827\n361194\n0\n350089\n11008\n97\n11105\n0.030477\n113.485000\n0.030533\n\n\n2\n1:2094007:C:G\n1\n2094007\nC\nG\nrs182067703\n1:2094007_C_G\nintron_variant\nnon_coding\n0.998611\n...\n0.763051\n361194\n0\n350145\n10961\n88\n11049\n0.030347\n124.557000\n0.030359\n\n\n3\n1:2108792:C:T\n1\n2108792\nC\nT\nrs116253512\n1:2108792_C_T\nintron_variant\nnon_coding\n0.974810\n...\n0.660102\n361194\n0\n350872\n10244\n78\n10322\n0.028362\n131.333000\n0.028379\n\n\n4\n1:2111080:C:T\n1\n2111080\nC\nT\nrs116005884\n1:2111080_C_T\nintron_variant\nnon_coding\n0.975114\n...\n0.703785\n361194\n0\n350845\n10271\n78\n10349\n0.028436\n131.679000\n0.028452\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41988\n22:41812084:G:A\n22\n41812084\nG\nA\nrs132922\n22:41812084_G_A\nintergenic_variant\nnon_coding\n0.999802\n...\n0.179555\n361194\n0\n17461\n124540\n219193\n343733\n0.344801\n0.568175\n0.344031\n\n\n41989\n22:41812439:A:G\n22\n41812439\nA\nG\nrs202661\n22:41812439_A_G\nintergenic_variant\nnon_coding\n0.999845\n...\n0.174893\n361194\n0\n17459\n124542\n219193\n343735\n0.344806\n0.568184\n0.344028\n\n\n41990\n22:41812755:C:T\n22\n41812755\nC\nT\nrs202662\n22:41812755_C_T\nintergenic_variant\nnon_coding\n0.999862\n...\n0.176463\n361194\n0\n17463\n124548\n219183\n343731\n0.344823\n0.568238\n0.344050\n\n\n41991\n22:41812819:T:C\n22\n41812819\nT\nC\nrs202663\n22:41812819_T_C\nintergenic_variant\nnon_coding\n0.999905\n...\n0.177995\n361194\n0\n17461\n124542\n219191\n343733\n0.344806\n0.568189\n0.344034\n\n\n41992\n22:46994296:T:A\n22\n46994296\nT\nA\nrs536520098\n22:46994296_T_A\nintron_variant\nnon_coding\n0.963021\n...\n0.486537\n361194\n0\n359880\n1314\n0\n1314\n0.003638\nNaN\n0.003631\n\n\n\n\n41993 rows × 25 columns\n\n\n\n\n\nCode\ndef count_nucleotides(row):\n    return len(row['ref']) + len(row['alt'])\n\nvariant_df['ntcount'] = variant_df.apply(count_nucleotides, axis = 1)\nvariant_df_filtered = variant_df[(variant_df['ntcount'] == 2) \n                               & (variant_df['minor_AF'] &gt;= 0.05)\n                               & (variant_df['info'] &gt;= 0.8)].drop(columns = ['ntcount'])\n\n\n\n\nCode\nvariant_df_filtered\n\n\n\n\n\n\n\n\n\nvariant\nchr\npos\nref\nalt\nrsid\nvarid\nconsequence\nconsequence_category\ninfo\n...\np_hwe\nn_called\nn_not_called\nn_hom_ref\nn_het\nn_hom_var\nn_non_ref\nr_heterozygosity\nr_het_hom_var\nr_expected_het_frequency\n\n\n\n\n9\n1:2978043:G:C\n1\n2978043\nG\nC\nrs2075969\n1:2978043_G_C\nnon_coding_transcript_exon_variant\nnon_coding\n0.992771\n...\n0.103637\n361194\n0\n114375\n177327\n69492\n246819\n0.490947\n2.551760\n0.492280\n\n\n10\n1:2984087:C:A\n1\n2984087\nC\nA\nrs2297829\n1:2984087_C_A\nnon_coding_transcript_exon_variant\nnon_coding\n0.980053\n...\n0.146547\n361194\n0\n148683\n165801\n46710\n212511\n0.459036\n3.549580\n0.460148\n\n\n11\n1:3065568:C:T\n1\n3065568\nC\nT\nrs61759161\n1:3065568_C_T\nintron_variant\nnon_coding\n0.897850\n...\n0.009582\n361183\n11\n215504\n127329\n18350\n145679\n0.352533\n6.938910\n0.351021\n\n\n12\n1:3066761:A:T\n1\n3066761\nA\nT\nrs10909886\n1:3066761_A_T\nintron_variant\nnon_coding\n0.904227\n...\n0.021327\n361187\n7\n214854\n127752\n18581\n146333\n0.353700\n6.875410\n0.352353\n\n\n13\n1:3070634:C:G\n1\n3070634\nC\nG\nrs207195\n1:3070634_C_G\nintron_variant\nnon_coding\n0.958558\n...\n0.454596\n361193\n1\n116581\n177438\n67174\n244612\n0.491255\n2.641470\n0.490645\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n41987\n22:41812044:G:A\n22\n41812044\nG\nA\nrs132921\n22:41812044_G_A\nintergenic_variant\nnon_coding\n0.999884\n...\n0.178013\n361194\n0\n17463\n124547\n219184\n343731\n0.344820\n0.568230\n0.344048\n\n\n41988\n22:41812084:G:A\n22\n41812084\nG\nA\nrs132922\n22:41812084_G_A\nintergenic_variant\nnon_coding\n0.999802\n...\n0.179555\n361194\n0\n17461\n124540\n219193\n343733\n0.344801\n0.568175\n0.344031\n\n\n41989\n22:41812439:A:G\n22\n41812439\nA\nG\nrs202661\n22:41812439_A_G\nintergenic_variant\nnon_coding\n0.999845\n...\n0.174893\n361194\n0\n17459\n124542\n219193\n343735\n0.344806\n0.568184\n0.344028\n\n\n41990\n22:41812755:C:T\n22\n41812755\nC\nT\nrs202662\n22:41812755_C_T\nintergenic_variant\nnon_coding\n0.999862\n...\n0.176463\n361194\n0\n17463\n124548\n219183\n343731\n0.344823\n0.568238\n0.344050\n\n\n41991\n22:41812819:T:C\n22\n41812819\nT\nC\nrs202663\n22:41812819_T_C\nintergenic_variant\nnon_coding\n0.999905\n...\n0.177995\n361194\n0\n17461\n124542\n219191\n343733\n0.344806\n0.568189\n0.344034\n\n\n\n\n37395 rows × 25 columns\n\n\n\n\n\nCode\nassoc_df_fvar = variant_df_filtered[['variant', 'rsid']].merge(assoc_df, on = ['variant'], how = 'inner')\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fvar['variant'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fvar['phenotype_id'].unique())}\")\n\n\nNumber of unique SNPs: 37395\nNumber of unique studies: 317\n\n\n\n\nCode\nzscore_df = assoc_df_fvar[['variant', 'phenotype_id', 'tstat']].pivot(index = 'variant', columns = 'phenotype_id', values = 'tstat').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nbeta_df   = assoc_df_fvar[['variant', 'phenotype_id', 'beta']].pivot(index = 'variant', columns = 'phenotype_id', values = 'beta').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nse_df     = assoc_df_fvar[['variant', 'phenotype_id', 'se']].pivot(index = 'variant', columns = 'phenotype_id', values = 'se').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\n\n\n\n\nCode\nzscore_df\n\n\n\n\n\n\n\n\n\n1160\n1200\n1220\n1920\n1970\n1980\n20002_1123\n20002_1240\n20002_1243\n20002_1246\n...\nR53\nR56\nR90\nSFN\nSLEEP\nT79\nTRAUMBRAIN_NONCONCUS\nVI_NERVOUS\nV_MENTAL_BEHAV\nZ43\n\n\n\n\n10:100000625:A:G\n0.501981\n-0.525399\n1.469430\n2.32670\n4.11523\n2.423000\n-1.383980\n-0.798416\n1.501170\n0.99798\n...\n1.031720\n0.563788\n1.543120\n-0.732158\n-0.639260\n0.041272\n0.122796\n-3.657790\n1.372840\n0.936180\n\n\n10:100003785:T:C\n0.727118\n-0.838564\n-0.011967\n-2.23214\n-1.18325\n-0.279210\n-0.208375\n-0.994745\n-0.626474\n-1.53071\n...\n-0.738045\n1.121560\n0.602487\n-0.086688\n-0.193665\n0.912439\n0.122715\n3.345200\n-1.647860\n-0.641863\n\n\n10:100004441:G:C\n-0.728567\n0.914938\n0.070534\n2.30550\n1.20813\n0.318050\n0.150605\n1.014940\n0.667526\n1.56572\n...\n0.764725\n-1.095360\n-0.706028\n0.112536\n0.206682\n-0.879437\n-0.133474\n-3.253040\n1.707040\n0.687995\n\n\n10:100004906:C:A\n0.504855\n-0.523555\n1.469210\n2.32101\n4.11411\n2.419160\n-1.382470\n-0.798095\n1.502150\n0.99878\n...\n1.032040\n0.564664\n1.543560\n-0.732081\n-0.637166\n0.041860\n0.123797\n-3.650540\n1.375100\n0.937855\n\n\n10:100004996:G:A\n0.733441\n-0.843107\n-0.008906\n-2.22630\n-1.18083\n-0.287227\n-0.205852\n-0.994113\n-0.626903\n-1.52962\n...\n-0.746129\n1.123710\n0.603249\n-0.085872\n-0.189963\n0.913379\n0.124349\n3.348560\n-1.648650\n-0.641694\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9:98265901:A:G\n-0.785029\n0.118622\n2.643240\n4.30311\n3.35008\n5.088650\n-0.048185\n0.606501\n0.197470\n1.65606\n...\n0.830156\n1.567530\n1.371840\n-0.001408\n0.609074\n-0.196145\n-0.200227\n-0.354429\n0.785742\n-1.053890\n\n\n9:98266855:T:A\n-0.615773\n-0.188766\n2.571380\n4.24034\n3.62091\n4.711810\n0.039514\n0.608059\n0.151668\n1.40410\n...\n0.704177\n1.213300\n1.207820\n0.050593\n1.022050\n-0.416852\n-0.208696\n-0.352999\n1.217760\n-1.058050\n\n\n9:98273305:T:G\n-0.846092\n0.070976\n2.650630\n4.32363\n3.35409\n5.113290\n-0.051482\n0.623136\n0.203378\n1.74384\n...\n0.865338\n1.432780\n1.166460\n0.018348\n0.479160\n-0.233870\n-0.179549\n-0.483875\n0.839336\n-0.996326\n\n\n9:98275789:C:T\n-0.687039\n-0.247633\n2.569970\n4.27250\n3.63573\n4.751370\n0.036028\n0.624772\n0.184598\n1.47482\n...\n0.741583\n1.084520\n1.003140\n0.071251\n0.892456\n-0.443611\n-0.191688\n-0.461093\n1.284170\n-0.991163\n\n\n9:98278413:C:T\n-0.819318\n-0.267375\n2.656800\n4.05938\n3.62366\n4.654410\n0.006371\n1.033790\n0.106624\n1.35686\n...\n0.849630\n0.951746\n0.913949\n-0.017823\n0.924901\n-0.552934\n-0.289266\n-0.395223\n1.057020\n-0.924260\n\n\n\n\n37395 rows × 317 columns\n\n\n\n\n\nCode\nmean_se  = se_df.median(axis = 0, skipna = True)\nmean_se  = pd.DataFrame(mean_se).set_axis([\"mean_se\"], axis = 1)\nbeta_std = beta_df.std(axis = 0, skipna = True)\nbeta_std = pd.DataFrame(beta_std).set_axis([\"beta_std\"], axis = 1)\nerror_df = pd.concat([mean_se, beta_std], axis = 1)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(np.log10(error_df['beta_std']), np.log10(error_df['mean_se']), alpha = 0.5, s = 100)\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.set_yticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.plot_diag(ax1)\n\nkeep_columns = error_df.query(\"mean_se &lt;= 0.2\").index\nfor pid in error_df.index.to_list():\n    if pid not in keep_columns:\n        pid_text = f\"{pid} / {phenotype_dict[pid]}\"\n        xval = np.log10(error_df.loc[pid]['beta_std'])\n        yval = np.log10(error_df.loc[pid]['mean_se'])\n        ax1.annotate(pid_text, (xval, yval))\n\nax1.set_xlabel(r\"Standard Deviation of mean $\\beta$\")\nax1.set_ylabel(r\"Median of SE\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Calibration of SE against std of beta\n\n\n\n\n\n\n\nCode\nzscore_df.to_pickle(\"../data/ukbb_zscore_df.pkl\")"
  },
  {
    "objectID": "notebooks/ukbb/2023-10-30-rpca-ialm.html",
    "href": "notebooks/ukbb/2023-10-30-rpca-ialm.html",
    "title": "First look at NPD phenotypes in the UKBB data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\nfrom matplotlib.gridspec import GridSpec\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\ndata_dir = \"../data\"\nzscore_df_filename = f\"{data_dir}/ukbb_zscore_df2.pkl\"\nzscore_df = pd.read_pickle(zscore_df_filename)\n\nphenotype_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/npd_phenotypes_broad_categories.tsv\"\nphenotype_df = pd.read_csv(phenotype_metafile, sep=\"\\t\")\n\nn_signif_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/npd_n_signif.tsv\"\nn_signif_df = pd.read_csv(n_signif_metafile, sep=\"\\t\", header = None, names = ['phenotype', 'n_signif'])\n\n\n\n\nCode\nzscore_df = zscore_df.loc[:, n_signif_df.loc[n_signif_df['n_signif'] &gt;= 4, 'phenotype']]\n\n\n\n\nCode\nphenotype_df\n\n\n\n\n\n\n\n\n\nPhenotype Code\nPhenotype Name\nPhenotype Class\nPhenotype Description\nvariable_type\nsource\nn_non_missing\nn_missing\nn_controls\nn_cases\n\n\n\n\n0\n20488\nPhysically abused by family as a child\nAbuse\nPhysically abused by family as a child\nordinal\nphesant\n117838\n243356\nNaN\nNaN\n\n\n1\n20107_10\nFather: Alzheimer's disease/dementia\nAlzheimer\nIllnesses of father: Alzheimer's disease/dementia\nbinary\nphesant\n312666\n48528\n297644.0\n15022.0\n\n\n2\n20110_10\nMother: Alzheimer's disease/dementia\nAlzheimer\nIllnesses of mother: Alzheimer's disease/dementia\nbinary\nphesant\n331041\n30153\n302534.0\n28507.0\n\n\n3\n20111_10\nSiblings: Alzheimer's disease/dementia\nAlzheimer\nIllnesses of siblings: Alzheimer's disease/dem...\nbinary\nphesant\n279062\n82132\n277453.0\n1609.0\n\n\n4\n20112_10\nAdopted father: Alzheimer's disease/dementia\nAlzheimer\nIllnesses of adopted father: Alzheimer's disea...\nbinary\nphesant\n2562\n358632\n2416.0\n146.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n312\n20499\nEver sought or received professional help for ...\nStress\nEver sought or received professional help for ...\nbinary\nphesant\n117677\n243517\n71657.0\n46020.0\n\n\n313\n20500\nEver suffered mental distress preventing usual...\nStress\nEver suffered mental distress preventing usual...\nbinary\nphesant\n116527\n244667\n77846.0\n38681.0\n\n\n314\nF43\nDiagnoses - main ICD10: F43 Reaction to severe...\nStress\nDiagnoses - main ICD10: F43 Reaction to severe...\nbinary\nicd10\n361194\n0\n360967.0\n227.0\n\n\n315\nO68\nDiagnoses - main ICD10: O68 Labour and deliver...\nStress\nDiagnoses - main ICD10: O68 Labour and deliver...\nbinary\nicd10\n361194\n0\n359312.0\n1882.0\n\n\n316\nT79\nDiagnoses - main ICD10: T79 Certain early comp...\nStress\nDiagnoses - main ICD10: T79 Certain early comp...\nbinary\nicd10\n361194\n0\n360989.0\n205.0\n\n\n\n\n317 rows × 10 columns\n\n\n\n\n\nCode\nphenotype_ids = list(zscore_df.columns)\nphenotype_names = [phenotype_df.loc[phenotype_df['Phenotype Code'] == x, 'Phenotype Name'].item() for x in phenotype_ids]\nphenotype_categories = [phenotype_df.loc[phenotype_df['Phenotype Code'] == x, 'Phenotype Class'].item() for x in phenotype_ids]\nunique_categories = list(set(phenotype_categories))\n\ntrait_indices = [np.array([i for i, x in enumerate(phenotype_categories) if x == catg]) for catg in unique_categories]\ntrait_colors  = {trait: color for trait, color in zip(unique_categories, (mpl_stylesheet.kelly_colors()))}\n\n\n\n\nCode\nX_nan = np.array(zscore_df).T\nX_nan_cent = X_nan - np.nanmean(X_nan, axis = 0, keepdims = True)\nX_nan_mask = np.isnan(X_nan)\nX_cent = np.nan_to_num(X_nan_cent, copy = True, nan = 0.0)\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(X_nan_mask) / np.prod(X_cent.shape):.3f}\")\n\n\nWe have 81 samples (phenotypes) and 3387 features (variants)\nFraction of Nan entries: 0.000\n\n\n\n\nCode\nU, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\nS2 = np.square(S)\npcomp = U @ np.diag(S)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(np.arange(S.shape[0]), np.cumsum(S2 / np.sum(S2)), 'o-')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Proportion of variance explained by the principal components of the input matrix\n\n\n\n\n\n\nRPCA - IALM\n\n\nCode\n1. / np.sqrt(10000)\n\n\n0.01\n\n\n\n\nCode\nrpca = IALM(max_iter = 10000, mu_update_method='admm', show_progress = True)\nrpca.fit(X_cent, mask = X_nan_mask)\n\n\n2023-11-27 13:54:03,376 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0172)\n2023-11-27 13:54:03,482 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 0. Primal residual 0.866828. Dual residual 0.000320574\n\n\n\n\nCode\nwith open (f\"{data_dir}/ukbb_npd_lowrank_X_ialm.pkl\", 'wb') as handle:\n    pickle.dump(rpca.L_, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open (f\"{data_dir}/ukbb_npd_lowrank_E_ialm.pkl\", 'wb') as handle:\n    pickle.dump(rpca.E_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n\n\nCode\nnp.linalg.norm(X_cent, 'nuc')\n\n\n5427.91402878039\n\n\n\n\nCode\nnnm_sparse = FrankWolfe(model = 'nnm-sparse', max_iter = 10000, svd_max_iter = 50, \n                        tol = 1e-3, step_tol = 1e-5, simplex_method = 'sort',\n                        show_progress = True, debug = True, print_skip = 100)\nnnm_sparse.fit(X_cent, (1024.0, 0.5))\n\n\n2023-11-27 14:23:56,518 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 3.45757e+06\n2023-11-27 14:24:06,819 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.006. Duality Gap 4026.97\n2023-11-27 14:24:17,141 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.002. Duality Gap 2322.95\n2023-11-27 14:24:27,405 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.002. Duality Gap 1759.15\n2023-11-27 14:24:37,681 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 400. Step size 0.003. Duality Gap 1873.36\n2023-11-27 14:24:47,921 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 500. Step size 0.001. Duality Gap 1094.16\n2023-11-27 14:24:58,214 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 600. Step size 0.001. Duality Gap 957.196\n2023-11-27 14:25:08,429 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 700. Step size 0.001. Duality Gap 925.217\n2023-11-27 14:25:18,682 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 800. Step size 0.001. Duality Gap 886.563\n2023-11-27 14:25:28,948 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 900. Step size 0.001. Duality Gap 718.781\n2023-11-27 14:25:39,204 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1000. Step size 0.000. Duality Gap 354.201\n2023-11-27 14:25:49,466 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1100. Step size 0.001. Duality Gap 647.135\n2023-11-27 14:25:59,752 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1200. Step size 0.001. Duality Gap 556.836\n2023-11-27 14:26:10,023 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1300. Step size 0.001. Duality Gap 742.643\n2023-11-27 14:26:20,256 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1400. Step size 0.001. Duality Gap 576.098\n2023-11-27 14:26:30,513 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1500. Step size 0.001. Duality Gap 439.903\n2023-11-27 14:26:40,767 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1600. Step size 0.001. Duality Gap 508.349\n2023-11-27 14:26:51,025 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1700. Step size 0.001. Duality Gap 496.27\n2023-11-27 14:27:01,305 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1800. Step size 0.000. Duality Gap 375.604\n2023-11-27 14:27:11,549 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 1900. Step size 0.000. Duality Gap 474.234\n2023-11-27 14:27:21,801 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2000. Step size 0.001. Duality Gap 455.82\n2023-11-27 14:27:32,025 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2100. Step size 0.000. Duality Gap 376.674\n2023-11-27 14:27:42,283 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2200. Step size 0.000. Duality Gap 343.212\n2023-11-27 14:27:52,590 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2300. Step size 0.001. Duality Gap 436.296\n2023-11-27 14:28:02,805 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2400. Step size 0.000. Duality Gap 358.472\n2023-11-27 14:28:13,035 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2500. Step size 0.000. Duality Gap 222.27\n2023-11-27 14:28:23,256 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2600. Step size 0.000. Duality Gap 275.813\n2023-11-27 14:28:33,543 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2700. Step size 0.000. Duality Gap 257.902\n2023-11-27 14:28:43,801 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2800. Step size 0.000. Duality Gap 250.31\n2023-11-27 14:28:54,026 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 2900. Step size 0.000. Duality Gap 213.307\n2023-11-27 14:29:04,294 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3000. Step size 0.000. Duality Gap 315.758\n2023-11-27 14:29:14,577 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3100. Step size 0.000. Duality Gap 212.123\n2023-11-27 14:29:24,817 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3200. Step size 0.000. Duality Gap 120.102\n2023-11-27 14:29:35,061 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3300. Step size 0.000. Duality Gap 295.67\n2023-11-27 14:29:43,527 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-11-27 14:29:43,834 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-11-27 14:29:44,553 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-11-27 14:29:45,310 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3400. Step size 0.000. Duality Gap 193.651\n2023-11-27 14:29:50,628 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-11-27 14:29:55,618 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3500. Step size 0.000. Duality Gap 287.296\n2023-11-27 14:30:05,878 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3600. Step size 0.000. Duality Gap 33.0914\n2023-11-27 14:30:16,112 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3700. Step size 0.000. Duality Gap 83.4953\n2023-11-27 14:30:26,388 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3800. Step size 0.000. Duality Gap 204.393\n2023-11-27 14:30:36,656 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 3900. Step size 0.000. Duality Gap 157.054\n2023-11-27 14:30:39,297 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-11-27 14:30:39,806 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-11-27 14:30:46,890 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 4000. Step size 0.000. Duality Gap 258.058\n2023-11-27 14:30:57,170 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 4100. Step size 0.000. Duality Gap 183.173\n2023-11-27 14:31:07,378 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 4200. Step size 0.000. Duality Gap 109.968\n\n\n\n\nCode\nwith open (f\"{data_dir}/ukbb_npd_lowrank_X_nnm_sparse.pkl\", 'wb') as handle:\n    pickle.dump(nnm_sparse.X_, handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open (f\"{data_dir}/ukbb_npd_lowrank_E_nnm_sparse.pkl\", 'wb') as handle:\n    pickle.dump(nnm_sparse.M_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n\n\nCode\nmf_methods = ['ialm', 'nnm_sparse', 'nnm_weighted']\n\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps, S\n\n\nlowrank_X = dict()\nloadings  = dict()\npcomps    = dict()\neigenvals = dict()\n\nfor method in mf_methods:\n    with open (f\"{data_dir}/ukbb_npd_lowrank_X_{method}.pkl\", 'rb') as handle:\n        lowrank_X[method] = pickle.load(handle)\n        \nloadings['tsvd'], pcomps['tsvd'], eigenvals['tsvd'] = get_principal_components(X_cent)\nfor m in mf_methods:\n    loadings[m], pcomps[m], eigenvals[m] = get_principal_components(lowrank_X[m])\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps['nnm_weighted'], phenotype_categories, unique_categories)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef get_cos2_scores(pcomps):\n    ntrait, npcomp = pcomps.shape\n    x = np.zeros((ntrait, npcomp))\n    for i in range(ntrait):\n        cos2_trait = np.array([np.square(pcomps[i, pcidx]) for pcidx in range(npcomp)])\n        x[i, :] = cos2_trait / np.sum(cos2_trait)\n    return x\n\ndef stacked_barplot(ax, data, xlabels, colors, bar_width = 1.0, alpha = 1.0, showxlabels = False):\n    '''\n    Parameters\n    ----------\n        data: \n            dict() of scores. \n            - &lt;key&gt; : items for the stacked bars (e.g. traits or components)\n            - &lt;value&gt; : list of scores for the items. All dict entries must have the same length of &lt;value&gt;\n        xlabels: \n            label for each entry in the data &lt;value&gt; list. Must be of same length of data &lt;value&gt;\n        colors: \n            dict(&lt;key&gt;, &lt;color&gt;) corresponding to each data &lt;key&gt;.\n    '''\n    indices = np.arange(len(xlabels))\n    bottom = np.zeros(len(xlabels))\n\n    for item, weights in data.items():\n        ax.bar(indices, weights, bar_width, label = item, bottom = bottom, color = colors[item], alpha = alpha)\n        bottom += weights\n\n    if showxlabels:\n        ax.set_xticks(indices)\n        ax.set_xticklabels(xlabels, rotation=90, ha='center')\n        ax.tick_params(bottom = True, top = False, left = False, right = False,\n                   labelbottom = True, labeltop = False, labelleft = False, labelright = False)\n    else:\n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n\n    for side, border in ax.spines.items():\n        border.set_visible(False)\n\n    return\n\n\ndef structure_plot(ax, pcomps, trait_labels, comp_colors, npcomp, showxlabels = False):\n    cos2_scores = get_cos2_scores(pcomps)[:, :npcomp]\n    cos2_plot_data = {\n        f\"{i+1}\" : cos2_scores[:, i] for i in range(npcomp)\n    }\n    stacked_barplot(ax, cos2_plot_data, trait_labels, comp_colors, alpha = 0.8, showxlabels = showxlabels)\n    return\n\n\n\n\nCode\nplot_methods = ['tsvd'] + mf_methods\nplot_methods_names = {\n    'tsvd' : 'Raw Data',\n    'ialm' : 'RPCA-IALM',\n    'nnm'  : 'NNM-FW',\n    'nnm_weighted' : 'NNM-Weighted',\n    'nnm_sparse' : 'NNM-Sparse-FW',\n}\n\n\n\n\nCode\n\"\"\"\nNumber of components to plot\n\"\"\"\nnpcomp = 10\n    \n\"\"\"\nSort the traits / phenotypes\n\"\"\"\ntrait_indices_sorted = list()\nfor idx in trait_indices:\n    trait_indices_sorted += list(idx)\n\ntrait_labels_sorted = [phenotype_categories[i] for i in trait_indices_sorted]\npcomp_colors  = {f\"{i+1}\": color for i, color in enumerate(mpl_stylesheet.kelly_colors() + mpl_stylesheet.banskt_colors())}\n    \nfig = plt.figure(figsize = (28, 18))\ngs = GridSpec(nrows = len(plot_methods) + 1, ncols=1, figure=fig, height_ratios=[0.3] + [1 for i in plot_methods])\nax = [None for i in range(len(plot_methods) + 1)]\nax[0] = fig.add_subplot(gs[0, 0])\n\nfor i, m in enumerate(plot_methods):\n    iplot = i + 1\n    showxlabels = True if iplot == len(plot_methods) else False\n    #showxlabels = False\n    ax[iplot] = fig.add_subplot(gs[iplot, 0])\n    structure_plot(ax[iplot], pcomps[m][trait_indices_sorted,:], trait_labels_sorted, pcomp_colors, npcomp, showxlabels = showxlabels)\n    ax[iplot].set_title(plot_methods_names[m])\n    \nplt_handles, plt_labels = ax[i].get_legend_handles_labels()\nax[0].legend(plt_handles, plt_labels, \n             loc = 'lower center', bbox_to_anchor=(0.5, 0), title = \"Principal Components\",\n             frameon = False, handlelength = 8, ncol = 5)\nfor side, border in ax[0].spines.items():\n    border.set_visible(False)\nax[0].tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n\n#legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()"
  },
  {
    "objectID": "notebooks/ukbb/index.html",
    "href": "notebooks/ukbb/index.html",
    "title": "UKBB",
    "section": "",
    "text": "We are looking at the UK Biobank summary statistics of NPD phenotypes.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n23-12-12\n\n\nPan-UKB Hidden Factors v01\n\n\n\n\n23-12-11\n\n\nPan-UKB Pleitropy of Diseases v01\n\n\n\n\n23-12-08\n\n\nPan-UKB Principal Components v01\n\n\n\n\n23-11-27\n\n\nLD filtering of UKBB data\n\n\n\n\n23-10-30\n\n\nPreprocessing UKBB data\n\n\n\n\n23-10-30\n\n\nFirst look at NPD phenotypes in the UKBB data\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks",
      "Subcategory",
      "UKBB"
    ]
  },
  {
    "objectID": "notebooks/explore/2023-05-16-robustpca.html",
    "href": "notebooks/explore/2023-05-16-robustpca.html",
    "title": "Robust PCA implementation",
    "section": "",
    "text": "About\nOur input matrix has missing data. Here, I try to use Robust PCA by Candes et. al., 2011 to obtain a low rank matrix \\mathbf{L} by stripping out some noise \\mathbf{M} and then apply PCA on \\mathbf{L} \n\\mathbf{X} = \\mathbf{L} + \\mathbf{M}\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nfrom sklearn.decomposition import PCA\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename = f\"{data_dir}/prec_df.pkl\"\nbeta_df = pd.read_pickle(beta_df_filename)\nprec_df = pd.read_pickle(prec_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n\nCode\nmean_se = prec_df.apply(lambda x : 1 / np.sqrt(x)).replace([np.inf, -np.inf], np.nan).mean(axis = 0, skipna = True)\nmean_se = pd.DataFrame(mean_se).set_axis([\"mean_se\"], axis = 1)\nbeta_std = beta_df.std(axis = 0, skipna = True)\nbeta_std = pd.DataFrame(beta_std).set_axis([\"beta_std\"], axis = 1)\nerror_df = pd.concat([mean_se, beta_std], axis = 1)\n\nselect_ids = error_df.query(\"mean_se &lt;= 0.2 and beta_std &lt;= 0.2\").index\nse_df = prec_df.apply(lambda x : 1 / np.sqrt(x)).replace([np.inf, -np.inf], np.nan)\n\nzscore_df = beta_df / se_df\nzscore_df = zscore_df.replace(np.nan, 0)\nX = np.array(zscore_df[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"After filtering, we have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\n\nAfter filtering, we have 69 samples (phenotypes) and 8403 features (variants)\n\n\n\n\nRobust PCA\n\n\nCode\ndef soft_thresholding(y: np.ndarray, mu: float):\n    \"\"\"\n    Soft thresholding operator as explained in Section 6.5.2 of https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf\n    Solves the following problem:\n    argmin_x (1/2)*||x-y||_F^2 + lmb*||x||_1\n\n    Parameters\n    ----------\n        y : np.ndarray\n            Target vector/matrix\n        lmb : float\n            Penalty parameter\n    Returns\n    -------\n        x : np.ndarray\n            argmin solution\n    \"\"\"\n    return np.sign(y) * np.clip(np.abs(y) - mu, a_min=0, a_max=None)\n\ndef svd_shrinkage(y: np.ndarray, tau: float):\n    \"\"\"\n    SVD shrinakge operator as explained in Theorem 2.1 of https://statweb.stanford.edu/~candes/papers/SVT.pdf\n    Solves the following problem:\n    argmin_x (1/2)*||x-y||_F^2 + tau*||x||_*\n    \n    Parameters\n    ----------\n        y : np.ndarray\n            Target vector/matrix\n        tau : float\n            Penalty parameter\n    Returns\n    -------\n        x : np.ndarray\n            argmin solution\n    \n    \"\"\"\n    U, s, Vh = np.linalg.svd(y, full_matrices=False)\n    s_t = soft_thresholding(s, tau)\n    return U.dot(np.diag(s_t)).dot(Vh)\n\nclass RobustPCA:\n    \"\"\"\n    Solves robust PCA using Inexact ALM as explained in Algorithm 5 of https://arxiv.org/pdf/1009.5055.pdf\n    Parameters\n    ----------\n        lmb: \n            penalty on sparse errors\n        mu_0: \n            initial lagrangian penalty\n        rho: \n            learning rate\n        tau:\n            mu update criterion parameter\n        max_iter:\n            max number of iterations for the algorithm to run\n        tol_rel:\n            relative tolerance\n        \n    \"\"\"\n    def __init__(self, lmb: float, mu_0: float=1e-5, rho: float=2, tau: float=10, \n                 max_iter: int=10, tol_rel: float=1e-3):\n        assert mu_0 &gt; 0\n        assert lmb &gt; 0\n        assert rho &gt; 1\n        assert tau &gt; 1\n        assert max_iter &gt; 0\n        assert tol_rel &gt; 0\n        self.mu_0_ = mu_0\n        self.lmb_ = lmb\n        self.rho_ = rho\n        self.tau_ = tau\n        self.max_iter_ = max_iter\n        self.tol_rel_ = tol_rel\n        \n    def fit(self, X: np.ndarray):\n        \"\"\"\n        Fits robust PCA to X and returns the low-rank and sparse components\n        Parameters\n        ----------\n            X:\n                Original data matrix\n\n        Returns\n        -------\n            L:\n                Low rank component of X\n            S:\n                Sparse error component of X\n        \"\"\"\n        assert X.ndim == 2\n        mu = self.mu_0_\n        Y = X / self._J(X, mu)\n        S = np.zeros_like(X)\n        S_last = np.empty_like(S)\n        for k in range(self.max_iter_):\n            # Solve argmin_L ||X - (L + S) + Y/mu||_F^2 + (lmb/mu)*||L||_*\n            L = svd_shrinkage(X - S + Y/mu, 1/mu)\n            \n            # Solve argmin_S ||X - (L + S) + Y/mu||_F^2 + (lmb/mu)*||S||_1\n            S_last = S.copy()\n            S = soft_thresholding(X - L + Y/mu, self.lmb_/mu)\n            \n            # Update dual variables Y &lt;- Y + mu * (X - S - L)\n            Y += mu*(X - S - L)\n            r, h = self._get_residuals(X, S, L, S_last, mu)\n            \n            # Check stopping cirteria\n            tol_r, tol_h = self._update_tols(X, L, S, Y)\n            if r &lt; tol_r and h &lt; tol_h:\n                break\n                \n            # Update mu\n            mu = self._update_mu(mu, r, h)\n            \n        return L, S\n            \n    def _J(self, X: np.ndarray, lmb: float):\n        \"\"\"\n        The function J() required for initialization of dual variables as advised in Section 3.1 of \n        https://people.eecs.berkeley.edu/~yima/matrix-rank/Files/rpca_algorithms.pdf            \n        \"\"\"\n        return max(np.linalg.norm(X), np.max(np.abs(X))/lmb)\n    \n    @staticmethod\n    def _get_residuals(X: np.ndarray, S: np.ndarray, L: np.ndarray, S_last: np.ndarray, mu: float):\n        primal_residual = np.linalg.norm(X - S - L, ord=\"fro\")\n        dual_residual = mu * np.linalg.norm(S - S_last, ord=\"fro\")\n        return primal_residual, dual_residual\n    \n    def _update_mu(self, mu: float, r: float, h: float):\n        if r &gt; self.tau_ * h:\n            return mu * self.rho_\n        elif h &gt; self.tau_ * r:\n            return mu / self.rho_\n        else:\n            return mu\n        \n    def _update_tols(self, X, S, L, Y):\n        tol_primal = self.tol_rel_ * max(np.linalg.norm(X), np.linalg.norm(S), np.linalg.norm(L))\n        tol_dual = self.tol_rel_ * np.linalg.norm(Y)\n        return tol_primal, tol_dual\n\n\n\n\nCode\nrpca = RobustPCA(lmb=0.0085, max_iter=1000)\nL, M = rpca.fit(Xcent)\n\n\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices=False)\npca_proj = U @ np.diag(S)\n\n\n\n\nCode\nLcent = L - np.mean(L, axis = 0, keepdims = True)\nMcent = M - np.mean(M, axis = 0, keepdims = True)\n\n\n\n\nCode\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Lcent, full_matrices = False)\nrpca_proj = U_rpca @ np.diag(S_rpca)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\n\nsvd_pc1 = pca_proj[:, idx1]\nsvd_pc2 = pca_proj[:, idx2]\nrpca_pc1 = rpca_proj[:, idx1]\nrpca_pc2 = rpca_proj[:, idx2]\n\nfig = plt.figure(figsize = (16, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    ax2.scatter(rpca_pc1[idx], rpca_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \nax2.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nax2.set_xlabel(f\"Component {idx1}\")\nax2.set_ylabel(f\"Component {idx2}\")\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.8, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, rpca_proj)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nnp.linalg.matrix_rank(L)\n\n14\n\n\n\nnp.linalg.matrix_rank(M)\n\n68"
  },
  {
    "objectID": "notebooks/explore/2023-10-23-structure-plot.html",
    "href": "notebooks/explore/2023-10-23-structure-plot.html",
    "title": "Structure plot from GWAS phenotypes",
    "section": "",
    "text": "Getting Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom scipy.stats import pearsonr\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nLoading data\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\nsnp_info_filename  = f\"{data_dir}/snp_info.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\nsnp_info  = pd.read_pickle(snp_info_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n\nCode\nzscore_df\n\n\n\n\n\n\n\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz\nCNCR_Insomnia_all\nGPC-NEO-NEUROTICISM\nIGAP_Alzheimer\nJones_et_al_2016_Chronotype\nJones_et_al_2016_SleepDuration\nMDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_...\nMDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUK...\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\n...\nieu-b-7\nieu-b-8\nieu-b-9\nocd_aug2017.txt.gz\npgc-bip2021-BDI.vcf.txt.gz\npgc-bip2021-BDII.vcf.txt.gz\npgc-bip2021-all.vcf.txt.gz\npgc.scz2\npgcAN2.2019-07.vcf.txt.gz\npts_all_freeze2_overall.txt.gz\n\n\n\n\nrs1000031\n-0.999531\n-0.327477\n1.241557\n0.441709\n-0.163658\n0.163658\n-0.336654\n-0.793129\n-1.075357\n-2.182304\n...\n0.532189\nNaN\nNaN\n-0.198735\n1.057089\n-0.269020\n1.279776\n-0.433158\n-1.573766\n-1.674269\n\n\nrs1000269\n-1.212805\n-1.046310\n0.741814\n-1.844296\n-2.673787\n-1.126391\n0.092067\n0.163246\n1.643581\n2.122280\n...\n1.665179\n-0.732000\n-0.699000\n0.100883\n-0.226381\n0.338368\n-0.924392\n0.832016\n0.681645\n-0.701776\n\n\nrs10003281\n-0.813444\n2.034345\n-1.750164\n-0.076778\n-0.954165\n1.805477\nNaN\nNaN\nNaN\nNaN\n...\n-0.475795\n4.437998\n2.366001\n0.967399\n0.286699\n-1.162661\n-0.199299\n0.014539\nNaN\n-1.379710\n\n\nrs10004866\n0.011252\n1.327108\n1.442363\n-1.215173\n-0.050154\n-1.439531\n2.458370\n2.407460\n-0.001038\n-1.678331\n...\n-1.234375\n-2.520001\n-0.593997\n-0.685110\n0.902252\n1.106939\n1.776456\n-1.654677\n-0.964630\n0.851608\n\n\nrs10005235\n0.612540\n-0.410609\n0.653087\n0.344062\n-2.183486\n1.514102\n-0.460191\n-0.393006\n1.015614\n0.180744\n...\n0.387805\n-0.345000\n-0.960998\n0.177317\n-1.339598\n1.795867\n-1.249969\n2.349671\n0.996305\n-0.333356\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrs9989571\n0.028306\n-0.208891\n0.366470\n0.821257\n0.453762\n-1.895698\n0.218149\n0.920789\n1.581000\n1.121551\n...\n-1.231511\n-0.820996\n0.712000\n-2.150176\n-0.877410\n-1.938969\n-2.729983\n3.207917\n1.469194\n-1.293122\n\n\nrs9991694\n-0.679790\n-1.005571\n0.753472\n-0.539271\n1.674665\n-2.862736\n-3.744820\n-3.583060\n-4.072853\n-2.804192\n...\n0.064417\nNaN\nNaN\n-2.884911\n-1.000231\n0.031860\n-1.248222\n2.309425\nNaN\n1.048454\n\n\nrs9992763\n0.691405\n-0.010299\n-0.140010\n-0.419843\n-0.138304\n0.568052\n0.019684\n-0.194404\n0.869694\n0.061210\n...\n0.191860\n-0.074000\n1.030997\n-0.228287\n-0.051297\n0.781766\n0.010638\n0.456681\n-0.503370\n-1.435277\n\n\nrs9993607\n-1.625392\n-0.391585\n0.514268\n0.027576\n0.150969\n-0.113039\n-4.638940\n-4.631950\n-2.918354\n-2.204015\n...\n-0.685106\n0.194000\n0.240001\n-0.790290\n-0.876804\n-0.577696\n-0.785670\n-0.062707\n0.240834\n-0.199740\n\n\nrs999494\n-0.303642\n0.872613\n-0.227674\n1.390424\n0.138304\n1.281552\n1.873050\n1.645590\n1.381878\n-0.010875\n...\n-0.437500\n0.253000\n-0.926997\n-1.449674\n0.910515\n0.783853\n1.376043\n-5.195746\n-1.151316\n0.660120\n\n\n\n\n10068 rows × 69 columns\n\n\n\n\nX_nan = np.array(zscore_df).T\nX_nan_cent = X_nan - np.nanmean(X_nan, axis = 0, keepdims = True)\nX_nan_mask = np.isnan(X_nan)\nX_cent = np.nan_to_num(X_nan_cent, copy = True, nan = 0.0)\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(X_nan_mask) / np.prod(X_cent.shape):.3f}\")\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\nFraction of Nan entries: 0.193\n\n\n\nselect_ids = zscore_df.columns\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\nnsample = X_cent.shape[0]\nntrait  = len(unique_labels)\n\ntrait_indices = [np.array([i for i, x in enumerate(labels) if x == label]) for label in unique_labels]\ntrait_colors  = {trait: color for trait, color in zip(unique_labels, (mpl_stylesheet.kelly_colors())[:ntrait])}\n\nWe perform PCA (using SVD) on the raw input data (mean centered). In ?@fig-input-pca-pve, we look at the proportion of variance explained by each principal component.\n\n\nLow rank matrices\nWe have run 3 different methods to obtain low rank matrices, namely IALM, NNM and NNM-Sparse. Here, we load the results from those methods.\n\n\nCode\nmf_methods = ['ialm', 'nnm', 'nnm_sparse']\nlowrank_X = dict()\n\nfor method in mf_methods:\n    with open (f\"{data_dir}/lowrank_X_{method}.pkl\", 'rb') as handle:\n        lowrank_X[method] = pickle.load(handle)\n\n\n\n\nPrincipal components\nSuppose, we decompose \\mathbf{X} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\intercal}. Columns of \\mathbf{V} are the principal axes (aka principal directions, aka eigenvectors). The principal components are the columns of \\mathbf{U}\\mathbf{S} – the projections of the data on the the principal axes (note \\mathbf{X}\\mathbf{V} = \\mathbf{U}\\mathbf{S}).\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps, S\n\nloadings  = dict()\npcomps    = dict()\neigenvals = dict()\n\nloadings['tsvd'], pcomps['tsvd'], eigenvals['tsvd'] = get_principal_components(X_cent)\nfor m in mf_methods:\n    loadings[m], pcomps[m], eigenvals[m] = get_principal_components(lowrank_X[m])\n\n\nIn Figure 1, we look at the eigenvalues obtained from PCA of the low rank matrices. The first plot on the left shows the PCA on the raw data, without noise removal. It is evident that a truncated SVD (as shown below) will not be able to capture the variance in the data.\n\n\nCode\nplot_methods = ['tsvd'] + mf_methods\nplot_methods_names = {\n    'tsvd' : 'Raw Data',\n    'ialm' : 'RPCA-IALM',\n    'nnm'  : 'NNM-FW',\n    'nnm_sparse' : 'NNM-Sparse-FW',\n}\n\nfig = plt.figure(figsize=(18, 5))\nax = [None for i in range(len(plot_methods))]\n\nfor i, m in enumerate(plot_methods):\n    ax[i] = fig.add_subplot(1, len(plot_methods), i+1)\n    S = eigenvals[m]\n    S2 = np.square(S)\n    ax[i].plot(np.arange(S.shape[0]), np.cumsum(S2 / np.sum(S2)), 'o-')\n    ax[i].set_title(plot_methods_names[m])\n\nplt.tight_layout(w_pad = 2)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Scree plots from PCA of low rank matrices\n\n\n\n\n\n\n\nContribution of principal components on phenotypes\nWe look at the contribution of the loadings on the phenotypes using structure plot. The importance of a principal component is reflected by its inertia or by the proportion of the total inertia “explained” by this factor. We can calculate the contribution of an observation to the component or the contribution of the component to an observation.\nSuppose, the principal components are \\mathbf{F} = \\mathbf{U}\\mathbf{S}. Then, the squared cosine shows the importance of a component for a given observation. \\cos_{i, l}^2 = \\frac{f_{i,l}^2}{\\sum_{l}f_{i,l}^2}\nIntuitively, the above factor corresponds to the square of the cosine of the angle from the right triangle made with the origin, the observation and its projection on the component.\nSee better explanation\n\n\nCode\ndef get_cos2_scores(pcomps):\n    ntrait, npcomp = pcomps.shape\n    x = np.zeros((ntrait, npcomp))\n    for i in range(ntrait):\n        cos2_trait = np.array([np.square(pcomps[i, pcidx]) for pcidx in range(npcomp)])\n        x[i, :] = cos2_trait / np.sum(cos2_trait)\n    return x\n\ndef stacked_barplot(ax, data, xlabels, colors, bar_width = 1.0, alpha = 1.0, showxlabels = False):\n    '''\n    Parameters\n    ----------\n        data: \n            dict() of scores. \n            - &lt;key&gt; : items for the stacked bars (e.g. traits or components)\n            - &lt;value&gt; : list of scores for the items. All dict entries must have the same length of &lt;value&gt;\n        xlabels: \n            label for each entry in the data &lt;value&gt; list. Must be of same length of data &lt;value&gt;\n        colors: \n            dict(&lt;key&gt;, &lt;color&gt;) corresponding to each data &lt;key&gt;.\n    '''\n    indices = np.arange(len(xlabels))\n    bottom = np.zeros(len(xlabels))\n\n    for item, weights in data.items():\n        ax.bar(indices, weights, bar_width, label = item, bottom = bottom, color = colors[item], alpha = alpha)\n        bottom += weights\n\n    if showxlabels:\n        ax.set_xticks(indices)\n        ax.set_xticklabels(xlabels, rotation=90, ha='center')\n        ax.tick_params(bottom = True, top = False, left = False, right = False,\n                   labelbottom = True, labeltop = False, labelleft = False, labelright = False)\n    else:\n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n\n    for side, border in ax.spines.items():\n        border.set_visible(False)\n\n    return\n\n\ndef structure_plot(ax, pcomps, trait_labels, comp_colors, npcomp, showxlabels = False):\n    cos2_scores = get_cos2_scores(pcomps)[:, :npcomp]\n    cos2_plot_data = {\n        f\"{i+1}\" : cos2_scores[:, i] for i in range(npcomp)\n    }\n    stacked_barplot(ax, cos2_plot_data, trait_labels, comp_colors, alpha = 0.8, showxlabels = showxlabels)\n    return\n\n\nIn Figure 2, we look at the contribution of the top 10 principal components to the disease phenotypes.\n\n\nCode\nphenotype_dict_readable = {\n    'AD_sumstats_Jansenetal_2019sept.txt.gz' : 'AD_Jansen_2019',\n    'anxiety.meta.full.cc.txt.gz' : 'anxiety',\n    'anxiety.meta.full.fs.txt.gz' : 'anxiety',\n    'CNCR_Insomnia_all' : 'Insomnia',\n    'daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz' : 'ADHD_Daner',\n    'daner_PGC_BIP32b_mds7a_0416a.txt.gz' : 'BD_Daner_PGC',\n    'daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz' : 'BD1_Daner_PGC',\n    'daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz' : 'BD2_Daner_PGC',\n    'ENIGMA_Intracraneal_Volume' : 'Intracraneal_Volume',\n    'ieu-a-1000' : 'Neuroticism',\n    'ieu-a-1041' : 'Intracranial volume',\n    'ieu-a-1042' : 'Nucleus accumbens volume',\n    'ieu-a-1043' : 'Amygdala volume',\n    'ieu-a-1044' : 'Caudate volume',\n    'ieu-a-1045' : 'Hippocampus volume',\n    'ieu-a-1046' : 'Pallidum volume',\n    'ieu-a-1047' : 'Putamen volume',\n    'ieu-a-1048' : 'Thalamus volume',\n    'ieu-a-1085' : 'Amyotrophic lateral sclerosis',\n    'ieu-a-118' : 'Neuroticism',\n    'ieu-a-1183' : 'ADHD',\n    'ieu-a-1184' : 'Autism Spectrum Disorder',\n    'ieu-a-1185' : 'Autism Spectrum Disorder',\n    'ieu-a-1186' : 'Anorexia Nervosa',\n    'ieu-a-1188' : 'Major Depressive Disorder',\n    'ieu-a-1189' : 'Obsessive Compulsive Disorder',\n    'ieu-a-22' : 'Schizophrenia',\n    'ieu-a-297' : 'Alzheimers disease',\n    'ieu-a-806' : 'Autism',\n    'ieu-a-990' : 'Bulimia nervosa',\n    'ieu-b-10' : 'Focal epilepsy',\n    'ieu-b-11' : 'Focal epilepsy',\n    'ieu-b-12' : 'Juvenile absence epilepsy',\n    'ieu-b-13' : 'Childhood absence epilepsy',\n    'ieu-b-14' : 'Focal epilepsy',\n    'ieu-b-15' : 'Focal epilepsy',\n    'ieu-b-16' : 'Generalized epilepsy',\n    'ieu-b-17' : 'Juvenile myoclonic epilepsy',\n    'ieu-b-18' : 'Multiple sclerosis',\n    'ieu-b-2' : 'Alzheimers disease',\n    'ieu-b-41' : 'Bipolar Disorder',\n    'ieu-b-42' : 'Schizophrenia',\n    'ieu-b-7' : 'Parkinsons',\n    'ieu-b-8' : 'Epilepsy',\n    'ieu-b-9' : 'Generalized epilepsy',\n    'IGAP_Alzheimer' : 'IGAP_Alzheimer',\n    'iPSYCH-PGC_ASD_Nov2017.txt.gz' : 'ASD_PGC_Nov2017',\n    'Jones_et_al_2016_Chronotype' : 'Chronotype_Jones_2016',\n    'Jones_et_al_2016_SleepDuration' : 'Sleep_duration_Jones_2016',\n    'MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz' : 'MDD_BIP_no23andMe_noUKBB',\n    'MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz' : 'MDD_METACARPA_no23andMe_noUKBB',\n    'MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Depression',\n    'MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Recurrent_Depression',\n    'MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Single_Depression',\n    'MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Subthreshold_WG',\n    'ocd_aug2017.txt.gz' : 'OCD_aug2017',\n    'PGC_ADHD_EUR_2017' : 'ADHD_PGC_EUR_2017',\n    'PGC_ASD_2017_CEU' : 'ASD_PGC_2017_CEU',\n    'pgc-bip2021-all.vcf.txt.gz' : 'BD_PGC_all_2021',\n    'pgc-bip2021-BDI.vcf.txt.gz' : 'BDI_PGC_2021',\n    'pgc-bip2021-BDII.vcf.txt.gz' : 'BDII_PGC_2021',\n    'pgc.scz2' : 'Schizophrenia_PGC_2',\n    'PGC3_SCZ_wave3_public.v2.txt.gz' : 'Schizophrenia_PGC_3',\n    'pgcAN2.2019-07.vcf.txt.gz' : 'pgcAN2.2019-07.vcf.txt.gz',\n    'pts_all_freeze2_overall.txt.gz' : 'pts_all_freeze2',\n    'SSGAC_Depressive_Symptoms' : 'SSGAC_Depressive_Symptoms',\n    'SSGAC_Education_Years_Pooled' : 'SSGAC_Education_Years_Pooled',\n    'UKB_1160_Sleep_duration' : 'UKB_1160_Sleep_duration',\n    'UKB_1180_Morning_or_evening_person_chronotype' : 'UKB_1180_Morning_or_evening_person_chronotype',\n    'UKB_1200_Sleeplessness_or_insomnia' : 'UKB_1200_Sleeplessness_or_insomnia',\n    'UKB_20002_1243_self_reported_psychological_or_psychiatric_problem' : 'UKB_20002_1243_self_reported_psychological_or_psychiatric_problem',\n    'UKB_20002_1262_self_reported_parkinsons_disease' : 'UKB_20002_1262_self_reported_parkinsons_disease',\n    'UKB_20002_1265_self_reported_migraine' : 'UKB_20002_1265_self_reported_migraine',\n    'UKB_20002_1289_self_reported_schizophrenia' : 'UKB_20002_1289_self_reported_schizophrenia',\n    'UKB_20002_1616_self_reported_insomnia' : 'UKB_20002_1616_self_reported_insomnia',\n    'UKB_20016_Fluid_intelligence_score' : 'UKB_20016_Fluid_intelligence_score',\n    'UKB_20127_Neuroticism_score' : 'UKB_20127_Neuroticism_score',\n    'UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy' : 'UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy',\n    'UKB_G43_Diagnoses_main_ICD10_G43_Migraine' : 'UKB_G43_Diagnoses_main_ICD10_G43_Migraine',\n    'ieu-b-5070' : 'Schizophrenia',\n    'GPC-NEO-NEUROTICISM' : 'GPC-NEO-NEUROTICISM',\n    'ieu-a-1009' : 'Subjective well being',\n    'ieu-a-1018' : 'Subjective well being',\n    'ieu-a-1019' : 'Migraine in bipolar disorder',\n    'ieu-a-1029' : 'Internalizing problems',\n    'ieu-a-1061' : 'G speed factor',\n    'ieu-a-1062' : 'Symbol search',\n    'ieu-a-1063' : '8-choice reaction time',\n    'ieu-a-1064' : '2-choice reaction time',\n    'ieu-a-1065' : 'Inspection time',\n    'ieu-a-1066' : 'Simple reaction time',\n    'ieu-a-1067' : 'Digit symbol',\n    'ieu-a-1068' : '4-choice reaction time',\n    'ieu-a-45' : 'Anorexia nervosa',\n    'ieu-a-298' : 'Alzheimers Disease',\n    'ieu-a-808' : 'Bipolar Disorder',\n    'ieu-a-810' : 'Schizophrenia',\n    'ieu-a-812' : 'Parkinsons',\n    'ieu-a-818' : 'Parkinsons',\n    'ieu-a-824' : 'Alzheimers Disease',\n    'ieu-b-43' : 'frontotemporal dementia',\n    'ILAE_Genetic_generalised_epilepsy' : 'ILAE_Genetic_generalised_epilepsy'\n}\n\nlabels_readable = [phenotype_dict_readable[x] for x in select_ids]\n\n\n\n\nCode\n\"\"\"\nNumber of components to plot\n\"\"\"\nnpcomp = 10\n    \n\"\"\"\nSort the traits / phenotypes\n\"\"\"\ntrait_indices_sorted = list()\nfor idx in trait_indices:\n    trait_indices_sorted += list(idx)\n\ntrait_labels_sorted = [labels_readable[i] for i in trait_indices_sorted]\npcomp_colors  = {f\"{i+1}\": color for i, color in enumerate(mpl_stylesheet.kelly_colors() + mpl_stylesheet.banskt_colors())}\n    \nfig = plt.figure(figsize = (18, 18))\ngs = GridSpec(nrows = len(plot_methods) + 1, ncols=1, figure=fig, height_ratios=[0.3] + [1 for i in plot_methods])\nax = [None for i in range(len(plot_methods) + 1)]\nax[0] = fig.add_subplot(gs[0, 0])\n\nfor i, m in enumerate(plot_methods):\n    iplot = i + 1\n    showxlabels = True if iplot == len(plot_methods) else False\n    ax[iplot] = fig.add_subplot(gs[iplot, 0])\n    structure_plot(ax[iplot], pcomps[m][trait_indices_sorted,:], trait_labels_sorted, pcomp_colors, npcomp, showxlabels = showxlabels)\n    ax[iplot].set_title(plot_methods_names[m])\n    \nplt_handles, plt_labels = ax[i].get_legend_handles_labels()\nax[0].legend(plt_handles, plt_labels, \n             loc = 'lower center', bbox_to_anchor=(0.5, 0), title = \"Principal Components\",\n             frameon = False, handlelength = 8, ncol = 5)\nfor side, border in ax[0].spines.items():\n    border.set_visible(False)\nax[0].tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n\n#legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Contribution of each principal component to phenotypes\n\n\n\n\n\n\n\nWhich SNPs are associated with the principal components (hidden factors)?\n\n\nCode\ndef get_corrmat(pcomps, x, ncomp = None):\n    if ncomp is None:\n        ncomp = pcomps.shape[1]\n    nvar  = x.shape[1]\n    corr  = np.zeros((nvar, ncomp))\n    pval  = np.zeros((nvar, ncomp))\n    for ivar in range(nvar):\n        d1 = x[:, ivar]\n        for icomp in range(ncomp):\n            d2 = pcomps[:, icomp]\n            corr[ivar, icomp], pval[ivar, icomp] = pearsonr(d1, d2)\n    return corr, pval\n\n\n\n\nCode\n'''\nCalculate the correlations and pvals, if required.\nThis takes time, so the pre-computed results can also be loaded from saved files\n'''\ncorrs = dict()\npvals = dict()\n\n# for m in plot_methods:\n#     corrs[m], pvals[m] = get_corrmat(pcomps[m], X_cent, ncomp = 20)\n#     with open (f\"{data_dir}/loading_corr_{m}.pkl\", 'wb') as handle:\n#         pickle.dump(corrs[m], handle, protocol=pickle.HIGHEST_PROTOCOL)\n#     with open (f\"{data_dir}/loading_corr_pval_{m}.pkl\", 'wb') as handle:\n#         pickle.dump(pvals[m], handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nfor m in plot_methods:\n    with open (f\"{data_dir}/loading_corr_{m}.pkl\", 'rb') as handle:\n        corrs[m] = pickle.load(handle)\n    with open (f\"{data_dir}/loading_corr_pval_{m}.pkl\", 'rb') as handle:\n        pvals[m] = pickle.load(handle)\n\n\n\n\nCode\ndef get_total_snps(sdict):\n    stot = {i + 1 : 0 for i in range(22)}\n    for snp, info in sdict.items():\n        chrm = int(info['CHR'])\n        bppos = info['BP']\n        if bppos &gt; stot[chrm]:\n            stot[chrm] = bppos\n    return stot\n\nrsid_list = zscore_df.index\nsnp_info_dict = snp_info.set_index('SNP').to_dict(orient = 'index')\nsnp_tot = get_total_snps(snp_info_dict)\n\n\n\n\nCode\ndef corr_to_manhattan_data(corr_data, pcidx):\n    data = {i+1: dict() for i in range(22)}\n    for i, val in enumerate(corr_data[:, pcidx]):\n        rsid = rsid_list[i]\n        chrm = int(snp_info_dict[rsid]['CHR'])\n        bppos = snp_info_dict[rsid]['BP']\n        data[chrm][bppos] = np.square(val)\n    return data\n\ndef pval_to_manhattan_data(pval_data, pcidx):\n    data = {i+1: dict() for i in range(22)}\n    for i, val in enumerate(pval_data[:, pcidx]):\n        rsid = rsid_list[i]\n        chrm = int(snp_info_dict[rsid]['CHR'])\n        bppos = snp_info_dict[rsid]['BP']\n        data[chrm][bppos] = - np.log10(val)\n    return data\n\ndef plot_manhattan(ax, data, ylabel, showx = True):\n    i = 0\n    start = 0\n    end = 0\n    xtickposlist = list()\n    offcolors = ['darkorange', 'firebrick']\n    \n    for chrm, cvals in data.items():\n        end = start + snp_tot[chrm]\n        xtickposlist.append(int((start + end) / 2))\n        x = [ start + bp for bp in list(cvals.keys())]\n        y = list(cvals.values())\n        ax.scatter(x, y, color=offcolors[chrm%2], s = 3, alpha = 0.8)  \n        start = end\n        \n    ax.set_xlim(0, end)\n    #ax.set_ylim(0, 1)\n    #ax.plot([0, lastbp], [log10cutoff, log10cutoff], ls = 'dashed', color='gainsboro', lw = 2)\n    #ax.text(0.05, 1.0, tname, transform=ax.transAxes, ha='left', va='top')\n    for side, border in ax.spines.items():\n        if side == 'top':\n            border.set_visible(False)\n            \n    ax.set_xticks(xtickposlist)\n    ax.tick_params(axis='x', labelsize=16)\n    ax.set_ylabel(ylabel)\n    if showx:\n        ax.set_xticklabels([\"{:d}\".format(x) for x in data.keys()], rotation = 90)\n    else:\n        ax.set_xticklabels([\"\" for x in data.keys()])\n    return\n\n\nIn Figure 3, we show the Manhattan plot for the association of each variant with the principal components. We calculate the Pearson’s coefficient of correlation between the variants’ z-scores and the principal components and use the -\\log_{10}(p) value as the significance of the correlation.\n\n\nCode\nimport matplotlib.patches as mpatches\n\nfig = plt.figure(figsize = (18, 2 * npcomp + (npcomp - 1) * 1))\n\nwhichmethod = 'nnm_sparse'\n\nfor icomp in range(npcomp):\n    ax = fig.add_subplot(npcomp, 1, icomp+1)\n    pval_data = pval_to_manhattan_data(pvals[whichmethod], icomp)\n    showx = True if icomp == npcomp - 1 else False\n    plot_manhattan(ax, pval_data, f\"PC{icomp + 1}\", showx = showx)\n    label_rect = ax.add_artist(mpatches.Rectangle((0.01, 0.9), 0.1, 0.1, edgecolor = 'none', facecolor = pcomp_colors[f\"{icomp + 1}\"], transform = ax.transAxes))\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Association of variants with principal components (hidden factors)"
  },
  {
    "objectID": "notebooks/explore/simplex-projection-methods.html",
    "href": "notebooks/explore/simplex-projection-methods.html",
    "title": "About",
    "section": "",
    "text": "About\nComparison of different algorithms for projection on \\ell_1 ball.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\nv_orig = np.random.normal(0, 1, 10)\n\n\n\n\nCode\n# def proj_l1ball_sort(y, a):\n#     if np.sum(y) == a and np.alltrue(y &gt;= 0):\n#         return y\n#     yabs = np.abs(y)\n#     u = np.sort(yabs)[::-1]\n#     ukvals = (np.cumsum(u) - a) / np.arange(1, y.shape[0] + 1)\n#     K = np.max(np.where(ukvals &lt; u))\n#     tau = ukvals[K]\n#     x = np.sign(y) * np.clip(yabs - tau, a_min=0, a_max=None)\n#     return x\n\ndef proj_simplex_sort(y, a = 1.0):\n    if np.sum(y) == a and np.alltrue(y &gt;= 0):\n        return y\n    u = np.sort(y)[::-1]\n    ukvals = (np.cumsum(u) - a) / np.arange(1, y.shape[0] + 1)\n    K = np.nonzero(ukvals &lt; u)[0][-1]\n    tau = ukvals[K]\n    x = np.clip(y - tau, a_min=0, a_max=None)\n    return x\n\ndef proj_l1ball_sort(y, a = 1.0):\n    return np.sign(y) * proj_simplex_sort(np.abs(y), a = a)\n\ndef l1_norm(x):\n    return np.sum(np.abs(x))\n\n\n\n\nCode\nl1_norm(proj_l1ball_sort(v_orig, 1.0))\n\n\n1.0\n\n\n\n\nCode\nl1_norm(v_orig)\n\n\n7.817970379144475\n\n\n\n\nCode\nproj_l1ball_sort(v_orig, 1.0)\n\n\narray([-0.00168506, -0.        ,  0.        ,  0.        ,  0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.99831494])\n\n\n\n\nCode\ndef proj_simplex_michelot(y, a = 1.0):\n    auxv = y.copy()\n    N = y.shape[0]\n    rho = (np.sum(y) - a) / N\n    istep = 0\n    vnorm_last = l1_norm(auxv)\n    while True:\n        istep += 1\n        allowed = auxv &gt; rho\n        auxv = auxv[allowed]\n        nv = np.sum(allowed)\n        vnorm = l1_norm(auxv)\n        if vnorm == vnorm_last:\n            break\n        rho = (np.sum(auxv) - a) / nv\n        vnorm_last = vnorm\n    x = np.clip(y - rho, a_min = 0, a_max = None)\n    return x\n    \ndef proj_l1_michelot(y, a = 1.0):\n    return np.sign(y) * proj_simplex_michelot(np.abs(y), a)\n\nproj_l1_michelot(v_orig)\n\n\narray([-0.00168506, -0.        ,  0.        ,  0.        ,  0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.99831494])\n\n\n\n\nCode\ndef proj_simplex_condat(y, a = 1.0):\n    auxv = np.array([y[0]])\n    vtilde = np.array([])\n    rho = y[0] - a\n    N = y.shape[0]\n    # Step 2\n    for i in range(1, N):\n        if y[i] &gt; rho:\n            rho += (y[i] - rho) / (auxv.shape[0] + 1)\n            if rho &gt; (y[i] - a):\n                auxv = np.append(auxv, y[i])\n            else:\n                vtilde = np.append(vtilde, auxv)\n                auxv = np.array([y[i]])\n                rho = y[i] - a\n    # Step 3\n    if vtilde.shape[0] &gt; 0:\n        for v in vtilde:\n            if v &gt; rho:\n                auxv = np.append(auxv, v)\n                rho += (v - rho) / (auxv.shape[0])                \n    # Step 4\n    nv_last = auxv.shape[0]\n    istep = 0\n    while True:\n        istep += 1\n        to_remove = list()\n        nv_ = auxv.shape[0]\n        for i, v in enumerate(auxv):\n            if v &lt;= rho:\n                to_remove.append(i)\n                nv_ = nv_ - 1\n                rho += (rho - v) / nv_\n        auxv = np.delete(auxv, to_remove)\n        nv = auxv.shape[0]\n        assert nv == nv_\n        if nv == nv_last:\n            break\n        nv_last = nv\n    # Step 5\n    x = np.clip(y - rho, a_min=0, a_max=None)\n    return x\n\ndef proj_l1_condat(y, a = 1.0):\n    return np.sign(y) * proj_simplex_condat(np.abs(y), a)\n\nproj_l1_condat(v_orig)\n\n\narray([-0.00168506, -0.        ,  0.        ,  0.        ,  0.        ,\n       -0.        ,  0.        ,  0.        ,  0.        ,  0.99831494])\n\n\n\n\nCode\n%%timeit\n\nproj_l1_condat(v_orig)\n\n\n84.8 µs ± 2.24 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nCode\n%%timeit\nproj_l1ball_sort(v_orig)\n\n\n56.4 µs ± 1.21 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nCode\nv_orig2 = np.random.normal(0, 1, 100000)\n\n\n\n\nCode\n%%timeit -n 1 -r 1\nproj_l1_condat(v_orig2)\n\n\n29.1 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\nCode\n%%timeit -n 1 -r 1\nproj_l1ball_sort(v_orig2)\n\n\n12.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)"
  },
  {
    "objectID": "notebooks/explore/2024-01-20-matrix-reconstruction-ukbb.html",
    "href": "notebooks/explore/2024-01-20-matrix-reconstruction-ukbb.html",
    "title": "Generate a masked input",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\nfrom matplotlib.gridspec import GridSpec\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\nCode\ndata_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/data\"\nresult_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/nnwmf\"\nzscore_filename = f\"{data_dir}/GWAS_Zscore.tsv\"\ntrait_filename = f\"{data_dir}/trait_manifest_TableS6_no_readme.tsv\"\nzscore_df = pd.read_csv(zscore_filename, sep = '\\t')\ntrait_df = pd.read_csv(trait_filename, sep = '\\t')\n\n# remove extra columns from trait_df\n\ncolnames = trait_df.columns.tolist()\ncolnames[0] = \"zindex\"\ntrait_df.columns = colnames\ntrait_df_mod = trait_df.drop(labels = ['coding', 'modifier', 'coding_description', 'filename', 'aws_link'], axis=1)\n#trait_df_mod\nzscore_df\n\n\n\n\n\n\n\n\nrsid\nz1\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\n...\nz2474\nz2475\nz2476\nz2477\nz2478\nz2479\nz2480\nz2481\nz2482\nz2483\n\n\n\n\n0\nrs6657440\n-0.903532\n0.561842\n0.711068\n-0.109174\n0.223668\n-1.728199\n0.374988\n-0.265971\n-2.823282\n...\n1.521092\n0.612532\n1.405428\n0.018029\n0.895337\n-0.008761\n-2.069432\n-4.292948\n-4.701711\n2.952899\n\n\n1\nrs7418179\n0.398166\n1.163539\n0.512118\n0.144794\n-1.313903\n-1.547410\n0.450270\n0.560324\n-1.502268\n...\n-0.296537\n-0.734266\n-0.093081\n0.412077\n1.961159\n0.716049\n-2.171984\n-5.314085\n-6.612137\n3.817518\n\n\n2\nrs80125161\n-1.739115\n-0.172328\n0.349145\n-0.329335\n-0.870640\n-1.004155\n1.128148\n0.151244\n-1.816075\n...\n2.222433\n1.092969\n2.328233\n1.160767\n0.909524\n-1.467249\n-0.135785\n-2.187241\n-3.223529\n4.508578\n\n\n3\nrs7524174\n-0.884478\n-1.762000\n1.312823\n-0.550764\n2.132540\n0.519828\n0.834194\n0.699441\n-0.885281\n...\n3.356354\n1.990588\n3.092179\n-0.133810\n-0.072845\n-1.376310\n1.317044\n0.913491\n0.535188\n2.245657\n\n\n4\nrs3829740\n-1.469931\n-0.519628\n-0.281605\n-0.267729\n-1.060167\n0.058116\n-0.638319\n-0.589767\n0.228514\n...\n-0.320075\n-0.128047\n-0.524757\n-0.232900\n-1.051020\n-0.483644\n2.026508\n4.400092\n5.407316\n1.125536\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n51394\nrs9616937\n-0.211947\n1.371231\n-1.800776\n0.609980\n-0.619822\n0.947269\n-1.166021\n0.478601\n-0.359714\n...\n0.714167\n0.354347\n0.611158\n-0.354725\n1.073043\n-0.831737\n0.870924\n1.432076\n2.228501\n0.536104\n\n\n51395\nrs1024374\n0.027097\n-1.817082\n0.530216\n0.813498\n-0.076514\n0.784427\n1.411160\n-1.111740\n-0.224438\n...\n1.107098\n1.482684\n1.512723\n0.322355\n-0.374603\n1.320194\n-0.700092\n-1.395039\n-2.270186\n0.360025\n\n\n51396\nrs144480800\n0.545682\n0.391830\n0.520505\n-1.280976\n0.453876\n-1.388940\n0.025094\n0.737788\n1.178641\n...\n-0.562063\n-1.148515\n-0.994185\n-0.268232\n-0.069619\n0.013256\n-0.777667\n-1.544760\n-1.406344\n2.205817\n\n\n51397\nrs5770994\n1.441851\n1.152368\n-1.500000\n-0.468137\n-0.444156\n-0.780139\n-0.853550\n-0.316097\n0.311219\n...\n-1.185702\n-0.624073\n-0.859522\n0.549669\n1.809912\n0.268733\n0.947441\n1.533302\n1.658537\n2.218653\n\n\n51398\nrs9616824\n0.669372\n-0.308184\n-0.811311\n-0.154224\n1.476823\n0.065167\n-1.030063\n-0.215723\n-0.631385\n...\n0.308486\n0.612102\n0.492977\n-0.621278\n-2.657386\n-0.732402\n-0.106474\n-0.029006\n0.070647\n-0.805253\n\n\n\n\n51399 rows × 2484 columns\nCode\n# Randomly select 10000 rows and 500 columns\nzscore_df_sampled = zscore_df.sample(n=10000, random_state = 42).sample(n=500, axis=1, random_state = 43)\nzscore_df_sampled\n\n\n\n\n\n\n\n\n\nz1167\nz281\nz2357\nz226\nz743\nz560\nz2069\nz2252\nz1644\nz491\n...\nz1779\nz1669\nz152\nz131\nz57\nz2225\nz2470\nz106\nz474\nz1160\n\n\n\n\n21135\n1.747967\n0.333150\n-0.256725\n1.101227\n1.877049\n0.653582\n2.184874\n0.649850\n0.624629\n-0.568762\n...\n-0.196378\n-1.007667\n0.916585\n0.108240\n0.540421\n-1.211147\n-0.751328\n0.078602\n0.253098\n0.203668\n\n\n36838\n-0.156658\n-0.492547\n-1.998114\n-1.086678\n0.063518\n-1.942668\n0.305734\n-0.919531\n-0.581345\n-0.129932\n...\n1.889727\n-0.108175\n-1.346732\n-0.801298\n1.277992\n4.373343\n1.351446\n-0.520159\n-1.190272\n0.731311\n\n\n26302\n-0.370480\n-1.051778\n-0.267559\n1.269372\n-0.370917\n0.783508\n1.393201\n-1.027494\n0.439653\n0.497404\n...\n-0.619194\n0.688585\n-0.695459\n-0.783111\n-0.642857\n4.030124\n0.826335\n1.014103\n1.701609\n0.433085\n\n\n35137\n-0.191911\n0.976049\n0.432594\n0.988569\n0.771049\n-1.526316\n-1.123623\n-0.723978\n0.883248\n0.618741\n...\n0.493395\n-0.514462\n-0.225855\n0.797255\n-0.076689\n0.757935\n3.851805\n-1.726221\n-0.194140\n0.040497\n\n\n25047\n2.262365\n-1.031990\n-0.402558\n2.940195\n0.545873\n1.604303\n-1.530728\n-2.784586\n-1.262213\n-0.455971\n...\n-0.752998\n-0.380618\n-0.508555\n1.104593\n0.556324\n0.495699\n0.744503\n1.554382\n-0.383324\n1.121189\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n44074\n-0.262920\n-1.041327\n1.641563\n-0.402090\n-1.077609\n-0.489245\n0.953831\n-0.653117\n-1.914530\n0.216378\n...\n0.682033\n0.072718\n-1.416000\n1.450482\n-0.228025\n0.266796\n-4.409254\n1.349206\n-0.533561\n0.779899\n\n\n28935\n0.162851\n1.425620\n0.522926\n-1.351447\n0.141509\n0.261944\n1.089231\n1.451613\n2.439355\n-0.309807\n...\n0.336451\n1.515305\n0.101981\n0.770801\n0.810334\n6.099345\n0.834298\n-0.259695\n0.294474\n-1.023478\n\n\n25329\n-0.195354\n-0.523103\n1.113656\n0.072485\n1.593814\n-1.990619\n-0.359724\n-0.447374\n1.534896\n-0.686970\n...\n0.081670\n-1.757484\n0.149871\n-0.367180\n-1.817944\n3.461449\n0.462327\n1.543165\n-0.364580\n0.585727\n\n\n22955\n1.236731\n1.064055\n-0.461035\n-0.702339\n-1.697798\n2.532889\n-0.161650\n-0.642910\n0.868244\n-0.398903\n...\n1.495010\n1.105210\n-0.106106\n1.004655\n-1.210867\n-1.722286\n2.081974\n-0.419449\n-0.989933\n-0.221453\n\n\n19274\n-1.146507\n-2.035411\n1.877513\n-0.835974\n0.134975\n-1.500741\n0.129923\n3.320623\n-0.383444\n-0.290063\n...\n0.729734\n0.295815\n-0.305492\n0.886234\n-1.774148\n-0.205381\n-0.784586\n0.601887\n0.047579\n-1.494981\n\n\n\n\n10000 rows × 500 columns\nCode\nX = np.array(zscore_df_sampled.loc[:, zscore_df_sampled.columns!='rsid']).T\nX_cent = X - np.mean(X, axis = 0, keepdims = True)\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(np.isnan(X)) / np.prod(X_cent.shape):.3f}\")\n\n\nWe have 500 samples (phenotypes) and 10000 features (variants)\nFraction of Nan entries: 0.000\nCode\nimport scipy.stats as sc_stats\n\ndef get_density(x, data):\n    density = sc_stats.gaussian_kde(data)\n    return density.pdf(x)\n\ndef get_bins(data, nbin, xmin, xmax):\n    xdelta = (np.max(data) - np.min(data)) / 10\n    if not xmin: xmin = np.min(data) - xdelta\n    if not xmax: xmax = np.max(data) + xdelta\n    bins = np.linspace(xmin, xmax, nbin)\n    xbin = [(bins[i] + bins[i+1]) / 2 for i in range(bins.shape[0] - 1)] # centers of the bins\n    return xmin, xmax, bins, xbin\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor n in range(100):\n    scores_cent = X_cent[n, :]\n    xmin, xmax, bins, xbin = get_bins(scores_cent, 100, -10, 10)\n    curve = get_density(xbin, scores_cent)\n    ax1.plot(xbin, curve)\n\nplt.show()\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n                                            \n#mpy_plotfn.plot_covariance_heatmap(ax1, Z * np.sqrt(p) / np.sqrt(nsample))\nmpy_plotfn.plot_covariance_heatmap(ax1, X)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2024-01-20-matrix-reconstruction-ukbb.html#generate-a-masked-input",
    "href": "notebooks/explore/2024-01-20-matrix-reconstruction-ukbb.html#generate-a-masked-input",
    "title": "Generate a masked input",
    "section": "Generate a masked input",
    "text": "Generate a masked input\n\n\nCode\ndef generate_masked_input(Y, mask):\n    Ymiss_nan = Y.copy()\n    Ymiss_nan[mask] = np.nan\n    Ymiss_nan_cent = Ymiss_nan - np.nanmean(Ymiss_nan, axis = 0, keepdims = True)\n    Ymiss_nan_cent[mask] = 0.0\n    return Ymiss_nan_cent\n\ndef generate_mask(n, p, ratio):\n    mask = np.ones(n * p)\n    nzero = int(ratio * n * p)\n    mask[:nzero] = 0.0\n    np.random.shuffle(mask)\n    return mask.reshape(n,p) == 0.\n\nZ_mask = generate_mask(X.shape[0], X.shape[1], 0.33)\nZ_cent = generate_masked_input(X_cent, Z_mask)"
  },
  {
    "objectID": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html",
    "href": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html",
    "title": "Application of denoising methods on GWAS phenotypes",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn"
  },
  {
    "objectID": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#ialm---rpca",
    "href": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#ialm---rpca",
    "title": "Application of denoising methods on GWAS phenotypes",
    "section": "IALM - RPCA",
    "text": "IALM - RPCA\n\n\nCode\nrpca = IALM(max_iter = 1000, mu_update_method='admm', show_progress = True)\nrpca.fit(X_cent, mask = X_nan_mask)\n\n\n2023-09-25 14:36:21,729 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0100)\n2023-09-25 14:36:21,904 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 0. Primal residual 0.893741. Dual residual 0.000574896\n2023-09-25 14:36:29,620 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 100. Primal residual 6.36112e-05. Dual residual 2.55942e-05\n2023-09-25 14:36:37,295 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 200. Primal residual 2.26287e-05. Dual residual 2.64972e-06\n2023-09-25 14:36:44,963 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 300. Primal residual 6.03351e-06. Dual residual 1.69738e-06\n2023-09-25 14:36:52,645 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 400. Primal residual 4.08978e-06. Dual residual 8.33885e-07\n2023-09-25 14:37:00,403 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 500. Primal residual 3.12383e-06. Dual residual 5.16786e-07\n2023-09-25 14:37:08,104 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 600. Primal residual 2.50002e-06. Dual residual 3.39206e-07\n2023-09-25 14:37:15,875 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 700. Primal residual 2.03607e-06. Dual residual 3.14692e-07\n2023-09-25 14:37:23,599 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 800. Primal residual 8.29061e-07. Dual residual 5.20502e-07\n2023-09-25 14:37:31,333 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 900. Primal residual 4.70128e-07. Dual residual 2.64999e-07\n\n\n\n\nCode\nnp.linalg.matrix_rank(rpca.L_)\n\n\n50\n\n\n\n\nCode\nnp.linalg.norm(rpca.L_, 'nuc')\n\n\n1351.8480150432201\n\n\n\n\nCode\nnp.sum(np.abs(rpca.E_)) / np.prod(X_cent.shape)\n\n\n0.5906261807412275"
  },
  {
    "objectID": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#fw---nnm",
    "href": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#fw---nnm",
    "title": "Application of denoising methods on GWAS phenotypes",
    "section": "FW - NNM",
    "text": "FW - NNM\nAlso check how the cross-validation works\n\n\nCode\nnnmcv = FrankWolfe_CV(chain_init = True, reverse_path = False, debug = True, kfolds = 5)\nnnmcv.fit(X_nan_cent)\n\n\n2023-08-08 23:54:07,309 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Cross-validation over 14 ranks.\n2023-08-08 23:54:07,338 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 1 ...\n2023-08-08 23:54:07,359 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-08 23:54:09,099 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-08 23:54:10,281 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-08 23:54:14,327 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-08 23:54:15,509 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-08 23:54:17,192 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-08 23:54:21,295 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-08 23:54:40,008 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-08 23:55:00,369 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-08 23:55:49,015 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-08 23:56:26,738 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-08 23:59:33,286 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 00:04:42,654 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 00:11:05,668 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 00:13:31,487 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 2 ...\n2023-08-09 00:13:31,526 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 00:13:32,692 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 00:13:33,858 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 00:13:35,041 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 00:13:36,189 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 00:13:39,628 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 00:13:44,260 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 00:14:00,394 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 00:14:48,814 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 00:15:04,288 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 00:15:29,648 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 00:17:34,640 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 00:22:51,611 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 00:28:55,921 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 00:31:24,474 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 3 ...\n2023-08-09 00:31:24,505 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 00:31:25,651 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 00:31:26,832 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 00:31:28,004 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 00:31:32,073 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 00:31:33,801 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 00:31:37,232 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 00:31:46,521 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 00:33:12,132 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 00:33:19,179 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 00:34:26,927 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 00:37:17,619 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 00:42:23,640 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 00:48:33,828 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 00:51:02,371 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 4 ...\n2023-08-09 00:51:02,394 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 00:51:03,577 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 00:51:04,724 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 00:51:06,453 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 00:51:07,619 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 00:51:09,221 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 00:51:10,957 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 00:51:20,711 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 00:52:36,563 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 00:52:50,815 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 00:54:04,960 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 00:56:22,779 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 01:01:47,072 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 01:08:15,284 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 01:10:46,516 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 5 ...\n2023-08-09 01:10:46,547 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 01:10:47,748 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 01:10:48,947 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 01:10:50,135 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 01:10:51,338 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 01:10:53,128 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 01:10:56,105 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 01:11:09,642 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 01:12:05,863 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 01:12:17,409 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 01:12:58,131 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 01:15:24,342 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 01:21:14,873 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 01:27:29,020 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n\n\nWe do a 5-fold cross-validation. We randomly mask a part of the data and apply our method to recover the masked data. The test error is the RMSE between the receovered data and input masked data. In Figure 4, we plot the RMSE of the test data for each CV fold.\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(5):\n    ax1.plot(np.log10(list(nnmcv.test_error.keys())), [x[k] for x in nnmcv.test_error.values()], 'o-')\nax1.set_xlabel(\"Rank\")\nax1.set_ylabel(\"RMSE on test data\")\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Error on held out test data for each of the 5-fold cross-validation sets to find the best rank constraint for NNM\n\n\n\n\n\n\n\nCode\nnp.linalg.norm(X_cent, 'nuc')\n\n\n7274.4182279699835\n\n\n\n\nCode\nnnm = FrankWolfe(model = 'nnm', svd_max_iter = 50, show_progress = True, debug = True)\nnnm.fit(X_cent, 1024.0)\n\n\n2023-09-25 14:38:29,678 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 0.459. Duality Gap 481580\n2023-09-25 14:39:02,342 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.011. Duality Gap 8251.95\n2023-09-25 14:39:35,146 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.006. Duality Gap 4536.94\n2023-09-25 14:40:07,834 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.005. Duality Gap 3466.49"
  },
  {
    "objectID": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#nnmsparse---fw",
    "href": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#nnmsparse---fw",
    "title": "Application of denoising methods on GWAS phenotypes",
    "section": "NNMSparse - FW",
    "text": "NNMSparse - FW\n\n\nCode\nnnm_sparse = FrankWolfe(model = 'nnm-sparse', max_iter = 1000, svd_max_iter = 50, \n                        tol = 1e-3, step_tol = 1e-5, simplex_method = 'sort',\n                        show_progress = True, debug = True, print_skip = 100)\nnnm_sparse.fit(X_cent, (1024.0, 0.5))\n\n\n2023-09-25 14:42:38,163 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 1.13739e+07\n2023-09-25 14:43:53,562 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.006. Duality Gap 4150.16\n2023-09-25 14:45:09,194 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.003. Duality Gap 2535.3\n2023-09-25 14:46:24,688 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.002. Duality Gap 1744.43\n2023-09-25 14:47:40,052 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 400. Step size 0.001. Duality Gap 1384.91\n2023-09-25 14:48:55,430 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 500. Step size 0.001. Duality Gap 1103.54\n2023-09-25 14:50:10,812 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 600. Step size 0.001. Duality Gap 950.145\n2023-09-25 14:51:26,043 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 700. Step size 0.001. Duality Gap 972.654\n2023-09-25 14:52:41,695 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 800. Step size 0.001. Duality Gap 814.188\n2023-09-25 14:53:57,015 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 900. Step size 0.001. Duality Gap 696.726\n\n\n\n\nCode\nloadings = Vt.T @ np.diag(S)\nloadings[:, 0].shape\n\n\n(10068,)"
  },
  {
    "objectID": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#principal-components-biplots",
    "href": "notebooks/explore/2023-09-25-npd-application-check-Copy1.html#principal-components-biplots",
    "title": "Application of denoising methods on GWAS phenotypes",
    "section": "Principal Components Biplots",
    "text": "Principal Components Biplots\nSuppose, we decompose \\mathbf{X} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\intercal}. Columns of \\mathbf{V} are the principal axes (aka principal directions, aka eigenvectors). The principal components are the columns of \\mathbf{U}\\mathbf{S} – the projections of the data on the the principal axes (note \\mathbf{X}\\mathbf{V} = \\mathbf{U}\\mathbf{S}). We plot the principal components as a scatter plot and color each point based on their broad disease category. To show the directions, we plot the loadings, \\mathbf{V}\\mathbf{S} as arrows. That is, the (x, y) coordinates of an i-th arrow endpoint are given by the i-th value in the first and second column of \\mathbf{V}\\mathbf{S}.\nA comprehensive discussion of biplot on Stackoverflow\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps\n\nloadings_rpca,       pcomps_rpca = get_principal_components(rpca.L_)\nloadings_nnm,        pcomps_nnm = get_principal_components(nnm.X)\nloadings_nnm_sparse, pcomps_nnm_sparse = get_principal_components(nnm_sparse.X)\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm, labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm_sparse, labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nquick_plot_trait_pc_scores(nnm_sparse.X, trait_indices, unique_labels, trait_colors)\n\n\n\n\n\n\n\n\n\n\n\nCode\nquick_plot_trait_pc_scores(nnm.X, trait_indices, unique_labels, trait_colors)\n\n\n\n\n\n\n\n\n\n\n\nCode\nquick_plot_trait_pc_scores(rpca.L_, trait_indices, unique_labels, trait_colors)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef quick_scale_plot_trait_pc_scores(ax, X, tindices, ulabels, tcolors, min_idx = 0, max_idx = 20):\n    '''\n    Quick helper function to plot the same thing many times\n    '''\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    \n    #data = get_trait_pc_scores(U, S, tindices, ulabels, min_idx = min_idx, max_idx = max_idx, use_proportion = False)\n    data_scaled = get_trait_pc_scores(U, S, tindices, ulabels, min_idx = min_idx, max_idx = max_idx, use_proportion = True)\n    xlabels = [f\"{i + 1}\" for i in np.arange(min_idx, max_idx)]\n    plot_stacked_bars(ax, data_scaled, xlabels, tcolors, alpha = 0.8)\n\n    ax.set_xlabel(\"Principal Components\")\n    ax.set_ylabel(\"Trait-wise scores for each PC (scaled)\")\n    return\n\nfig = plt.figure(figsize = (12, 14))\nax1 = fig.add_subplot(311)\nax2 = fig.add_subplot(312)\nax3 = fig.add_subplot(313)\n\nquick_scale_plot_trait_pc_scores(ax1, rpca.L_, trait_indices, unique_labels, trait_colors)\nquick_scale_plot_trait_pc_scores(ax2, nnm.X, trait_indices, unique_labels, trait_colors)\nquick_scale_plot_trait_pc_scores(ax3, nnm_sparse.X, trait_indices, unique_labels, trait_colors)\nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_title(\"RPCA - IALM\")\n\nax2.set_title(\"NNM - FW\")\nax3.set_title(\"NNM Sparse - FW\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/robust_pca_cross_validation.html",
    "href": "notebooks/explore/robust_pca_cross_validation.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.utils import model_errors as merr\n\n\n\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 1.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\n\nunique_labels  = list(range(len(sample_indices)))\nclass_labels = [None for x in range(ngwas)]\nfor k, idxs in enumerate(sample_indices):\n    for i in idxs:\n        class_labels[i] = k\n\n\n\nCode\ndef _generate_fold_labels(Y, kfolds = 5, shuffle = True, test_size = None):\n    n, p = Y.shape\n    fold_labels = np.ones(n * p)\n    if test_size is None:\n        ntest = int ((n * p) / kfolds)\n    else:\n        ntest = int(test_size * n * p)\n    for k in range(1, kfolds):\n        start = k * ntest\n        end = (k + 1) * ntest\n        fold_labels[start: end] = k + 1\n    if shuffle:\n        np.random.shuffle(fold_labels)\n    return fold_labels.reshape(n, p)\n\n\ndef _generate_masked_input(Y, mask):\n    Ymiss_nan = Y.copy()\n    Ymiss_nan[mask] = np.nan\n    Ymiss_nan_cent = Ymiss_nan - np.nanmean(Ymiss_nan, axis = 0, keepdims = True)\n    Ymiss_nan_cent[mask] = 0.0\n    return Ymiss_nan_cent\n\n\n\n\nCode\ndef _generate_rseq(r_min = None, r_max = None, nseq = None):\n    a_min = 0 if r_min is None else int(np.floor(np.log2(r_min)))\n    a_max = max(0, a_min + 1) if r_max is None else int(np.floor(np.log2(r_max)) + 1)\n    if nseq is None:\n        nseq  = a_max - a_min + 1\n    rseq = np.logspace(a_min, a_max, num = nseq, base = 2.0)\n    return rseq\n\n_generate_rseq(0.015, 0.05, nseq = 10)\n\n\narray([0.0078125 , 0.00984313, 0.01240157, 0.015625  , 0.01968627,\n       0.02480314, 0.03125   , 0.03937253, 0.04960628, 0.0625    ])\n\n\n\n\nCode\nlmb_values = _generate_rseq(0.015, 0.05, nseq = 10)\nkfolds = 2\n\ntest_errors =  {r: list() for r in lmb_values}\ntrain_errors = {r: list() for r in lmb_values}\nrpca_models  = {r: list() for r in lmb_values}\n\nfold_labels = _generate_fold_labels(Y_cent, kfolds = kfolds)\nfor k in range(kfolds):\n    print (f\"Fold {k + 1}\")\n    mask = fold_labels == k + 1\n    Ymiss = _generate_masked_input(Y_cent, mask)\n    for lmb in lmb_values:\n        rpca_cv = IALM(mu_update_method='admm', show_progress = False, debug = True)\n        rpca_cv.fit(Ymiss, lmb = lmb, mask = mask)\n        rpca_models[lmb].append(rpca_cv)\n        recovered = mpy_simulate.do_standtardize(rpca_cv.L_, scale = False)\n        train_err_k  = merr.get(Y_cent, recovered, mask, method = 'rmse')\n        test_err_k = merr.get(Y_cent, recovered, ~mask, method = 'rmse')\n        train_errors[lmb].append(train_err_k)\n        test_errors[lmb].append(test_err_k)\n\n\nFold 1\n2023-08-08 14:35:56,970 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0078\n2023-08-08 14:36:14,507 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0098\n2023-08-08 14:36:32,442 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0124\n2023-08-08 14:36:50,250 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0156\n2023-08-08 14:37:08,042 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0197\n2023-08-08 14:37:26,196 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0248\n2023-08-08 14:37:44,624 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0312\n2023-08-08 14:38:03,188 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0394\n2023-08-08 14:38:21,014 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0496\n2023-08-08 14:38:38,694 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0625\nFold 2\n2023-08-08 14:38:56,530 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0078\n2023-08-08 14:39:15,379 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0098\n2023-08-08 14:39:35,125 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0124\n2023-08-08 14:39:52,928 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0156\n2023-08-08 14:40:11,219 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0197\n2023-08-08 14:40:31,581 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0248\n2023-08-08 14:40:49,862 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0312\n2023-08-08 14:41:08,431 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0394\n2023-08-08 14:41:26,439 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0496\n2023-08-08 14:41:44,277 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0625\n\n\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn import metrics as skmetrics\n\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    return pcomps\n\ndef get_adjusted_MI_score(x, class_labels):\n    pcomp = get_principal_components(x)\n    distance_matrix = skmetrics.pairwise.pairwise_distances(pcomp, metric='euclidean')\n    model = AgglomerativeClustering(n_clusters = 4, linkage = 'average', metric = 'precomputed')\n    class_pred = model.fit_predict(distance_matrix)\n    return skmetrics.adjusted_mutual_info_score(class_labels, class_pred)\n\n#adj_mi_scores = {r: list() for r in lmb_values}\n#recovered_matrix_ranks = {r: list() for r in lmb_values}\ntrain_errors_ytrue = {r: list() for r in lmb_values}\nfor k in range(kfolds):\n    for lmb in lmb_values:\n        mask = fold_labels == k + 1\n        # adj_mi_scores[lmb].append(get_adjusted_MI_score(rpca_models[lmb][k].L_, class_labels))\n        # recovered_matrix_ranks[lmb].append(np.linalg.matrix_rank(rpca_models[lmb][k].L_))\n        # recovered = mpy_simulate.do_standardize(rpca_models[lmb][k].L_, scale = False)\n        recovered = mpy_simulate.do_standardize(rpca_models[lmb][k].L_ + rpca_models[lmb][k].E_, scale = False)\n        train_errors_ytrue[lmb].append(merr.get(Y_cent, recovered, mask = ~mask, method = 'rmse'))\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax1 = fig.add_subplot(131)\nfor k in range(kfolds):\n    ax1.plot(np.log2(lmb_values), [x[k] for x in train_errors_ytrue.values()], 'o-', label = k)\nax1.legend()\nax1.set_ylabel(\"Test error\")\nax1.set_xlabel(\"Lambda\")\n\nax2 = fig.add_subplot(132)\nfor k in range(kfolds):\n    ax2.plot(np.log2(lmb_values), [x[k] for x in recovered_matrix_ranks.values()], 'o-')\nax2.set_ylabel(\"Rank of the recovered matrix\")\nax2.set_xlabel(\"Lambda\")\n\nax3 = fig.add_subplot(133)\nfor k in range(kfolds):\n    ax3.plot(np.log2(lmb_values), [x[k] for x in adj_mi_scores.values()], 'o-')\nax3.set_ylabel(\"Adjusted MI scores\")\nax3.set_xlabel(\"Lambda\")\n\nfor ax in [ax1, ax2, ax3]:\n    mpl_utils.set_xticks(ax, scale = 'log2', spacing = 'log2')\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.std(Y_cent, axis = 0)\n\n\narray([0.79278496, 0.23689886, 1.42405339, 0.81935309, 0.23729693,\n       0.09144476, 0.42372639, 2.6104184 , 0.74470766, 0.26436492,\n       0.34641285, 0.51679419, 1.00400527, 0.23558578, 0.22506705,\n       0.8008573 , 2.56458015, 1.13226585, 0.53951641, 2.29523606,\n       1.20775272, 2.06884158, 0.23195788, 1.82242354, 2.61664765,\n       0.11415811, 1.82049267, 1.27572457, 0.59081701, 0.2556268 ,\n       0.59683477, 0.15716198, 1.90974463, 1.39929027, 0.21565277,\n       1.56208744, 2.3567678 , 2.38656392, 1.29505895, 0.88790988,\n       2.39916502, 1.29966879, 2.02256183, 1.08538357, 1.51615966,\n       1.15170198, 0.91148067, 2.12905593, 0.11275457, 0.28150256,\n       4.06321808, 0.21707237, 3.37273166, 1.43174134, 1.5379488 ,\n       0.80468443, 0.69749435, 0.59450189, 0.14475763, 0.37834804,\n       0.9781591 , 0.24520509, 1.70803553, 1.99788534, 1.26419147,\n       0.79987723, 1.1692679 , 2.24607765, 0.8052394 , 2.13117163,\n       2.29366181, 3.21508755, 1.51214671, 0.72536485, 2.75564029,\n       0.40444014, 1.22903899, 1.17563331, 0.30289377, 0.25948922,\n       1.39571923, 2.33608071, 0.67391737, 0.30691724, 1.58124487,\n       0.24247159, 1.26303531, 0.78134375, 0.34909298, 1.49723777,\n       2.5633782 , 0.63248332, 2.06315958, 2.73655591, 0.63746812,\n       0.37483633, 1.05799185, 2.16625899, 2.10228697, 2.95139672,\n       1.49851798, 0.50163253, 0.12364699, 1.94635225, 2.41543301,\n       0.87045793, 0.4338245 , 1.73066518, 0.86636104, 0.77612454,\n       0.39393338, 1.50999924, 1.34199064, 0.87694626, 0.97384605,\n       1.57078914, 3.42632271, 0.57961345, 0.198969  , 2.91650149,\n       0.16863642, 0.27450925, 0.88192257, 0.87241327, 0.61367052,\n       0.92294337, 0.18900335, 1.54389002, 1.16358431, 0.9995563 ,\n       2.59116694, 1.87114326, 0.95387216, 3.1368942 , 2.85617531,\n       0.18298352, 0.320864  , 0.95540369, 1.00825274, 0.37934842,\n       1.32758014, 0.57435658, 1.92188203, 0.10220346, 0.60202924,\n       1.57842301, 0.91552888, 0.43951333, 0.65186528, 1.96801502,\n       2.72180531, 1.10932928, 0.93263535, 1.28405924, 2.1243162 ,\n       1.39385595, 2.65452983, 0.92137201, 0.13512012, 0.22107176,\n       0.60792139, 0.43786306, 2.73821955, 1.19433802, 0.89981103,\n       2.30429746, 0.76762917, 0.19139105, 2.18354359, 0.17295042,\n       2.85694905, 0.47347341, 1.07653131, 0.64606758, 0.34438932,\n       0.18799157, 2.56747325, 1.14556747, 1.30213267, 0.14430694,\n       2.30301441, 0.15554613, 1.94908114, 1.81960156, 1.21584217,\n       1.96378325, 0.36024668, 0.69206144, 0.20750373, 0.22330197,\n       0.98992743, 0.79531352, 3.5160207 , 3.32408446, 0.25726628,\n       1.00454216, 2.00544454, 0.15957821, 3.05604908, 1.49415551,\n       0.34066636, 0.74549553, 1.50204117, 0.65602372, 2.11447607,\n       2.21963321, 0.71562671, 2.49790207, 1.91196613, 0.18339314,\n       1.3338508 , 3.0738865 , 1.23000143, 0.37389596, 0.38242603,\n       0.47814541, 0.46815268, 1.81005465, 1.7759139 , 0.19490319,\n       2.62170889, 0.7274351 , 1.43057142, 0.35252002, 1.60559107,\n       0.30478939, 0.50812421, 1.84793352, 0.87486808, 0.58737922,\n       0.23325643, 1.46350661, 1.76672398, 1.07101475, 0.2510534 ,\n       1.6674357 , 1.73329013, 1.30080634, 0.89958686, 0.46853519,\n       1.34787325, 0.75419132, 1.06126043, 0.41433223, 1.3074349 ,\n       2.69527457, 2.16130515, 1.77974254, 1.36476683, 1.01961488,\n       0.61973534, 0.45699834, 0.70896517, 2.24514123, 1.66315655,\n       1.58113577, 0.66519896, 1.342617  , 0.88202759, 3.96755334,\n       0.47506965, 3.11697749, 0.55045717, 1.60908617, 1.34285765,\n       1.4361946 , 0.52270145, 1.93964405, 1.67576408, 0.32823198,\n       0.36293814, 1.97719954, 0.69592039, 1.20323708, 1.58722814,\n       1.22956864, 2.01519003, 0.75764934, 0.31073221, 1.23778734,\n       0.45615268, 0.52327669, 0.58285618, 1.04038017, 0.13423341,\n       0.30840806, 1.06738081, 0.20863863, 1.02953719, 0.29546987,\n       3.78599415, 0.69463861, 1.16269867, 0.2509111 , 1.38911577,\n       0.81375559, 0.27911542, 1.77120078, 0.70094215, 0.93884575,\n       1.23331716, 1.81981076, 2.52595536, 0.61043517, 0.14712453,\n       0.17754012, 0.98472343, 2.05846025, 0.72956734, 1.84482945,\n       2.33328057, 0.78819742, 1.29483993, 1.3578304 , 0.17121228,\n       2.01416966, 1.33231416, 2.33931023, 2.48272378, 0.59198319,\n       1.52312738, 2.03677688, 0.26975459, 0.69725068, 1.19001804,\n       0.37338966, 0.80537804, 0.62435045, 2.10219728, 0.24484637,\n       1.74065903, 0.17665709, 0.33063724, 1.19211017, 1.80392461,\n       1.14773834, 0.375949  , 0.28962153, 1.27052977, 0.64216151,\n       0.53188272, 0.63922106, 3.71904879, 0.63369339, 1.28836969,\n       2.94780032, 0.19774351, 3.00131009, 1.63003278, 1.0013431 ,\n       2.16030959, 1.91734935, 1.15640019, 2.56934233, 1.27370418,\n       2.22777096, 0.13877242, 1.54311099, 4.02585204, 2.25727882,\n       2.0111711 , 0.22045637, 3.08292233, 2.35502129, 2.8550196 ,\n       0.58654744, 1.26195011, 1.26941769, 0.20908145, 1.57546688,\n       2.30826731, 1.81823604, 0.28238357, 2.13765788, 0.27002061,\n       0.62421141, 0.47793152, 5.13279049, 0.7522935 , 0.76690898,\n       1.6820422 , 0.85107274, 0.82242058, 0.38328327, 1.3211005 ,\n       1.21201157, 1.341193  , 0.51910486, 2.11055676, 0.88397171,\n       0.31657948, 0.690313  , 2.45996327, 1.78113257, 0.50052269,\n       0.3416163 , 0.334879  , 1.21792653, 0.55314141, 3.56387844,\n       0.13743515, 0.52163463, 0.11428136, 0.96441506, 0.30387341,\n       0.99152258, 3.22069135, 0.87947481, 1.66999876, 0.57280872,\n       1.32688258, 0.32485651, 0.21373014, 0.28717232, 0.34311837,\n       1.64035057, 1.74529168, 0.55699193, 1.50455621, 2.47843895,\n       0.16610272, 2.66564667, 0.22381172, 2.48857911, 0.99716851,\n       1.52791597, 0.30644231, 1.48007491, 0.19483383, 0.43170631,\n       0.15052792, 1.01298304, 0.36532876, 1.46959575, 1.63900285,\n       1.09860951, 0.47757527, 3.53246883, 1.24025589, 2.08363204,\n       0.26935567, 2.93724387, 0.97212839, 0.96208566, 0.95096473,\n       3.31249932, 1.39235207, 0.12257131, 0.54617048, 0.46469654,\n       2.63203771, 1.43064653, 0.85519918, 1.14027615, 0.82470954,\n       0.49415366, 1.30415123, 0.49471268, 0.99082814, 2.39391629,\n       1.8864201 , 0.55410211, 1.18498071, 1.23046494, 1.56704821,\n       2.36489156, 0.56427642, 0.94642179, 1.25894415, 0.45456272,\n       0.73652262, 0.45056825, 1.40866897, 0.36740644, 1.49515378,\n       1.54350404, 0.88609604, 1.79165954, 0.99215686, 1.9192165 ,\n       0.970096  , 0.47629613, 1.75256122, 1.73048404, 0.83116407,\n       0.42478391, 0.51527305, 0.97201362, 0.39376403, 1.05986961,\n       0.8530535 , 0.37021188, 0.85141406, 0.16403011, 2.03345378,\n       1.1412117 , 1.87841139, 1.33687923, 0.91560346, 0.84860093,\n       0.27616176, 0.52134769, 0.50519048, 1.23334629, 1.03333948,\n       1.64510506, 1.5265191 , 1.10425986, 0.54187902, 3.01186283,\n       1.96338681, 1.91508011, 0.38627011, 1.65954357, 0.30282136,\n       1.17772117, 2.36033373, 0.73857982, 0.09838789, 1.75908448,\n       0.73759057, 0.46061778, 0.15564214, 0.27132464, 0.92783315,\n       0.9754013 , 1.34653561, 2.53410515, 2.34807657, 0.59095344,\n       2.24467319, 1.47170716, 0.61408383, 1.28511975, 1.69966247,\n       3.8968736 , 0.1941589 , 0.66137797, 1.08255599, 1.24118775,\n       1.75125904, 0.50159618, 2.82902232, 0.57297841, 1.00974782,\n       2.6581435 , 2.36289017, 2.05329551, 0.99266975, 2.17365432,\n       0.30264393, 0.66762845, 0.48931042, 0.7397716 , 1.35469608,\n       0.63556035, 1.47088546, 2.25980492, 0.28875814, 1.93443479,\n       0.55532329, 0.60159529, 2.05031281, 1.1221856 , 0.5874901 ,\n       1.21805953, 1.30807777, 1.90244637, 1.11788232, 1.53552753,\n       1.60374274, 0.57729306, 0.44705124, 0.91444064, 0.30094749,\n       0.51363002, 0.95234177, 1.39269306, 0.97294212, 2.53218411,\n       0.76367962, 2.18175816, 0.93461987, 1.77158593, 0.11484618,\n       2.6840181 , 1.87061143, 0.53483875, 1.18913621, 1.43399407,\n       3.96054365, 2.90672692, 2.05640045, 1.36038204, 0.76318198,\n       0.86007647, 1.40609103, 0.98462128, 2.09217876, 0.79182675,\n       0.18816787, 0.84874607, 2.85270697, 0.29139034, 0.60282988,\n       2.72697035, 0.70779196, 2.16161802, 0.47071293, 0.78326494,\n       0.88818964, 1.49782466, 0.80222581, 0.95042402, 0.220556  ,\n       0.88433769, 1.27802745, 0.85383781, 2.2503185 , 1.37066662,\n       1.41880294, 1.73586313, 0.90452059, 1.8504055 , 1.33961186,\n       2.5143152 , 1.36912382, 0.35625077, 1.42963021, 1.84989564,\n       1.56314939, 2.04539618, 1.83888209, 0.60321233, 1.44395108,\n       2.56700242, 0.40362929, 1.16050234, 1.7624774 , 0.55107086,\n       0.6713127 , 2.09692261, 0.65667942, 0.92220126, 0.98745218,\n       2.22588951, 1.23687354, 0.66361231, 0.30700346, 2.07422187,\n       0.69963889, 0.12404808, 1.04997164, 1.29547578, 0.25995134,\n       2.03581852, 0.60148454, 3.21703577, 2.0575175 , 0.56229172,\n       0.88749702, 2.24323814, 0.18162149, 0.50244874, 4.22700434,\n       1.24538419, 0.93580684, 2.23558378, 1.2624758 , 1.55834157,\n       0.82591477, 0.71940543, 0.50181052, 0.46741884, 0.51055402,\n       1.91517703, 2.15899402, 0.96937215, 0.16656169, 1.94739179,\n       0.20323291, 0.55327333, 1.51362812, 0.08452712, 1.38554369,\n       0.96735875, 0.3030606 , 1.37859057, 0.35333226, 2.2913031 ,\n       1.35018042, 0.21654567, 1.38750491, 2.93460761, 0.53328737,\n       0.8895235 , 0.20013323, 1.99748326, 1.85093448, 1.02320778,\n       2.91659539, 3.34950802, 1.10879293, 0.4848102 , 0.20967442,\n       2.0405376 , 0.81025612, 0.51233735, 0.14450453, 1.30489221,\n       1.72228416, 3.55808337, 0.66767358, 1.58424947, 0.80846524,\n       0.70856206, 0.16471875, 1.93159029, 0.65659395, 0.42898942,\n       3.86321381, 3.02477935, 1.14210476, 1.7164433 , 0.86002427,\n       2.09304719, 2.73682184, 1.06409503, 0.94560987, 2.73216215,\n       0.15721892, 0.36849794, 1.6363238 , 1.61408563, 0.80201301,\n       0.4171714 , 3.84441148, 0.57577929, 0.27458624, 1.05051116,\n       0.23810414, 1.47833521, 2.71074674, 1.30061039, 1.94110414,\n       2.80898617, 1.55436831, 1.67278595, 1.77480395, 2.94881363,\n       0.28298887, 0.22788126, 0.99391991, 1.3819918 , 1.08301827,\n       0.79298418, 0.16522167, 0.96182566, 0.25904642, 0.54994867,\n       2.22586415, 0.58292509, 1.059376  , 0.65580229, 3.56659797,\n       2.61962999, 0.46956143, 0.82496726, 1.19859169, 0.64947331,\n       1.51913366, 0.9580465 , 0.94972555, 0.67914591, 0.22670566,\n       0.10449637, 1.92021221, 0.17991699, 0.90862403, 2.38873709,\n       0.8889717 , 1.28478495, 1.34562709, 0.99611017, 2.52170463,\n       3.77605832, 1.37108848, 0.63592639, 0.34050208, 2.88714295,\n       1.24872633, 0.45497452, 0.39725475, 0.22675761, 0.59751686,\n       0.58930819, 0.13223208, 0.36670276, 0.29999077, 1.38022358,\n       0.2800674 , 1.73034838, 1.37877107, 0.3435115 , 1.83694898,\n       0.14600441, 2.86057276, 1.73015744, 1.94226432, 1.03139453,\n       0.26225897, 2.6902902 , 1.39877413, 2.63595468, 4.06663285,\n       1.58773822, 0.54954373, 0.8775699 , 1.89166681, 1.63555554,\n       1.11700462, 1.94555334, 1.17791581, 1.06207843, 0.72045537,\n       1.64080997, 2.26824854, 4.39801831, 0.71599736, 1.44617145,\n       0.47748615, 1.5304846 , 1.34878061, 1.03910964, 1.46050563,\n       1.569277  , 2.81147938, 0.42738148, 1.40120425, 0.09830139,\n       1.7920018 , 0.78910924, 1.31580126, 2.95456639, 2.6696023 ,\n       0.95547921, 0.63469506, 0.97278447, 1.54383052, 0.38435556,\n       1.34399436, 0.32009965, 1.74143525, 0.95051389, 1.15274901,\n       0.46300411, 0.4522756 , 2.55144559, 3.14615297, 0.4613339 ,\n       0.22985098, 1.08131081, 1.24903498, 0.7487147 , 1.5210816 ,\n       1.51701012, 0.33128085, 2.68182669, 1.13275403, 0.19470342,\n       2.37091474, 0.8478102 , 0.77997619, 2.53977101, 0.31552768,\n       0.47765196, 0.32939736, 1.42401604, 1.44399284, 0.33115053,\n       1.19470768, 1.85113179, 0.92587957, 0.31304147, 0.90478566,\n       0.63151125, 0.2773992 , 0.4357118 , 0.51402666, 1.0211793 ,\n       1.59686646, 0.69968032, 1.01753481, 3.10449478, 1.05437909,\n       0.44209465, 0.85138771, 1.30981878, 2.32220776, 1.65500241,\n       0.55646966, 0.6224708 , 0.62216843, 0.64881441, 0.26243256,\n       0.1561138 , 0.34752168, 2.15337271, 0.17821086, 1.10649961,\n       0.17649146, 0.17086709, 1.23639751, 0.77405116, 1.95953531,\n       1.11339181, 0.5485678 , 0.52849984, 0.63413281, 1.3932477 ,\n       0.59768752, 0.89883079, 0.3162256 , 1.63327663, 0.95152221,\n       0.80046264, 0.72831066, 0.38990687, 0.26469529, 0.40745626,\n       2.06309561, 0.84644444, 0.81099086, 1.75856023, 1.11186497,\n       0.94821554, 2.1007442 , 0.58284306, 1.77861865, 1.92010375,\n       0.95817322, 1.36499275, 2.29956356, 2.84174382, 1.14704701,\n       0.19255283, 0.21603661, 0.97893761, 0.98645233, 0.76539722,\n       0.62755831, 0.38193078, 0.34149537, 0.78518147, 1.19147668,\n       0.71765921, 0.66853278, 1.21599077, 5.41963092, 2.30308696,\n       1.02085125, 0.14165504, 0.22577172, 2.13276269, 2.50771155,\n       0.33833828, 0.8010081 , 1.25851859, 2.25227358, 0.26221439,\n       2.56110094, 0.33677557, 4.22305806, 0.68351125, 0.31348117,\n       4.76398731, 0.96385165, 0.65151369, 0.15280628, 2.41700693,\n       2.34899913, 0.67248655, 0.42596669, 1.21937724, 0.90985042,\n       1.53875013, 1.17953058, 0.13289024, 1.04235682, 1.10214984,\n       0.47142508, 1.06494732, 1.5333827 , 1.20012696, 0.48030268,\n       0.15021558, 2.20214691, 4.83431444, 0.9040633 , 3.43006545])\n\n\n\n\nCode\nnp.array(~(fold_labels == 1)).astype(int)\n\n\narray([[0, 0, 1, ..., 0, 0, 1],\n       [1, 1, 0, ..., 0, 0, 1],\n       [1, 1, 1, ..., 0, 1, 0],\n       ...,\n       [0, 0, 0, ..., 1, 0, 1],\n       [1, 0, 0, ..., 1, 1, 1],\n       [1, 0, 1, ..., 1, 0, 0]])\n\n\n\n\nCode\nnsnp * ngwas\n\n\n500000\n\n\n\n\nCode\nrecovered_matrix_ranks\n\n\n{0.0078125: [0, 0],\n 0.009843133202303695: [0, 0],\n 0.012401570718501561: [0, 0],\n 0.015625: [0, 1],\n 0.01968626640460739: [0, 1],\n 0.024803141437003108: [2, 3],\n 0.03125: [31, 31],\n 0.03937253280921478: [425, 425],\n 0.049606282874006216: [500, 500],\n 0.0625: [500, 500]}\n\n\n\n\nCode\n1. / np.sqrt(np.max(Y_cent.shape))\n\n\n0.03162277660168379"
  },
  {
    "objectID": "notebooks/explore/2023-06-23-external-validation-metrics.html",
    "href": "notebooks/explore/2023-06-23-external-validation-metrics.html",
    "title": "Metrices for evaluating clusters given true labels",
    "section": "",
    "text": "About\nThere are many aspects of “rightness” for clustering. Broadly, there are two kinds of validity indices to measure the quality of clustering results: external indices and internal indices. An external index is a measure of agreement between two partitions where the first partition is the a priori known clustering structure, and the second results from the clustering procedure. Internal indices are used to measure the goodness of a clustering structure without external information. For external indices, we evaluate the results of a clustering algorithm based on a known cluster structure of a data set (or cluster labels). Here, we look at several possible external validation metrics.\n\n\nGetting set up\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import model_selection as skmodel\nfrom sklearn import metrics as skmetrics\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n'''\nX matrix (n_samples x n_features) -- obtain from Z-scores\n'''\nselect_ids = beta_df.columns\nX = np.array(zscore_df[select_ids]).T # contain NaN values\ncolmeans = np.nanmean(X, axis = 0, keepdims = True)\nXcent = X - colmeans # contain NaN values\nXcent = np.nan_to_num(X, nan=0) # remove NaN values\n\n'''\nY vector (n_samples) -- contain class labels\n'''\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\nencoding = {x:i for i, x in enumerate(unique_labels)}\nYlabels = np.array([encoding[x] for x in labels])\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\n\n\nSample counts of input data\n\n\nCode\nsample_counts = {label : (Ylabels == idx).sum() for label, idx in encoding.items()}\nprint (f\"Count   Phenotype\")\nprint (f\"-----   ---------\")\nfor phenotype, count in sample_counts.items():\n    print (f\"{count}\\t{phenotype}\")\n\n\nCount   Phenotype\n-----   ---------\n7   Sleep\n3   SZ\n2   ASD\n2   Migraine\n2   ADHD\n1   OCD\n8   Depression\n2   Intel/education\n11  Epilepsy\n10  Other psych\n7   Cognition\n6   BD\n8   Neurodegenerative\n\n\n\n\nSplit into training and test data\n\n\nCode\nfrom sklearn import model_selection as skmodel\n\n'''\nOne-liner to split:\n# X_train, X_test, y_train, y_test = skmodel.train_test_split(X, Ylabels, test_size = 0.33)\nbut it does not return the index for the training and test data,\nso I use a little more verbose solution\n'''\nitrain, itest = skmodel.train_test_split(np.arange(Ylabels.shape[0]), test_size = 0.33)\nX_train = X[itrain, :]\nX_test  = X[itest, :]\ny_train = Ylabels[itrain]\ny_test  = Ylabels[itest]\n\nprint (f\"Train   Test    Phenotype\")\nprint (f\"-----   ----    ---------\")\nfor phenotype, idx in encoding.items():\n    train_count = np.sum(y_train == idx)\n    test_count  = np.sum(y_test  == idx)\n    print (f\"{train_count}\\t{test_count}\\t{phenotype}\")\n\n\nTrain   Test    Phenotype\n-----   ----    ---------\n7   0   Sleep\n2   1   SZ\n0   2   ASD\n1   1   Migraine\n1   1   ADHD\n0   1   OCD\n5   3   Depression\n1   1   Intel/education\n9   2   Epilepsy\n5   5   Other psych\n4   3   Cognition\n6   0   BD\n5   3   Neurodegenerative\n\n\n\n\nClustering from distance matrix\nWe want to cluster the samples based on the Euclidean distance between them, obtained from the feature matrix. There are hundreds of algorithms to choose from, for example: - Hierarchical clustering in it’s myriad of variants. Cut the dendrogram as desired, e.g., to get k clusters - PAM, the closest match to k-means on a distance matrix (minimizes the average distance from the cluster center) - Spectral clustering - DBSCAN - OPTICS - HDBSCAN* - Affinity Propagation\nAvailable Software in Python: - pyclustering for fast Python implementation of different algorithms. They have nice documentation and examples. - sklearn.cluster - HDBSCAN* provides a very nice documentation for comparing different algorithms (albeit a bit biased, highlighting their own strength). - scipy.cluster provides the hierarchy module which has functions for hierarchical and agglomerative clustering.\n\n\nCode\ndistance_matrix = skmetrics.pairwise.pairwise_distances(Xcent, metric='euclidean')\n\n\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\n\nmodel = AgglomerativeClustering(n_clusters = len(unique_labels), linkage = 'average', metric = 'precomputed')\nY_pred = model.fit_predict(distance_matrix)\n#km = KMeans(n_clusters = len(unique_labels), random_state = 0, n_init=\"auto\")\n#km.fit(Xcent)\n#Y_pred = km.labels_\n\n\n\n\nCode\nY_random = np.random.choice(len(unique_labels), size=Ylabels.shape[0], replace=True)\n\n\n\n\nComparison Metrics\nWe can use several external validation techniques to assess the quality or “correctness” of the clusters since we have manually assigned the cluster labels. For example, we can use adjusted rand index, adjusted mutual information, homogeneity/completeness/v-measure, Fowlkes-Mallows score.\n\nAdjusted Rand Index\n\n\nCode\nprint (f\"Random: {skmetrics.adjusted_rand_score(Ylabels, Y_random):.5f}\")\nprint (f\"Predicted: {skmetrics.adjusted_rand_score(Ylabels, Y_pred):.5f}\")\n\n\nRandom: -0.01573\nPredicted: 0.15229\n\n\n\n\nAdjusted Mutual Information\n\n\nCode\nprint (f\"Random: {skmetrics.adjusted_mutual_info_score(Ylabels, Y_random):.5f}\")\nprint (f\"Predicted: {skmetrics.adjusted_mutual_info_score(Ylabels, Y_pred):.5f}\")\n\n\nRandom: -0.02511\nPredicted: 0.36902\n\n\n\n\nHomogeneity and V-measure\nRosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment: - homogeneity: each cluster contains only members of a single class. - completeness: all members of a given class are assigned to the same cluster.\nWe turn those concept as scores homogeneity_score and completeness_score. Both are bounded below by 0.0 and above by 1.0 (higher is better). Their harmonic mean called V-measure is computed by v_measure_score.\nNote. v_measure_score is symmetric: it can be used to evaluate the agreement of two independent assignments on the same dataset. This is not the case for completeness_score and homogeneity_score: both are bound by the relationship:\nhomogeneity_score(a, b) == completeness_score(b, a)\n\n\nCode\nprint (f\"        Homogeneity \\tCompleteness \\tV-Measure\")\n\nhcv_random = skmetrics.homogeneity_completeness_v_measure(Ylabels, Y_random)\nhcv_pred   = skmetrics.homogeneity_completeness_v_measure(Ylabels, Y_pred)\n\nprint (\"Random:    \" + ' \\t'.join([f\"{x:.5f}\" for x in hcv_random]))\nprint (\"Predicted: \" + ' \\t'.join([f\"{x:.5f}\" for x in hcv_pred]))\n\n\n        Homogeneity     Completeness    V-Measure\nRandom:    0.36793  0.35036     0.35893\nPredicted: 0.47942  0.70042     0.56922\n\n\n\n\nFowlkes-Mallows scores\nFMI is defined as the geometric mean of the pairwise precision and recall.\n\n\nCode\nprint (f\"Random: {skmetrics.fowlkes_mallows_score(Ylabels, Y_random):.5f}\")\nprint (f\"Random: {skmetrics.fowlkes_mallows_score(Ylabels, Y_pred):.5f}\")\n\n\nRandom: 0.07035\nRandom: 0.34056\n\n\n\n\n\nDoes distance matrix from truncated SVD improve score?\n\n\nCode\nK = 20\n\nU, S, Vt = np.linalg.svd(Xcent, full_matrices=False)\npcomp_tsvd = U[:, :K] @ np.diag(S[:K])\n\ndistance_matrix_tsvd = skmetrics.pairwise.pairwise_distances(pcomp_tsvd, metric='euclidean')\n\nmodel = AgglomerativeClustering(n_clusters = len(unique_labels), linkage = 'average', metric = 'precomputed')\nY_pred_tsvd = model.fit_predict(distance_matrix_tsvd)\n\nskmetrics.adjusted_mutual_info_score(Ylabels, Y_pred_tsvd)\n\n\n0.3878603292380521\n\n\n\n\nFurther Reading\n\nScikit: Clustering Performance Evaluation\nHow to compare a clustering algorithm partition to a “ground truth”?"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html",
    "title": "Comparison of different noise models",
    "section": "",
    "text": "Our goal is to obtain a denoised, low-rank approximation of the input matrix. Such low-rank matrix can be obtained using different noise models:\n\nGaussian noise \n\\min_{\\mathbf{X}} \\frac{1}{2}\\left\\lVert \\mathbf{Y} - \\mathbf{X} \\right\\rVert_{F}^2 \\quad \\textrm{s.t.} \\quad \\left\\lVert \\mathbf{X} \\right\\rVert_{*} \\le r\n\nSparse noise \n\\min_{\\mathbf{X}, \\mathbf{M}} \\left\\lVert \\mathbf{X} \\right\\rVert_{*} + \\lambda \\left\\lVert \\mathbf{M} \\right\\rVert_{1} \\quad \\textrm{s.t.} \\quad \\mathbf{L} + \\mathbf{M} = \\mathbf{Y}\n\nGaussian + sparse \n\\min_{\\mathbf{X}} \\frac{1}{2}\\left\\lVert \\mathbf{Y} - \\mathbf{X} - \\mathbf{M} \\right\\rVert_{F}^2 \\quad \\textrm{s.t.} \\quad \\left\\lVert \\mathbf{X} \\right\\rVert_{*} \\le r_L\\,, \\left\\lVert \\mathbf{M} \\right\\rVert_{1} \\le r_M\n\n\nFor models (1) and (3), we use the Frank-Wolfe method, and for model (2), we use the inexact augmented Lagrangian method (IALM) with a slight modification of the step-size update (borrowing the ADMM updates of Boyd et. al.). We use a simple simulation (as described previously) with additive Gaussian noise to realistically mimic the z-scores obtained from multiple GWAS."
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#ialm---rpca",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#ialm---rpca",
    "title": "Comparison of different noise models",
    "section": "IALM - RPCA",
    "text": "IALM - RPCA\n\n\nCode\nrpca = IALM(benchmark = True, max_iter = 1000, mu_update_method='admm', show_progress = True)\nrpca.fit(Y_cent, Xtrue = Y_cent)\n\n\n2023-08-02 14:46:55,016 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 0. Primal residual 0.807755. Dual residual 0.00319521\n2023-08-02 14:47:16,524 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 100. Primal residual 6.9408e-06. Dual residual 1.68576e-06"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnm",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnm",
    "title": "Comparison of different noise models",
    "section": "FW - NNM",
    "text": "FW - NNM\nWe do a little cheating here. Instead of cross-validation, we directly use the known rank of the underlying true data. However, based on my previous work, I know that cross-validation does provide the correct rank.\n\n\nCode\nnnm = FrankWolfe(model = 'nnm', svd_max_iter = 50, \n                 show_progress = True, debug = True, benchmark = True)\nnnm.fit(Y_cent, 40.0, Ytrue = Y_cent)\n\n\n2023-08-02 14:47:37,144 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 5383.02"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnml1",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#fw---nnml1",
    "title": "Comparison of different noise models",
    "section": "FW - NNML1",
    "text": "FW - NNML1\nFor the third model, we use the sparse noise threshold from the RPCA result and the rank from the true data. I have not checked cross-validation results for this method yet.\n\nnp.sum(np.abs(rpca.E_)) / (ngwas * nsnp)\n\n0.7012888342395619\n\n\n\n\nCode\nnnm_sparse = FrankWolfe(model = 'nnm-sparse', max_iter = 1000, svd_max_iter = 50, \n                        tol = 1e-3, step_tol = 1e-5, simplex_method = 'sort',\n                        show_progress = True, debug = True, benchmark = True, print_skip = 100)\nnnm_sparse.fit(Y_cent, (40.0, 0.7), Ytrue = Y_cent)\n\n\n2023-08-02 14:47:49,393 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 6.42438e+06\n2023-08-02 14:48:03,988 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.004. Duality Gap 6.28097"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#cpu-time-and-accuracy",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#cpu-time-and-accuracy",
    "title": "Comparison of different noise models",
    "section": "CPU time and accuracy",
    "text": "CPU time and accuracy\nIn Figure 2, we look at the RMSE (w.r.t the observed input data) and CPU time.\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nkp = 0\nax1.plot(np.log10(np.cumsum(rpca.cpu_time_[kp:])), rpca.rmse_[kp:], 'o-', label = 'IALM - RPCA')\nax1.plot(np.log10(np.cumsum(nnm.cpu_time_[kp:])), nnm.rmse_[kp:], 'o-', label = 'FW - NNM')\nax1.plot(np.log10(np.cumsum(nnm_sparse.cpu_time_[kp:])), nnm_sparse.rmse_[kp:], 'o-', label = 'FW - NNML1')\nax1.legend()\n\nax1.set_xlabel(\"CPU Time\")\nax1.set_ylabel(\"RMSE from input matrix\")\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Comparison of time and accuracy of different methods"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#principal-components",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#principal-components",
    "title": "Comparison of different noise models",
    "section": "Principal Components",
    "text": "Principal Components\nCan the principal components of the low rank matrix help in classification?\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    return pcomps\n\npcomps_rpca = get_principal_components(rpca.L_)\npcomps_nnm = get_principal_components(nnm.X)\npcomps_nnm_sparse = get_principal_components(nnm_sparse.X)\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: First 6 principal components compared against each other (FW-NNM).\n\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: First 6 principal components compared against each other (IALM - RPCA).\n\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm_sparse, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: First 6 principal components compared against each other (FW-NNML1)."
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#benchmark-stats",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#benchmark-stats",
    "title": "Comparison of different noise models",
    "section": "Benchmark stats",
    "text": "Benchmark stats\nWe look at different statistics to determine the efficiency of the methods. In particular:\n\nRMSE (root mean square error) with respect to the ground truth\nPSNR (peak signal-to-noise-ratio) with respect to the ground truth (frequently used in image denoising literature)\nAdjusted MI score measures how good the methods are in the classification task.\n\n\n\nCode\nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\nY_rpca_cent = mpy_simulate.do_standardize(rpca.L_, scale = False)\nY_nnm_cent  = mpy_simulate.do_standardize(nnm.X, scale = False)\nY_nnm_sparse_cent = mpy_simulate.do_standardize(nnm_sparse.X, scale = False)"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#rmse",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#rmse",
    "title": "Comparison of different noise models",
    "section": "RMSE",
    "text": "RMSE\n\n\nCode\nrmse_rpca = merr.get(Y_true_cent, Y_rpca_cent, method = 'rmse')\nrmse_nnm = merr.get(Y_true_cent, Y_nnm_cent, method = 'rmse')\nrmse_nnm_sparse = merr.get(Y_true_cent, Y_nnm_sparse_cent, method = 'rmse')\n\nprint (f\"{rmse_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{rmse_nnm_sparse:.4f}\\tSparse Nuclear Norm Minimization\")\nprint (f\"{rmse_rpca:.4f}\\tRobust PCA\")\n\n\n0.1602  Nuclear Norm Minimization\n0.1371  Sparse Nuclear Norm Minimization\n0.3972  Robust PCA"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#psnr",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#psnr",
    "title": "Comparison of different noise models",
    "section": "PSNR",
    "text": "PSNR\n\n\nCode\npsnr_rpca = merr.get(Y_true_cent, Y_rpca_cent, method = 'psnr')\npsnr_nnm = merr.get(Y_true_cent, Y_nnm_cent, method = 'psnr')\npsnr_nnm_sparse = merr.get(Y_true_cent, Y_nnm_sparse_cent, method = 'psnr')\n\nprint (f\"{psnr_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{psnr_nnm_sparse:.4f}\\tSparse Nuclear Norm Minimization\")\nprint (f\"{psnr_rpca:.4f}\\tRobust PCA\")\n\n\n19.6846 Nuclear Norm Minimization\n21.0394 Sparse Nuclear Norm Minimization\n11.7991 Robust PCA"
  },
  {
    "objectID": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#adjusted-mi-score",
    "href": "notebooks/explore/2023-08-02-comparison-of-noise-models.html#adjusted-mi-score",
    "title": "Comparison of different noise models",
    "section": "Adjusted MI Score",
    "text": "Adjusted MI Score\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn import metrics as skmetrics\n\ndef get_adjusted_MI_score(pcomp, class_labels):\n    distance_matrix = skmetrics.pairwise.pairwise_distances(pcomp, metric='euclidean')\n    model = AgglomerativeClustering(n_clusters = 4, linkage = 'average', metric = 'precomputed')\n    class_pred = model.fit_predict(distance_matrix)\n    return skmetrics.adjusted_mutual_info_score(class_labels, class_pred)\n\nadjusted_mi_nnm = get_adjusted_MI_score(pcomps_nnm,   class_labels)\nadjusted_mi_nnm_sparse = get_adjusted_MI_score(pcomps_nnm_sparse, class_labels)\nadjusted_mi_rpca = get_adjusted_MI_score(pcomps_rpca, class_labels)\n\nprint (f\"{adjusted_mi_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{adjusted_mi_nnm_sparse:.4f}\\tSparse Nuclear Norm Minimization\")\nprint (f\"{adjusted_mi_rpca:.4f}\\tRobust PCA\")\n\n\n-0.0011 Nuclear Norm Minimization\n1.0000  Sparse Nuclear Norm Minimization\n0.0099  Robust PCA"
  },
  {
    "objectID": "notebooks/explore/weighted_nuclear_norm_minimization.html",
    "href": "notebooks/explore/weighted_nuclear_norm_minimization.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.extmath import randomized_svd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\nselect_ids = beta_df.columns\n\nX = np.array(zscore_df.replace(np.nan, 0)[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices = False)\nprint (f\"Nuclear Norm of input matrix: {np.sum(S)}\")\n\n\nNuclear Norm of input matrix: 7292.701186600059\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(np.arange(S.shape[0]), S)\nax1.set_xlabel(\"Component\")\nax1.set_ylabel(\"Eigenvalue (S)\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Eigenvalues obtained using SVD of the input matrix\n\n\n\n\n\n\n# weight = np.array(prec_df[select_ids]).T * np.array(mean_se.loc[select_ids]) * np.array(mean_se.loc[select_ids])\n# weight = np.array(prec_df[select_ids]).T\nweight = np.ones(X.shape)\n\n\ndef nuclear_norm(X):\n    '''\n    Nuclear norm of input matrix\n    '''\n    return np.sum(np.linalg.svd(X)[1])\n\ndef f_objective(X, Y, W = None, mask = None):\n    '''\n    Objective function\n    Y is observed, X is estimated\n    W is the weight of each observation.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    # The * operator can be used as a shorthand for np.multiply on ndarrays.\n    if Wmask is None:\n        f_obj = 0.5 * np.linalg.norm(Y - Xmask, 'fro')**2\n    else:\n        f_obj = 0.5 * np.linalg.norm(Wmask * (Y - Xmask), 'fro')**2\n    return f_obj\n\n\ndef f_gradient(X, Y, W = None, mask = None):\n    '''\n    Gradient of the objective function.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    if Wmask is None:\n        f_grad = Xmask - Y\n    else:\n        f_grad = np.square(Wmask) * (Xmask - Y)\n    \n    return f_grad\n\n\ndef linopt_oracle(grad, r = 1.0, max_iter = 10):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a nuclear norm ball for some r\n    '''\n    U1, V1_T = singular_vectors_power_method(grad, max_iter = max_iter)\n    S = - r * U1 @ V1_T\n    return S\n\n\ndef singular_vectors_randomized_method(X, max_iter = 10):\n    u, s, vh = randomized_svd(X, n_components = 1, n_iter = max_iter,\n                              power_iteration_normalizer = 'none',\n                              random_state = 0)\n    return u, vh\n\n\ndef singular_vectors_power_method(X, max_iter = 10):\n    '''\n    Power method.\n        \n        Computes approximate top left and right singular vector.\n        \n    Parameters:\n    -----------\n        X : array {m, n},\n            input matrix\n        max_iter : integer, optional\n            number of steps\n            \n    Returns:\n    --------\n        u, v : (n, 1), (p, 1)\n            two arrays representing approximate top left and right\n            singular vectors.\n    '''\n    n, p = X.shape\n    u = np.random.normal(0, 1, n)\n    u /= np.linalg.norm(u)\n    v = X.T.dot(u)\n    v /= np.linalg.norm(v)\n    for _ in range(max_iter):      \n        u = X.dot(v)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)       \n    return u.reshape(-1, 1), v.reshape(1, -1)\n\n\ndef do_step_size(dg, D, W = None):\n    if W is None:\n        denom = np.linalg.norm(D, 'fro')**2\n    else:\n        denom = np.linalg.norm(W * D, 'fro')**2\n    step_size = dg / denom\n    step_size = min(step_size, 1.0)\n    if step_size &lt; 0:\n        print (\"Warning: Step Size is less than 0\")\n        step_size = 1.0\n    return step_size\n\n\ndef frank_wolfe_minimize_step(X, Y, r, istep, W = None, mask = None):\n    \n    # 1. Gradient for X_(t-1)\n    G = f_gradient(X, Y, W = W, mask = mask)\n    # 2. Linear optimization subproblem\n    power_iter = 10 + int(istep / 50)\n    S = linopt_oracle(G, r, max_iter = power_iter)\n    # 3. Define D\n    D = X - S\n    # 4. Duality gap\n    dg = np.trace(D.T @ G)\n    # 5. Step size\n    step = do_step_size(dg, D, W = W)\n    # 6. Update\n    Xnew = X - step * D\n    return Xnew, G, dg, step\n\n\ndef frank_wolfe_minimize(Y, r, X0 = None,\n                         weight = None,\n                         mask = None,\n                         max_iter = 1000, tol = 1e-8,\n                         return_all = True,\n                         debug = False, debug_step = 10):\n    \n    # Step 0\n    old_X = np.zeros_like(Y) if X0 is None else X0.copy()\n    dg = np.inf\n\n    if return_all:\n        dg_list = [dg]\n        fx_list = [f_objective(old_X, Y, W = weight, mask = mask)]\n        st_list = [1]\n        \n    # Steps 1, ..., max_iter\n    for istep in range(max_iter):\n        X, G, dg, step = \\\n            frank_wolfe_minimize_step(old_X, Y, r, istep, W = weight, mask = mask)\n        f_obj = f_objective(X, Y, W = weight, mask = mask)\n\n        if return_all:\n            dg_list.append(dg)\n            fx_list.append(f_obj)\n            st_list.append(step)\n        \n        if debug:\n            if (istep % debug_step == 0):\n                print (f\"Iteration {istep}. Step size {step:.3f}. Duality Gap {dg:g}\")\n        if np.abs(dg) &lt;= tol:\n            break\n            \n        old_X = X.copy()\n        \n    if return_all:\n        return X, dg_list, fx_list, st_list\n    else:\n        return X\n\n\n\nCode\nX_opt, dg_list, fx_list, step_list = frank_wolfe_minimize(Xcent, 100.0, max_iter = 10, debug = True, debug_step = 1)\n\n\nIteration 0. Step size 1.000. Duality Gap 47280.8\nWarning: Step Size is less than 0\nIteration 1. Step size 1.000. Duality Gap -0.17212\nWarning: Step Size is less than 0\nIteration 2. Step size 1.000. Duality Gap -9.51455\nWarning: Step Size is less than 0\nIteration 3. Step size 1.000. Duality Gap -11.2205\nIteration 4. Step size 0.386. Duality Gap 582.621\nIteration 5. Step size 1.000. Duality Gap 143.495\nIteration 6. Step size 0.671. Duality Gap 47.3667\nWarning: Step Size is less than 0\nIteration 7. Step size 1.000. Duality Gap -454.516\nIteration 8. Step size 0.529. Duality Gap 1893.78\nIteration 9. Step size 1.000. Duality Gap 419.05\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nkp = 10\nax1.plot(np.arange(kp - 2), dg_list[2:kp])\nax2.plot(np.arange(kp - 1), fx_list[1:kp])\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(X_opt)\n\n\n3\n\n\n\n\nCode\nnuclear_norm(X_opt)\n\n\n99.99956475737909\n\n\n\n\nCode\nX_opt_cent = X_opt - np.mean(X_opt, axis = 0, keepdims = True)\nU_fw, S_fw, Vt_fw = np.linalg.svd(X_opt_cent)\npca_fw = U_fw @ np.diag(S_fw)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\nsvd_pc1 = pca_fw[:, idx1]\nsvd_pc2 = pca_fw[:, idx2]\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-08-08-npd-application.html",
    "href": "notebooks/explore/2023-08-08-npd-application.html",
    "title": "Which noise model is the best to capture the distinct GWAS phenotypes?",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn"
  },
  {
    "objectID": "notebooks/explore/2023-08-08-npd-application.html#ialm---rpca",
    "href": "notebooks/explore/2023-08-08-npd-application.html#ialm---rpca",
    "title": "Which noise model is the best to capture the distinct GWAS phenotypes?",
    "section": "IALM - RPCA",
    "text": "IALM - RPCA\n\n\nCode\nrpca = IALM(max_iter = 1000, mu_update_method='admm', show_progress = True)\nrpca.fit(X_cent, mask = X_nan_mask)\n\n\n2023-08-08 23:52:50,980 | nnwmf.optimize.inexact_alm               | DEBUG   | Fit RPCA using IALM (mu update admm, lamba = 0.0100)\n2023-08-08 23:52:51,174 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 0. Primal residual 0.893741. Dual residual 0.000574896\n2023-08-08 23:52:58,800 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 100. Primal residual 6.36112e-05. Dual residual 2.55942e-05\n2023-08-08 23:53:06,396 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 200. Primal residual 2.26287e-05. Dual residual 2.64972e-06\n2023-08-08 23:53:13,952 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 300. Primal residual 6.03351e-06. Dual residual 1.69738e-06\n2023-08-08 23:53:21,531 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 400. Primal residual 4.08978e-06. Dual residual 8.33885e-07\n2023-08-08 23:53:29,124 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 500. Primal residual 3.12383e-06. Dual residual 5.16786e-07\n2023-08-08 23:53:36,784 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 600. Primal residual 2.50002e-06. Dual residual 3.39206e-07\n2023-08-08 23:53:44,356 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 700. Primal residual 2.03607e-06. Dual residual 3.14692e-07\n2023-08-08 23:53:51,946 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 800. Primal residual 8.29061e-07. Dual residual 5.20502e-07\n2023-08-08 23:53:59,570 | nnwmf.optimize.inexact_alm               | INFO    | Iteration 900. Primal residual 4.70128e-07. Dual residual 2.64999e-07\n\n\n\n\nCode\nnp.linalg.matrix_rank(rpca.L_)\n\n\n50\n\n\n\n\nCode\nnp.linalg.norm(rpca.L_, 'nuc')\n\n\n1351.8480150432197\n\n\n\n\nCode\nnp.sum(np.abs(rpca.E_)) / np.prod(X_cent.shape)\n\n\n0.590626180741227"
  },
  {
    "objectID": "notebooks/explore/2023-08-08-npd-application.html#fw---nnm",
    "href": "notebooks/explore/2023-08-08-npd-application.html#fw---nnm",
    "title": "Which noise model is the best to capture the distinct GWAS phenotypes?",
    "section": "FW - NNM",
    "text": "FW - NNM\nAlso check how the cross-validation works\n\n\nCode\nnnmcv = FrankWolfe_CV(chain_init = True, reverse_path = False, debug = True, kfolds = 5)\nnnmcv.fit(X_nan_cent)\n\n\n2023-08-08 23:54:07,309 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Cross-validation over 14 ranks.\n2023-08-08 23:54:07,338 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 1 ...\n2023-08-08 23:54:07,359 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-08 23:54:09,099 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-08 23:54:10,281 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-08 23:54:14,327 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-08 23:54:15,509 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-08 23:54:17,192 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-08 23:54:21,295 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-08 23:54:40,008 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-08 23:55:00,369 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-08 23:55:49,015 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-08 23:56:26,738 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-08 23:59:33,286 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 00:04:42,654 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 00:11:05,668 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 00:13:31,487 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 2 ...\n2023-08-09 00:13:31,526 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 00:13:32,692 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 00:13:33,858 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 00:13:35,041 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 00:13:36,189 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 00:13:39,628 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 00:13:44,260 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 00:14:00,394 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 00:14:48,814 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 00:15:04,288 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 00:15:29,648 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 00:17:34,640 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 00:22:51,611 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 00:28:55,921 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 00:31:24,474 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 3 ...\n2023-08-09 00:31:24,505 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 00:31:25,651 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 00:31:26,832 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 00:31:28,004 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 00:31:32,073 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 00:31:33,801 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 00:31:37,232 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 00:31:46,521 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 00:33:12,132 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 00:33:19,179 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 00:34:26,927 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 00:37:17,619 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 00:42:23,640 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 00:48:33,828 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 00:51:02,371 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 4 ...\n2023-08-09 00:51:02,394 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 00:51:03,577 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 00:51:04,724 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 00:51:06,453 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 00:51:07,619 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 00:51:09,221 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 00:51:10,957 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 00:51:20,711 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 00:52:36,563 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 00:52:50,815 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 00:54:04,960 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 00:56:22,779 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 01:01:47,072 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 01:08:15,284 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n2023-08-09 01:10:46,516 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 5 ...\n2023-08-09 01:10:46,547 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1.0000\n2023-08-09 01:10:47,748 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2.0000\n2023-08-09 01:10:48,947 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4.0000\n2023-08-09 01:10:50,135 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8.0000\n2023-08-09 01:10:51,338 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 16.0000\n2023-08-09 01:10:53,128 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 32.0000\n2023-08-09 01:10:56,105 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 64.0000\n2023-08-09 01:11:09,642 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 128.0000\n2023-08-09 01:12:05,863 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 256.0000\n2023-08-09 01:12:17,409 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 512.0000\n2023-08-09 01:12:58,131 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 1024.0000\n2023-08-09 01:15:24,342 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 2048.0000\n2023-08-09 01:21:14,873 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 4096.0000\n2023-08-09 01:27:29,020 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Rank 8192.0000\n\n\nWe do a 5-fold cross-validation. We randomly mask a part of the data and apply our method to recover the masked data. The test error is the RMSE between the receovered data and input masked data. In Figure 4, we plot the RMSE of the test data for each CV fold.\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(5):\n    ax1.plot(np.log10(list(nnmcv.test_error.keys())), [x[k] for x in nnmcv.test_error.values()], 'o-')\nax1.set_xlabel(\"Rank\")\nax1.set_ylabel(\"RMSE on test data\")\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Error on held out test data for each of the 5-fold cross-validation sets to find the best rank constraint for NNM\n\n\n\n\n\n\n\nCode\nnp.linalg.norm(X_cent, 'nuc')\n\n\n7274.4182279699835\n\n\n\n\nCode\nnnm = FrankWolfe(model = 'nnm', svd_max_iter = 50, show_progress = True, debug = True)\nnnm.fit(X_cent, 1024.0)\n\n\n2023-08-09 10:37:56,367 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 0.459. Duality Gap 481580\n2023-08-09 10:38:52,567 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.007. Duality Gap 7151\n2023-08-09 10:39:49,462 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.007. Duality Gap 5557.79\n2023-08-09 10:40:46,524 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.002. Duality Gap 1879.71"
  },
  {
    "objectID": "notebooks/explore/2023-08-08-npd-application.html#nnmsparse---fw",
    "href": "notebooks/explore/2023-08-08-npd-application.html#nnmsparse---fw",
    "title": "Which noise model is the best to capture the distinct GWAS phenotypes?",
    "section": "NNMSparse - FW",
    "text": "NNMSparse - FW\n\n\nCode\nnnm_sparse = FrankWolfe(model = 'nnm-sparse', max_iter = 1000, svd_max_iter = 50, \n                        tol = 1e-3, step_tol = 1e-5, simplex_method = 'sort',\n                        show_progress = True, debug = True, print_skip = 100)\nnnm_sparse.fit(X_cent, (1024.0, 0.5))\n\n\n2023-08-09 11:18:55,842 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 1.13739e+07\n2023-08-09 11:20:58,655 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.004. Duality Gap 3961.77\n2023-08-09 11:23:02,637 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.002. Duality Gap 2424.21\n2023-08-09 11:25:05,620 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.002. Duality Gap 1759.04\n2023-08-09 11:27:09,644 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 400. Step size 0.001. Duality Gap 1295.86\n2023-08-09 11:29:13,135 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 500. Step size 0.001. Duality Gap 1060.19\n2023-08-09 11:31:17,120 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 600. Step size 0.001. Duality Gap 1058.08\n2023-08-09 11:33:20,274 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 700. Step size 0.002. Duality Gap 1046.78\n2023-08-09 11:35:24,039 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 800. Step size 0.001. Duality Gap 849.241\n2023-08-09 11:37:26,504 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 900. Step size 0.001. Duality Gap 944.618\n\n\n\n\nCode\nloadings = Vt.T @ np.diag(S)\nloadings[:, 0].shape\n\n\n(10068,)"
  },
  {
    "objectID": "notebooks/explore/2023-08-08-npd-application.html#principal-components-biplots",
    "href": "notebooks/explore/2023-08-08-npd-application.html#principal-components-biplots",
    "title": "Which noise model is the best to capture the distinct GWAS phenotypes?",
    "section": "Principal Components Biplots",
    "text": "Principal Components Biplots\nSuppose, we decompose \\mathbf{X} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\intercal}. Columns of \\mathbf{V} are the principal axes (aka principal directions, aka eigenvectors). The principal components are the columns of \\mathbf{U}\\mathbf{S} – the projections of the data on the the principal axes (note \\mathbf{X}\\mathbf{V} = \\mathbf{U}\\mathbf{S}). We plot the principal components as a scatter plot and color each point based on their broad disease category. To show the directions, we plot the loadings, \\mathbf{V}\\mathbf{S} as arrows. That is, the (x, y) coordinates of an i-th arrow endpoint are given by the i-th value in the first and second column of \\mathbf{V}\\mathbf{S}.\nA comprehensive discussion of biplot on Stackoverflow\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps\n\nloadings_rpca,       pcomps_rpca = get_principal_components(rpca.L_)\nloadings_nnm,        pcomps_nnm = get_principal_components(nnm.X)\nloadings_nnm_sparse, pcomps_nnm_sparse = get_principal_components(nnm_sparse.X)\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm, labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm_sparse, labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nquick_plot_trait_pc_scores(nnm_sparse.X, trait_indices, unique_labels, trait_colors)\n\n\n\n\n\n\n\n\n\n\n\nCode\nquick_plot_trait_pc_scores(nnm.X, trait_indices, unique_labels, trait_colors)\n\n\n\n\n\n\n\n\n\n\n\nCode\nquick_plot_trait_pc_scores(rpca.L_, trait_indices, unique_labels, trait_colors)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef quick_scale_plot_trait_pc_scores(ax, X, tindices, ulabels, tcolors, min_idx = 0, max_idx = 20):\n    '''\n    Quick helper function to plot the same thing many times\n    '''\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    \n    #data = get_trait_pc_scores(U, S, tindices, ulabels, min_idx = min_idx, max_idx = max_idx, use_proportion = False)\n    data_scaled = get_trait_pc_scores(U, S, tindices, ulabels, min_idx = min_idx, max_idx = max_idx, use_proportion = True)\n    xlabels = [f\"{i + 1}\" for i in np.arange(min_idx, max_idx)]\n    plot_stacked_bars(ax, data_scaled, xlabels, tcolors, alpha = 0.8)\n\n    ax.set_xlabel(\"Principal Components\")\n    ax.set_ylabel(\"Trait-wise scores for each PC (scaled)\")\n    return\n\nfig = plt.figure(figsize = (12, 14))\nax1 = fig.add_subplot(311)\nax2 = fig.add_subplot(312)\nax3 = fig.add_subplot(313)\n\nquick_scale_plot_trait_pc_scores(ax1, rpca.L_, trait_indices, unique_labels, trait_colors)\nquick_scale_plot_trait_pc_scores(ax2, nnm.X, trait_indices, unique_labels, trait_colors)\nquick_scale_plot_trait_pc_scores(ax3, nnm_sparse.X, trait_indices, unique_labels, trait_colors)\nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_title(\"RPCA - IALM\")\n\nax2.set_title(\"NNM - FW\")\nax3.set_title(\"NNM Sparse - FW\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-05-12-preprocess.html",
    "href": "notebooks/explore/2023-05-12-preprocess.html",
    "title": "Preprocess NPD summary statistics",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as sc_stats\nimport collections\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120)\n\n\n\nRead\nSummary statistics for association of selected SNPs with multiple diseases. SNPs which had significant association with at least one phenotype were selected in a previous preprocessing step, details in this repository. Next, the summary statistics of those SNPs for all available phenotypes were included from the respective GWAS.\n\n\nCode\ndata_dir = \"../data\"\nstudies  = ['gtex', 'pgc', 'ieu']\nassoc_file = {}\nfor s in studies:\n    assoc_file[s] = f\"{data_dir}/{s}_assoc_sb.txt.gz\"\ntrait_df_filename = f\"{data_dir}/trait_meta.csv\"\n\n\n\n\nCode\nassoc = {}\n\nreq_cols = {\n    'SNP' : 'string',\n    'CHR' : 'string',\n    'BP'  : 'Int64', # use Pandas Int64 to handle NA values\n    'A1'  : 'string',\n    'A2'  : 'string',\n    'Z'   : np.float64,\n    'P'   : np.float64,\n    'BETA': np.float64,\n    'SE'  : np.float64,\n    'ID'  : 'string',\n    'TRAIT': 'string'\n}\n\nfor s in studies:\n    print (f\"Read summary statistics for {s}\")\n    header = pd.read_csv(assoc_file[s], nrows=0, sep='\\t')\n    use_cols   = [x for x in req_cols.keys() if x in header.columns]\n    use_dtypes = {k:v for k, v in req_cols.items() if k in use_cols}\n    assoc[s] = pd.read_csv(assoc_file[s], sep = '\\t',\n                           usecols = use_cols,\n                           dtype = use_dtypes\n                          )\n\n\nRead summary statistics for gtex\nRead summary statistics for pgc\nRead summary statistics for ieu\n\n\n\n\nCode\ntrait_df = pd.read_csv(trait_df_filename)\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n\nClean\n\nCombine all summary statistics to a single DataFrame.\n\n\nCode\nassoc_df = pd.concat([v for k,v in assoc.items()])\nassoc_df['A1'] = assoc_df['A1'].str.upper()\nassoc_df['A2'] = assoc_df['A2'].str.upper()\nassoc_df['TRAIT'] = assoc_df['TRAIT'].fillna(assoc_df['ID'])\n\n\n\n\nCode\nassoc_df = assoc_df.drop_duplicates()\n\n\n\nassoc_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 680512 entries, 0 to 258531\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     680512 non-null  string \n 1   CHR     664837 non-null  string \n 2   BP      664837 non-null  Int64  \n 3   A2      680512 non-null  string \n 4   A1      680512 non-null  string \n 5   Z       260187 non-null  float64\n 6   P       680512 non-null  float64\n 7   BETA    640083 non-null  float64\n 8   SE      640068 non-null  float64\n 9   ID      680512 non-null  string \n 10  TRAIT   680512 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 63.0 MB\n\n\n\nassoc_df\n\n\n\n\n\n\n\n\nSNP\nCHR\nBP\nA2\nA1\nZ\nP\nBETA\nSE\nID\nTRAIT\n\n\n\n\n0\nrs147538909\nchr1\n115746\nT\nC\n-1.197695\n0.231036\nNaN\nNaN\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n1\nrs2977608\nchr1\n832873\nC\nA\n0.205174\n0.837436\n0.000454\n0.002213\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n2\nrs9442391\nchr1\n1048922\nC\nT\n1.415529\n0.156913\n0.002705\n0.001911\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n3\nrs6685064\nchr1\n1275912\nT\nC\n-1.869576\n0.061543\n-0.007038\n0.003764\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n4\nrs79113395\nchr1\n1659060\nA\nG\n-0.462598\n0.643653\n-0.000987\n0.002134\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n258527\nrs7599488\n2\n60718347\nT\nC\nNaN\n0.730000\n0.000530\n0.001563\nieu-b-13\nchildhood absence epilepsy\n\n\n258528\nrs75010292\n2\n55172536\nG\nA\nNaN\n0.091000\n-0.004222\n0.002496\nieu-b-13\nchildhood absence epilepsy\n\n\n258529\nrs10168513\n2\n55452375\nA\nG\nNaN\n0.860000\n-0.000413\n0.002291\nieu-b-13\nchildhood absence epilepsy\n\n\n258530\nrs17428810\n2\n32736043\nC\nT\nNaN\n0.840000\n-0.000342\n0.001684\nieu-b-13\nchildhood absence epilepsy\n\n\n258531\nrs615449\n2\n40469480\nA\nC\nNaN\n0.420000\n-0.002027\n0.002493\nieu-b-13\nchildhood absence epilepsy\n\n\n\n\n680512 rows × 11 columns\n\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10262\nNumber of unique studies: 92\n\n\n\n\nKeep only Biallelic SNPs\nassoc_df_fa: fa or ‘filter_alleles’ removes alleles which consists of more than two nucleotides.\n\n\nCode\ndef count_nucleotides(row, df):\n    '''\n    df must have 3 columns: SNP, A1, A2\n    '''\n    snp = row['SNP']\n    snp_dict = df[df['SNP'] == snp][['A1', 'A2']].to_dict('records')\n    return len(set([v for x in snp_dict for k, v in x.items()]))\n\n\n'''\nWhether to count the alleles.\nIf a SNP appears twice, most likely it is biallelic with (A1, A2) and (A2, A1) in the two rows.\nHowever, if the second row has (A2, A3), then it is not biallelic.\nTherefore, we count the unique nucleotides for each SNP.\n\n!!! This is slow. I have checked there are no (A2, A3) for the current data. Hence, avoid.\n'''\ndo_count_nucleotides = False\n\nalleles       = assoc_df[['SNP', 'A1', 'A2']].drop_duplicates()\ncount_colname = 'count'\nalleles_count = alleles['SNP'].value_counts().reset_index(name=count_colname)\nif do_count_nucleotides:\n    count_colname = 'nucleotide_count'\n    alleles_count[count_colname] = alleles_count.apply(count_nucleotides, axis = 1, args = (alleles,))\n    \nbiallelic = alleles_count[alleles_count[count_colname] &lt;= 2].merge(alleles, on='SNP', how='inner')\n\n# Arbitrary choice of alleles\none_arrange = biallelic.drop_duplicates('SNP', keep='first').filter(regex = '^(?!.*count)', axis = 1)\n\n\n\n\nCode\nassoc_df_allele1 = one_arrange.merge(assoc_df, on=['SNP', 'A1', 'A2'], how='inner')\nassoc_df_allele2 = one_arrange.merge(assoc_df, \n                        left_on=['SNP',  'A1', 'A2'], \n                        right_on=['SNP', 'A2', 'A1'],\n                        suffixes=('', '_DROP')).filter(regex='^(?!.*_DROP)')\nassoc_df_allele2['BETA'] = assoc_df_allele2['BETA'] * -1\nassoc_df_allele2['Z']    = assoc_df_allele2['Z'] * -1\n\nassoc_df_fa = pd.concat([assoc_df_allele1, assoc_df_allele2]) # fa: fixed alleles\n\n\n\nassoc_df_fa.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 677779 entries, 0 to 105160\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     677779 non-null  string \n 1   A1      677779 non-null  string \n 2   A2      677779 non-null  string \n 3   CHR     662150 non-null  string \n 4   BP      662150 non-null  Int64  \n 5   Z       259246 non-null  float64\n 6   P       677779 non-null  float64\n 7   BETA    637545 non-null  float64\n 8   SE      637530 non-null  float64\n 9   ID      677779 non-null  string \n 10  TRAIT   677779 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 62.7 MB\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fa['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fa['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10224\nNumber of unique studies: 92\n\n\n\n\nRemove SNPs found in fewer studies\nSNPs which are available in only a few studies are less powerful ‘features’. For a distribution of SNPs, see Figure 1.\n\n\nCode\nsnps_count = assoc_df_fa['SNP'].value_counts().reset_index(name='count')\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.hist(snps_count['count'], density = True)\nax1.set_xlabel('Number of studies available for each SNP')\nax1.set_ylabel('Density')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Distribution of SNPs in different studies\n\n\n\n\n\n\n\nCode\nassoc_df_fa_nsnp = snps_count[snps_count['count'] &gt;=20 ].merge(assoc_df_fa, on=['SNP'], how='inner').filter(regex = '^(?!.*count)', axis = 1)\n\n\n\nassoc_df_fa_nsnp.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 676261 entries, 0 to 676260\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     676261 non-null  string \n 1   A1      676261 non-null  string \n 2   A2      676261 non-null  string \n 3   CHR     660634 non-null  string \n 4   BP      660634 non-null  Int64  \n 5   Z       259054 non-null  float64\n 6   P       676261 non-null  float64\n 7   BETA    636027 non-null  float64\n 8   SE      636012 non-null  float64\n 9   ID      676261 non-null  string \n 10  TRAIT   676261 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 57.4 MB\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fa_nsnp['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fa_nsnp['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10068\nNumber of unique studies: 92\n\n\n\n\nRemove duplicate (SNP, ID) combinations\n\n\nCode\nassoc_df_fa_nsnp_nodup = assoc_df_fa_nsnp.drop_duplicates(subset=['SNP', 'ID'], keep = False)\n\n\n\nassoc_df_fa_nsnp_nodup.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 676229 entries, 0 to 676260\nData columns (total 11 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   SNP     676229 non-null  string \n 1   A1      676229 non-null  string \n 2   A2      676229 non-null  string \n 3   CHR     660602 non-null  string \n 4   BP      660602 non-null  Int64  \n 5   Z       259054 non-null  float64\n 6   P       676229 non-null  float64\n 7   BETA    635995 non-null  float64\n 8   SE      635980 non-null  float64\n 9   ID      676229 non-null  string \n 10  TRAIT   676229 non-null  string \ndtypes: Int64(1), float64(4), string(6)\nmemory usage: 62.6 MB\n\n\n\nassoc_df_fa_nsnp_nodup\n\n\n\n\n\n\n\n\nSNP\nA1\nA2\nCHR\nBP\nZ\nP\nBETA\nSE\nID\nTRAIT\n\n\n\n\n0\nrs10486722\nC\nT\nchr7\n41772310\n0.881222\n0.378198\n0.001757\n0.001994\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n1\nrs10486722\nC\nT\nchr7\n41772310\n1.369354\n0.170889\n0.003476\n0.002538\nUKB_1180_Morning_or_evening_person_chronotype\nUKB_1180_Morning_or_evening_person_chronotype\n\n\n2\nrs10486722\nC\nT\nchr7\n41772310\n-0.138782\n0.889622\n-0.000257\n0.001850\nUKB_1200_Sleeplessness_or_insomnia\nUKB_1200_Sleeplessness_or_insomnia\n\n\n3\nrs10486722\nC\nT\nchr7\n41772310\n0.442336\n0.658246\n0.000035\n0.000080\nUKB_20002_1243_self_reported_psychological_or_...\nUKB_20002_1243_self_reported_psychological_or_...\n\n\n4\nrs10486722\nC\nT\nchr7\n41772310\n0.275442\n0.782977\n0.000030\n0.000110\nUKB_20002_1262_self_reported_parkinsons_disease\nUKB_20002_1262_self_reported_parkinsons_disease\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n676256\nrs2322\nC\nG\n4\n9701603\nNaN\n0.396100\n-0.077600\n0.091500\nieu-b-7\nParkinson's disease\n\n\n676257\nrs2322\nC\nG\n4\n9701603\nNaN\n0.107599\n0.024041\n0.014941\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\n\n\n676258\nrs2322\nC\nG\n4\n9701603\nNaN\n0.430338\n0.016141\n0.020468\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\n\n\n676259\nrs2322\nC\nG\n4\n9701603\nNaN\n0.068770\n0.049301\n0.027090\nMHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_F...\nMHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_F...\n\n\n676260\nrs2322\nC\nG\n4\n9701603\nNaN\n0.721340\n0.006329\n0.017746\nMHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filter...\nMHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filter...\n\n\n\n\n676229 rows × 11 columns\n\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fa_nsnp_nodup['SNP'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fa_nsnp_nodup['ID'].unique())}\")\n\n\nNumber of unique SNPs: 10068\nNumber of unique studies: 92\n\n\n\n\n\nData Matrix\n\n\nCode\nbeta_df   = assoc_df_fa_nsnp_nodup[['SNP', 'ID', 'BETA']].pivot(index = 'SNP', columns = 'ID', values = 'BETA').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nse_df     = assoc_df_fa_nsnp_nodup[['SNP', 'ID', 'SE']].pivot(index = 'SNP', columns = 'ID', values = 'SE').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nzscore_df = assoc_df_fa_nsnp_nodup[['SNP', 'ID', 'Z']].pivot(index = 'SNP', columns = 'ID', values = 'Z').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\n\nse_df     = se_df.fillna(beta_df / zscore_df).replace(0, np.nan)\nzscore_df = zscore_df.fillna(beta_df / se_df)\nprec_df   = se_df.apply(lambda x: 1 / x / x)\n\n\n\nzscore_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10068 entries, rs1000031 to rs999494\nData columns (total 92 columns):\n #   Column                                                                                            Non-Null Count  Dtype  \n---  ------                                                                                            --------------  -----  \n 0   AD_sumstats_Jansenetal_2019sept.txt.gz                                                            10020 non-null  float64\n 1   CNCR_Insomnia_all                                                                                 9792 non-null   float64\n 2   ENIGMA_Intracraneal_Volume                                                                        9706 non-null   float64\n 3   GPC-NEO-NEUROTICISM                                                                               9430 non-null   float64\n 4   IGAP_Alzheimer                                                                                    9651 non-null   float64\n 5   ILAE_Genetic_generalised_epilepsy                                                                 9484 non-null   float64\n 6   Jones_et_al_2016_Chronotype                                                                       9782 non-null   float64\n 7   Jones_et_al_2016_SleepDuration                                                                    9782 non-null   float64\n 8   MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                       7801 non-null   float64\n 9   MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                           7826 non-null   float64\n 10  MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz            7977 non-null   float64\n 11  MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz  7971 non-null   float64\n 12  MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz     7972 non-null   float64\n 13  MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz          7973 non-null   float64\n 14  PGC3_SCZ_wave3_public.v2.txt.gz                                                                   9707 non-null   float64\n 15  PGC_ADHD_EUR_2017                                                                                 9688 non-null   float64\n 16  PGC_ASD_2017_CEU                                                                                  9626 non-null   float64\n 17  SSGAC_Depressive_Symptoms                                                                         9672 non-null   float64\n 18  SSGAC_Education_Years_Pooled                                                                      9740 non-null   float64\n 19  UKB_1160_Sleep_duration                                                                           9776 non-null   float64\n 20  UKB_1180_Morning_or_evening_person_chronotype                                                     9776 non-null   float64\n 21  UKB_1200_Sleeplessness_or_insomnia                                                                9776 non-null   float64\n 22  UKB_20002_1243_self_reported_psychological_or_psychiatric_problem                                 9776 non-null   float64\n 23  UKB_20002_1262_self_reported_parkinsons_disease                                                   9776 non-null   float64\n 24  UKB_20002_1265_self_reported_migraine                                                             9776 non-null   float64\n 25  UKB_20002_1289_self_reported_schizophrenia                                                        9776 non-null   float64\n 26  UKB_20002_1616_self_reported_insomnia                                                             9776 non-null   float64\n 27  UKB_20016_Fluid_intelligence_score                                                                9776 non-null   float64\n 28  UKB_20127_Neuroticism_score                                                                       9776 non-null   float64\n 29  UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy                                                         9776 non-null   float64\n 30  UKB_G43_Diagnoses_main_ICD10_G43_Migraine                                                         9776 non-null   float64\n 31  anxiety.meta.full.cc.txt.gz                                                                       8449 non-null   float64\n 32  anxiety.meta.full.fs.txt.gz                                                                       8418 non-null   float64\n 33  daner_PGC_BIP32b_mds7a_0416a.txt.gz                                                               9988 non-null   float64\n 34  daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz                                       8108 non-null   float64\n 35  daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz                                       8096 non-null   float64\n 36  daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz                8795 non-null   float64\n 37  iPSYCH-PGC_ASD_Nov2017.txt.gz                                                                     9654 non-null   float64\n 38  ieu-a-1000                                                                                        8792 non-null   float64\n 39  ieu-a-1009                                                                                        4022 non-null   float64\n 40  ieu-a-1018                                                                                        4009 non-null   float64\n 41  ieu-a-1019                                                                                        1439 non-null   float64\n 42  ieu-a-1029                                                                                        4180 non-null   float64\n 43  ieu-a-1041                                                                                        8494 non-null   float64\n 44  ieu-a-1042                                                                                        8492 non-null   float64\n 45  ieu-a-1043                                                                                        8496 non-null   float64\n 46  ieu-a-1044                                                                                        8495 non-null   float64\n 47  ieu-a-1045                                                                                        8496 non-null   float64\n 48  ieu-a-1046                                                                                        8491 non-null   float64\n 49  ieu-a-1047                                                                                        8496 non-null   float64\n 50  ieu-a-1048                                                                                        8498 non-null   float64\n 51  ieu-a-1061                                                                                        4117 non-null   float64\n 52  ieu-a-1062                                                                                        3940 non-null   float64\n 53  ieu-a-1063                                                                                        4059 non-null   float64\n 54  ieu-a-1064                                                                                        4120 non-null   float64\n 55  ieu-a-1065                                                                                        4117 non-null   float64\n 56  ieu-a-1066                                                                                        4120 non-null   float64\n 57  ieu-a-1067                                                                                        4116 non-null   float64\n 58  ieu-a-1068                                                                                        4117 non-null   float64\n 59  ieu-a-1085                                                                                        9043 non-null   float64\n 60  ieu-a-118                                                                                         8939 non-null   float64\n 61  ieu-a-297                                                                                         8870 non-null   float64\n 62  ieu-a-298                                                                                         242 non-null    float64\n 63  ieu-a-45                                                                                          2539 non-null   float64\n 64  ieu-a-808                                                                                         789 non-null    float64\n 65  ieu-a-810                                                                                         702 non-null    float64\n 66  ieu-a-812                                                                                         1206 non-null   float64\n 67  ieu-a-818                                                                                         377 non-null    float64\n 68  ieu-a-824                                                                                         719 non-null    float64\n 69  ieu-a-990                                                                                         7676 non-null   float64\n 70  ieu-b-10                                                                                          5586 non-null   float64\n 71  ieu-b-11                                                                                          5817 non-null   float64\n 72  ieu-b-12                                                                                          5828 non-null   float64\n 73  ieu-b-13                                                                                          5820 non-null   float64\n 74  ieu-b-14                                                                                          5780 non-null   float64\n 75  ieu-b-15                                                                                          5821 non-null   float64\n 76  ieu-b-16                                                                                          5829 non-null   float64\n 77  ieu-b-17                                                                                          5814 non-null   float64\n 78  ieu-b-18                                                                                          7781 non-null   float64\n 79  ieu-b-2                                                                                           9360 non-null   float64\n 80  ieu-b-43                                                                                          964 non-null    float64\n 81  ieu-b-5070                                                                                        7585 non-null   float64\n 82  ieu-b-7                                                                                           9227 non-null   float64\n 83  ieu-b-8                                                                                           5508 non-null   float64\n 84  ieu-b-9                                                                                           5631 non-null   float64\n 85  ocd_aug2017.txt.gz                                                                                9774 non-null   float64\n 86  pgc-bip2021-BDI.vcf.txt.gz                                                                        9551 non-null   float64\n 87  pgc-bip2021-BDII.vcf.txt.gz                                                                       9418 non-null   float64\n 88  pgc-bip2021-all.vcf.txt.gz                                                                        9718 non-null   float64\n 89  pgc.scz2                                                                                          9742 non-null   float64\n 90  pgcAN2.2019-07.vcf.txt.gz                                                                         9057 non-null   float64\n 91  pts_all_freeze2_overall.txt.gz                                                                    9988 non-null   float64\ndtypes: float64(92)\nmemory usage: 7.1 MB\n\n\n\npd.testing.assert_index_equal(beta_df.index, prec_df.index)\npd.testing.assert_index_equal(beta_df.index, zscore_df.index)\npd.testing.assert_index_equal(beta_df.columns, prec_df.columns)\npd.testing.assert_index_equal(beta_df.columns, zscore_df.columns)\n\n\nRemove studies with missingness\nStudies with high missingness are not good samples, hence they are removed. See histogram of missingness in Figure 2\n\n\nCode\nzscore_df\n\n\n\n\n\n\n\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz\nCNCR_Insomnia_all\nENIGMA_Intracraneal_Volume\nGPC-NEO-NEUROTICISM\nIGAP_Alzheimer\nILAE_Genetic_generalised_epilepsy\nJones_et_al_2016_Chronotype\nJones_et_al_2016_SleepDuration\nMDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_...\nMDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUK...\n...\nieu-b-7\nieu-b-8\nieu-b-9\nocd_aug2017.txt.gz\npgc-bip2021-BDI.vcf.txt.gz\npgc-bip2021-BDII.vcf.txt.gz\npgc-bip2021-all.vcf.txt.gz\npgc.scz2\npgcAN2.2019-07.vcf.txt.gz\npts_all_freeze2_overall.txt.gz\n\n\n\n\nrs1000031\n0.999531\n-0.327477\n2.184712\n1.241557\n0.441709\n-1.041816\n0.163658\n-0.163658\n0.336654\n0.793129\n...\n0.532189\nNaN\nNaN\n-0.198735\n1.057089\n-0.269020\n1.279776\n-0.433158\n-1.573766\n-1.674269\n\n\nrs1000269\n1.212805\n-1.046310\n0.001880\n0.741814\n-1.844296\n0.771000\n2.673787\n1.126391\n-0.092067\n-0.163246\n...\n1.665179\n-0.732000\n-0.699000\n0.100883\n-0.226381\n0.338368\n-0.924392\n0.832016\n0.681645\n-0.701776\n\n\nrs10003281\n0.813444\n2.034345\n-2.023031\n-1.750164\n-0.076778\n1.448634\n0.954165\n-1.805477\nNaN\nNaN\n...\n-0.475795\n4.437998\n2.366001\n0.967399\n0.286699\n-1.162661\n-0.199299\n0.014539\nNaN\n-1.379710\n\n\nrs10004866\n-0.011252\n1.327108\n1.004786\n1.442363\n-1.215173\n0.139000\n0.050154\n1.439531\n-2.458370\n-2.407460\n...\n-1.234375\n-2.520001\n-0.593997\n-0.685110\n0.902252\n1.106939\n1.776456\n-1.654677\n-0.964630\n0.851608\n\n\nrs10005235\n-0.612540\n-0.410609\n1.526040\n0.653087\n0.344062\n-1.868000\n2.183486\n-1.514102\n0.460191\n0.393006\n...\n0.387805\n-0.345000\n-0.960998\n0.177317\n-1.339598\n1.795867\n-1.249969\n2.349671\n0.996305\n-0.333356\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrs9989571\n-0.028306\n-0.208891\n1.883150\n0.366470\n0.821257\n0.712000\n-0.453762\n1.895698\n-0.218149\n-0.920789\n...\n-1.231511\n-0.820996\n0.712000\n-2.150176\n-0.877410\n-1.938969\n-2.729983\n3.207917\n1.469194\n-1.293122\n\n\nrs9991694\n0.679790\n-1.005571\n-1.004740\n0.753472\n-0.539271\n0.368971\n-1.674665\n2.862736\n3.744820\n3.583060\n...\n0.064417\nNaN\nNaN\n-2.884911\n-1.000231\n0.031860\n-1.248222\n2.309425\nNaN\n1.048454\n\n\nrs9992763\n-0.691405\n-0.010299\n2.067894\n-0.140010\n-0.419843\n1.320000\n0.138304\n-0.568052\n-0.019684\n0.194404\n...\n0.191860\n-0.074000\n1.030997\n-0.228287\n-0.051297\n0.781766\n0.010638\n0.456681\n-0.503370\n-1.435277\n\n\nrs9993607\n1.625392\n-0.391585\n-0.113291\n0.514268\n0.027576\n-0.512000\n-0.150969\n0.113039\n4.638940\n4.631950\n...\n-0.685106\n0.194000\n0.240001\n-0.790290\n-0.876804\n-0.577696\n-0.785670\n-0.062707\n0.240834\n-0.199740\n\n\nrs999494\n0.303642\n0.872613\n1.522435\n-0.227674\n1.390424\n-1.022395\n-0.138304\n-1.281552\n-1.873050\n-1.645590\n...\n-0.437500\n0.253000\n-0.926997\n-1.449674\n0.910515\n0.783853\n1.376043\n-5.195746\n-1.151316\n0.660120\n\n\n\n\n10068 rows × 92 columns\n\n\n\n\n\nCode\npd.DataFrame(zscore_df.isnull()).mean(axis=0)\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz    0.004768\nCNCR_Insomnia_all                         0.027414\nENIGMA_Intracraneal_Volume                0.035956\nGPC-NEO-NEUROTICISM                       0.063369\nIGAP_Alzheimer                            0.041418\n                                            ...   \npgc-bip2021-BDII.vcf.txt.gz               0.064561\npgc-bip2021-all.vcf.txt.gz                0.034764\npgc.scz2                                  0.032380\npgcAN2.2019-07.vcf.txt.gz                 0.100417\npts_all_freeze2_overall.txt.gz            0.007946\nLength: 92, dtype: float64\n\n\n\n\nCode\nmissing_df = pd.DataFrame(zscore_df.isnull()).mean(axis = 0).reset_index(name = 'missingness').rename(columns = {'index': 'ID'})\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.hist(missing_df['missingness'], density = True, bins = 20)\nax1.set_xlabel('Missingness')\nax1.set_ylabel('Density')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Missingness of SNPs in different studies\n\n\n\n\n\n\n\nCode\nkeep_columns = missing_df[missing_df['missingness'] &lt;= 0.6]['ID'].to_list()\nprint(f\"Keeping {len(keep_columns)} phenotypes with low missingness\")\n\nbeta_df_sub   = beta_df[keep_columns]\nprec_df_sub   = prec_df[keep_columns]\nzscore_df_sub = zscore_df[keep_columns]\nse_df_sub     = se_df[keep_columns]\n\nremove_columns = [x for x in missing_df['ID'].to_list() if x not in keep_columns]\nprint(\"Phenotypes removed:\")\nfor x in remove_columns:\n    missingness = missing_df.loc[missing_df['ID'] == x]['missingness'].iloc[0]\n    print(f\"\\t{missingness:.3f}\\t{x}\\t{phenotype_dict[x]}\")\n\n\nKeeping 80 phenotypes with low missingness\nPhenotypes removed:\n    0.601   ieu-a-1009  Other psych\n    0.602   ieu-a-1018  Other psych\n    0.857   ieu-a-1019  BD\n    0.609   ieu-a-1062  Cognition\n    0.976   ieu-a-298   Neurodegenerative\n    0.748   ieu-a-45    Other psych\n    0.922   ieu-a-808   BD\n    0.930   ieu-a-810   SZ\n    0.880   ieu-a-812   Neurodegenerative\n    0.963   ieu-a-818   Neurodegenerative\n    0.929   ieu-a-824   Neurodegenerative\n    0.904   ieu-b-43    Other psych\n\n\n\nbeta_df_sub.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10068 entries, rs1000031 to rs999494\nData columns (total 80 columns):\n #   Column                                                                                            Non-Null Count  Dtype  \n---  ------                                                                                            --------------  -----  \n 0   AD_sumstats_Jansenetal_2019sept.txt.gz                                                            10020 non-null  float64\n 1   CNCR_Insomnia_all                                                                                 8950 non-null   float64\n 2   ENIGMA_Intracraneal_Volume                                                                        8351 non-null   float64\n 3   GPC-NEO-NEUROTICISM                                                                               3623 non-null   float64\n 4   IGAP_Alzheimer                                                                                    8033 non-null   float64\n 5   ILAE_Genetic_generalised_epilepsy                                                                 0 non-null      float64\n 6   Jones_et_al_2016_Chronotype                                                                       8807 non-null   float64\n 7   Jones_et_al_2016_SleepDuration                                                                    8807 non-null   float64\n 8   MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                       7801 non-null   float64\n 9   MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                           7826 non-null   float64\n 10  MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz            7977 non-null   float64\n 11  MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz  7971 non-null   float64\n 12  MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz     7972 non-null   float64\n 13  MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz          7973 non-null   float64\n 14  PGC3_SCZ_wave3_public.v2.txt.gz                                                                   9707 non-null   float64\n 15  PGC_ADHD_EUR_2017                                                                                 7880 non-null   float64\n 16  PGC_ASD_2017_CEU                                                                                  7287 non-null   float64\n 17  SSGAC_Depressive_Symptoms                                                                         7888 non-null   float64\n 18  SSGAC_Education_Years_Pooled                                                                      8824 non-null   float64\n 19  UKB_1160_Sleep_duration                                                                           8829 non-null   float64\n 20  UKB_1180_Morning_or_evening_person_chronotype                                                     8829 non-null   float64\n 21  UKB_1200_Sleeplessness_or_insomnia                                                                8829 non-null   float64\n 22  UKB_20002_1243_self_reported_psychological_or_psychiatric_problem                                 8829 non-null   float64\n 23  UKB_20002_1262_self_reported_parkinsons_disease                                                   8829 non-null   float64\n 24  UKB_20002_1265_self_reported_migraine                                                             8829 non-null   float64\n 25  UKB_20002_1289_self_reported_schizophrenia                                                        8829 non-null   float64\n 26  UKB_20002_1616_self_reported_insomnia                                                             8829 non-null   float64\n 27  UKB_20016_Fluid_intelligence_score                                                                8829 non-null   float64\n 28  UKB_20127_Neuroticism_score                                                                       8829 non-null   float64\n 29  UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy                                                         8829 non-null   float64\n 30  UKB_G43_Diagnoses_main_ICD10_G43_Migraine                                                         8829 non-null   float64\n 31  anxiety.meta.full.cc.txt.gz                                                                       8449 non-null   float64\n 32  anxiety.meta.full.fs.txt.gz                                                                       8418 non-null   float64\n 33  daner_PGC_BIP32b_mds7a_0416a.txt.gz                                                               9988 non-null   float64\n 34  daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz                                       8108 non-null   float64\n 35  daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz                                       8096 non-null   float64\n 36  daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz                8795 non-null   float64\n 37  iPSYCH-PGC_ASD_Nov2017.txt.gz                                                                     9654 non-null   float64\n 38  ieu-a-1000                                                                                        8792 non-null   float64\n 39  ieu-a-1029                                                                                        4180 non-null   float64\n 40  ieu-a-1041                                                                                        8494 non-null   float64\n 41  ieu-a-1042                                                                                        8492 non-null   float64\n 42  ieu-a-1043                                                                                        8496 non-null   float64\n 43  ieu-a-1044                                                                                        8495 non-null   float64\n 44  ieu-a-1045                                                                                        8496 non-null   float64\n 45  ieu-a-1046                                                                                        8491 non-null   float64\n 46  ieu-a-1047                                                                                        8496 non-null   float64\n 47  ieu-a-1048                                                                                        8498 non-null   float64\n 48  ieu-a-1061                                                                                        4117 non-null   float64\n 49  ieu-a-1063                                                                                        4059 non-null   float64\n 50  ieu-a-1064                                                                                        4120 non-null   float64\n 51  ieu-a-1065                                                                                        4117 non-null   float64\n 52  ieu-a-1066                                                                                        4120 non-null   float64\n 53  ieu-a-1067                                                                                        4116 non-null   float64\n 54  ieu-a-1068                                                                                        4117 non-null   float64\n 55  ieu-a-1085                                                                                        9043 non-null   float64\n 56  ieu-a-118                                                                                         8939 non-null   float64\n 57  ieu-a-297                                                                                         8870 non-null   float64\n 58  ieu-a-990                                                                                         7676 non-null   float64\n 59  ieu-b-10                                                                                          5586 non-null   float64\n 60  ieu-b-11                                                                                          5817 non-null   float64\n 61  ieu-b-12                                                                                          5828 non-null   float64\n 62  ieu-b-13                                                                                          5820 non-null   float64\n 63  ieu-b-14                                                                                          5780 non-null   float64\n 64  ieu-b-15                                                                                          5821 non-null   float64\n 65  ieu-b-16                                                                                          5829 non-null   float64\n 66  ieu-b-17                                                                                          5814 non-null   float64\n 67  ieu-b-18                                                                                          7781 non-null   float64\n 68  ieu-b-2                                                                                           9360 non-null   float64\n 69  ieu-b-5070                                                                                        7585 non-null   float64\n 70  ieu-b-7                                                                                           9227 non-null   float64\n 71  ieu-b-8                                                                                           5508 non-null   float64\n 72  ieu-b-9                                                                                           5631 non-null   float64\n 73  ocd_aug2017.txt.gz                                                                                9774 non-null   float64\n 74  pgc-bip2021-BDI.vcf.txt.gz                                                                        9551 non-null   float64\n 75  pgc-bip2021-BDII.vcf.txt.gz                                                                       9418 non-null   float64\n 76  pgc-bip2021-all.vcf.txt.gz                                                                        9718 non-null   float64\n 77  pgc.scz2                                                                                          8775 non-null   float64\n 78  pgcAN2.2019-07.vcf.txt.gz                                                                         9057 non-null   float64\n 79  pts_all_freeze2_overall.txt.gz                                                                    9988 non-null   float64\ndtypes: float64(80)\nmemory usage: 6.2 MB\n\n\n\nprec_df_sub.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 10068 entries, rs1000031 to rs999494\nData columns (total 80 columns):\n #   Column                                                                                            Non-Null Count  Dtype  \n---  ------                                                                                            --------------  -----  \n 0   AD_sumstats_Jansenetal_2019sept.txt.gz                                                            10020 non-null  float64\n 1   CNCR_Insomnia_all                                                                                 8950 non-null   float64\n 2   ENIGMA_Intracraneal_Volume                                                                        8351 non-null   float64\n 3   GPC-NEO-NEUROTICISM                                                                               3623 non-null   float64\n 4   IGAP_Alzheimer                                                                                    8033 non-null   float64\n 5   ILAE_Genetic_generalised_epilepsy                                                                 0 non-null      float64\n 6   Jones_et_al_2016_Chronotype                                                                       8807 non-null   float64\n 7   Jones_et_al_2016_SleepDuration                                                                    8807 non-null   float64\n 8   MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                       7801 non-null   float64\n 9   MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz                                           7826 non-null   float64\n 10  MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz            7977 non-null   float64\n 11  MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz  7971 non-null   float64\n 12  MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz     7972 non-null   float64\n 13  MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz          7973 non-null   float64\n 14  PGC3_SCZ_wave3_public.v2.txt.gz                                                                   9707 non-null   float64\n 15  PGC_ADHD_EUR_2017                                                                                 7880 non-null   float64\n 16  PGC_ASD_2017_CEU                                                                                  7287 non-null   float64\n 17  SSGAC_Depressive_Symptoms                                                                         7888 non-null   float64\n 18  SSGAC_Education_Years_Pooled                                                                      8824 non-null   float64\n 19  UKB_1160_Sleep_duration                                                                           8829 non-null   float64\n 20  UKB_1180_Morning_or_evening_person_chronotype                                                     8829 non-null   float64\n 21  UKB_1200_Sleeplessness_or_insomnia                                                                8829 non-null   float64\n 22  UKB_20002_1243_self_reported_psychological_or_psychiatric_problem                                 8829 non-null   float64\n 23  UKB_20002_1262_self_reported_parkinsons_disease                                                   8829 non-null   float64\n 24  UKB_20002_1265_self_reported_migraine                                                             8829 non-null   float64\n 25  UKB_20002_1289_self_reported_schizophrenia                                                        8829 non-null   float64\n 26  UKB_20002_1616_self_reported_insomnia                                                             8829 non-null   float64\n 27  UKB_20016_Fluid_intelligence_score                                                                8829 non-null   float64\n 28  UKB_20127_Neuroticism_score                                                                       8829 non-null   float64\n 29  UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy                                                         8829 non-null   float64\n 30  UKB_G43_Diagnoses_main_ICD10_G43_Migraine                                                         8829 non-null   float64\n 31  anxiety.meta.full.cc.txt.gz                                                                       8449 non-null   float64\n 32  anxiety.meta.full.fs.txt.gz                                                                       8418 non-null   float64\n 33  daner_PGC_BIP32b_mds7a_0416a.txt.gz                                                               9988 non-null   float64\n 34  daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz                                       8108 non-null   float64\n 35  daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz                                       8096 non-null   float64\n 36  daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz                8795 non-null   float64\n 37  iPSYCH-PGC_ASD_Nov2017.txt.gz                                                                     9654 non-null   float64\n 38  ieu-a-1000                                                                                        8792 non-null   float64\n 39  ieu-a-1029                                                                                        4180 non-null   float64\n 40  ieu-a-1041                                                                                        8494 non-null   float64\n 41  ieu-a-1042                                                                                        8492 non-null   float64\n 42  ieu-a-1043                                                                                        8496 non-null   float64\n 43  ieu-a-1044                                                                                        8495 non-null   float64\n 44  ieu-a-1045                                                                                        8496 non-null   float64\n 45  ieu-a-1046                                                                                        8491 non-null   float64\n 46  ieu-a-1047                                                                                        8496 non-null   float64\n 47  ieu-a-1048                                                                                        8498 non-null   float64\n 48  ieu-a-1061                                                                                        4117 non-null   float64\n 49  ieu-a-1063                                                                                        4059 non-null   float64\n 50  ieu-a-1064                                                                                        4120 non-null   float64\n 51  ieu-a-1065                                                                                        4117 non-null   float64\n 52  ieu-a-1066                                                                                        4120 non-null   float64\n 53  ieu-a-1067                                                                                        4116 non-null   float64\n 54  ieu-a-1068                                                                                        4117 non-null   float64\n 55  ieu-a-1085                                                                                        9043 non-null   float64\n 56  ieu-a-118                                                                                         8939 non-null   float64\n 57  ieu-a-297                                                                                         8870 non-null   float64\n 58  ieu-a-990                                                                                         7676 non-null   float64\n 59  ieu-b-10                                                                                          5586 non-null   float64\n 60  ieu-b-11                                                                                          5817 non-null   float64\n 61  ieu-b-12                                                                                          5828 non-null   float64\n 62  ieu-b-13                                                                                          5820 non-null   float64\n 63  ieu-b-14                                                                                          5780 non-null   float64\n 64  ieu-b-15                                                                                          5821 non-null   float64\n 65  ieu-b-16                                                                                          5829 non-null   float64\n 66  ieu-b-17                                                                                          5814 non-null   float64\n 67  ieu-b-18                                                                                          7781 non-null   float64\n 68  ieu-b-2                                                                                           9360 non-null   float64\n 69  ieu-b-5070                                                                                        7585 non-null   float64\n 70  ieu-b-7                                                                                           9227 non-null   float64\n 71  ieu-b-8                                                                                           5508 non-null   float64\n 72  ieu-b-9                                                                                           5631 non-null   float64\n 73  ocd_aug2017.txt.gz                                                                                9774 non-null   float64\n 74  pgc-bip2021-BDI.vcf.txt.gz                                                                        9551 non-null   float64\n 75  pgc-bip2021-BDII.vcf.txt.gz                                                                       9418 non-null   float64\n 76  pgc-bip2021-all.vcf.txt.gz                                                                        9718 non-null   float64\n 77  pgc.scz2                                                                                          8760 non-null   float64\n 78  pgcAN2.2019-07.vcf.txt.gz                                                                         9057 non-null   float64\n 79  pts_all_freeze2_overall.txt.gz                                                                    9988 non-null   float64\ndtypes: float64(80)\nmemory usage: 6.2 MB\n\n\n\n\n\nRemove studies with unreliable standard errors\nTo eliminate unreliable estimates of genetic associations, we remove samples (phenotypes) whose median standard error (SE) of beta value or log odds ratio is more than 0.2. The median SE for all the phenotypes are shown in Figure 3\nNote: For SE, the mean is not optimal because, in some studies, there are few SNPs with very high (~10000) SE, which inflates the mean.\n\n\nCode\nmean_se  = se_df_sub.median(axis = 0, skipna = True)\nmean_se  = pd.DataFrame(mean_se).set_axis([\"mean_se\"], axis = 1)\nbeta_std = beta_df_sub.std(axis = 0, skipna = True)\nbeta_std = pd.DataFrame(beta_std).set_axis([\"beta_std\"], axis = 1)\nerror_df = pd.concat([mean_se, beta_std], axis = 1)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(np.log10(error_df['beta_std']), np.log10(error_df['mean_se']), alpha = 0.5, s = 100)\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.set_yticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.plot_diag(ax1)\n\nkeep_columns = error_df.query(\"mean_se &lt;= 0.2\").index\nfor pid in error_df.index.to_list():\n    if pid not in keep_columns:\n        pid_text = f\"{pid} / {phenotype_dict[pid]}\"\n        xval = np.log10(error_df.loc[pid]['beta_std'])\n        yval = np.log10(error_df.loc[pid]['mean_se'])\n        ax1.annotate(pid_text, (xval, yval))\n\nax1.set_xlabel(r\"Standard Deviation of mean $\\beta$\")\nax1.set_ylabel(r\"Median of SE\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Calibration of SE against std of beta\n\n\n\n\n\n\n\nCode\nprint(f\"Keeping {len(keep_columns)} phenotypes with low standard error\")\n\nbeta_df_sub_lowse   = beta_df_sub[keep_columns]\nprec_df_sub_lowse   = prec_df_sub[keep_columns]\nzscore_df_sub_lowse = zscore_df_sub[keep_columns]\nse_df_sub_lowse     = se_df_sub[keep_columns]\n\nremove_columns = [x for x in error_df.index.to_list() if x not in keep_columns]\nprint(\"Phenotypes removed:\")\nfor x in remove_columns:\n    se = error_df.loc[x]['mean_se']\n    bstd = error_df.loc[x]['beta_std']\n    print(f\"\\t{se:.3f}\\t{bstd:.3f}\\t{x}\\t{phenotype_dict[x]}\")\n\n\nKeeping 70 phenotypes with low standard error\nPhenotypes removed:\n    2322.222    4272.766    ENIGMA_Intracraneal_Volume  Volume\n    nan nan ILAE_Genetic_generalised_epilepsy   Epilepsy\n    2330.240    4287.398    ieu-a-1041  Volume\n    1.326   2.479   ieu-a-1042  Volume\n    2.855   5.450   ieu-a-1043  Volume\n    5.930   11.025  ieu-a-1044  Volume\n    5.691   10.837  ieu-a-1045  Volume\n    2.319   4.306   ieu-a-1046  Volume\n    7.255   13.720  ieu-a-1047  Volume\n    7.590   14.253  ieu-a-1048  Volume\n\n\n\n\nRemove studies with unreliable z-scores\nTo eliminate unreliable z-scores, we make sure that the median of z-scores is less than 0.2\n\n\nCode\nmedian_zscore = zscore_df_sub_lowse.median(axis = 0, skipna = True).abs()\nmedian_zscore  = pd.DataFrame(median_zscore).set_axis([\"median_zscore\"], axis = 1)\n\nkeep_columns = median_zscore.query(\"median_zscore &lt;= 0.2\").index\nremove_columns = median_zscore.query(\"median_zscore &gt; 0.2\").index\n\nprint(f\"Keeping {len(keep_columns)} phenotypes with reasonable z-scores.\")\n\nbeta_df_sub_lowse_zchi2   = beta_df_sub_lowse[keep_columns]\nprec_df_sub_lowse_zchi2   = prec_df_sub_lowse[keep_columns]\nzscore_df_sub_lowse_zchi2 = zscore_df_sub_lowse[keep_columns]\nse_df_sub_lowse_zchi2     = se_df_sub_lowse[keep_columns]\n\nprint(\"Phenotypes removed:\")\nfor x in remove_columns:\n    print(f\"\\t{x}\\t{phenotype_dict[x]}\")\n\n\nKeeping 69 phenotypes with reasonable z-scores.\nPhenotypes removed:\n    ieu-b-5070  SZ\n\n\n\n\nCode\ndef q1(x, axis = None):\n    return np.percentile(x, 25, axis = axis)\n\ndef q3(x, axis = None):\n    return np.percentile(x, 75, axis = axis)\n\ndef iqr_outlier(x, axis = None, bar = 1.5, side = 'both'):\n    assert side in ['gt', 'lt', 'both'], 'Side should be `gt`, `lt` or `both`.'\n\n    d_iqr = sc_stats.iqr(x, axis = axis)\n    d_q1 = q1(x, axis = axis)\n    d_q3 = q3(x, axis = axis)\n    iqr_distance = np.multiply(d_iqr, bar)\n\n    stat_shape = list(x.shape)\n\n    if isinstance(axis, collections.abc.Iterable):\n        for single_axis in axis:\n            stat_shape[single_axis] = 1\n    else:\n        stat_shape[axis] = 1\n\n    if side in ['gt', 'both']:\n        upper_range = d_q3 + iqr_distance\n        upper_outlier = np.greater(x - upper_range.reshape(stat_shape), 0)\n    if side in ['lt', 'both']:\n        lower_range = d_q1 - iqr_distance\n        lower_outlier = np.less(x - lower_range.reshape(stat_shape), 0)\n\n    if side == 'gt':\n        return upper_outlier\n    if side == 'lt':\n        return lower_outlier\n    if side == 'both':\n        return np.logical_or(upper_outlier, lower_outlier)\n\ndef get_density(x, data):\n    density = sc_stats.gaussian_kde(data)\n    return density.pdf(x)\n\ndef get_bins(data, nbin, xmin, xmax):\n    xdelta = (np.max(data) - np.min(data)) / 10\n    if not xmin: xmin = np.min(data) - xdelta\n    if not xmax: xmax = np.max(data) + xdelta\n    bins = np.linspace(xmin, xmax, nbin)\n    xbin = [(bins[i] + bins[i+1]) / 2 for i in range(bins.shape[0] - 1)] # centers of the bins\n    return xmin, xmax, bins, xbin\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nremove_columns = []\nfor pid in zscore_df_sub_lowse_zchi2.columns.to_list():\n    x = zscore_df_sub_lowse_zchi2[pid].values\n    x_dropna = x[~np.isnan(x)]\n    outlier_mask = iqr_outlier(x_dropna, axis = 0, bar = 5)\n    #data = np.square(x_dropna[~outlier_mask])\n    data = x_dropna[~outlier_mask]\n    xmin, xmax, bins, xbin = get_bins(data, 100, None, None)\n    curve = get_density(xbin, data)\n    #print (pid, xmax)\n    if xmax &gt;= 1000:\n        print (pid, xmax)\n        remove_columns.append(pid)\n    else:\n        ax1.plot(xbin, curve, label = pid)\n        \n#ax1.legend()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Distribution of z-scores for all phenotypes\n\n\n\n\n\n\n\nFlip Signs\n\n\nCode\nto_flip = [\n    \"AD_sumstats_Jansenetal_2019sept.txt.gz\",\n    \"daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz\",\n    \"Jones_et_al_2016_Chronotype\",\n    \"Jones_et_al_2016_SleepDuration\",\n    \"UKB_1160_Sleep_duration\",\n    #\"UKB_1180_Morning_or_evening_person_chronotype\",\n    \"MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\",\n    \"MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\",\n    \"PGC3_SCZ_wave3_public.v2.txt.gz\",\n]\n\nremaining_columns = beta_df_sub_lowse_zchi2.columns.to_list()\nfor flip_id in to_flip:\n    if flip_id in remaining_columns:\n        beta_df_sub_lowse_zchi2.loc[:, (flip_id)] = - beta_df_sub_lowse_zchi2[flip_id]\n        zscore_df_sub_lowse_zchi2.loc[:, (flip_id)] = - zscore_df_sub_lowse_zchi2[flip_id]\n        print(flip_id)\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz\ndaner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz\nJones_et_al_2016_Chronotype\nJones_et_al_2016_SleepDuration\nUKB_1160_Sleep_duration\nMDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\nMDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz\nPGC3_SCZ_wave3_public.v2.txt.gz\n\n\n\n\nSave Pickle\n\nbeta_df_filename = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename = f\"{data_dir}/prec_df.pkl\"\nse_df_filename   = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\nbeta_df_sub_lowse_zchi2.to_pickle(beta_df_filename)\nprec_df_sub_lowse_zchi2.to_pickle(prec_df_filename)\nse_df_sub_lowse_zchi2.to_pickle(se_df_filename)\nzscore_df_sub_lowse_zchi2.to_pickle(zscore_df_filename)\n\n\n\nCode\nzscore_df_sub_lowse_zchi2.columns.to_list()\n\n\n['AD_sumstats_Jansenetal_2019sept.txt.gz',\n 'CNCR_Insomnia_all',\n 'GPC-NEO-NEUROTICISM',\n 'IGAP_Alzheimer',\n 'Jones_et_al_2016_Chronotype',\n 'Jones_et_al_2016_SleepDuration',\n 'MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz',\n 'MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz',\n 'MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz',\n 'PGC3_SCZ_wave3_public.v2.txt.gz',\n 'PGC_ADHD_EUR_2017',\n 'PGC_ASD_2017_CEU',\n 'SSGAC_Depressive_Symptoms',\n 'SSGAC_Education_Years_Pooled',\n 'UKB_1160_Sleep_duration',\n 'UKB_1180_Morning_or_evening_person_chronotype',\n 'UKB_1200_Sleeplessness_or_insomnia',\n 'UKB_20002_1243_self_reported_psychological_or_psychiatric_problem',\n 'UKB_20002_1262_self_reported_parkinsons_disease',\n 'UKB_20002_1265_self_reported_migraine',\n 'UKB_20002_1289_self_reported_schizophrenia',\n 'UKB_20002_1616_self_reported_insomnia',\n 'UKB_20016_Fluid_intelligence_score',\n 'UKB_20127_Neuroticism_score',\n 'UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy',\n 'UKB_G43_Diagnoses_main_ICD10_G43_Migraine',\n 'anxiety.meta.full.cc.txt.gz',\n 'anxiety.meta.full.fs.txt.gz',\n 'daner_PGC_BIP32b_mds7a_0416a.txt.gz',\n 'daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz',\n 'daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz',\n 'daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz',\n 'iPSYCH-PGC_ASD_Nov2017.txt.gz',\n 'ieu-a-1000',\n 'ieu-a-1029',\n 'ieu-a-1061',\n 'ieu-a-1063',\n 'ieu-a-1064',\n 'ieu-a-1065',\n 'ieu-a-1066',\n 'ieu-a-1067',\n 'ieu-a-1068',\n 'ieu-a-1085',\n 'ieu-a-118',\n 'ieu-a-297',\n 'ieu-a-990',\n 'ieu-b-10',\n 'ieu-b-11',\n 'ieu-b-12',\n 'ieu-b-13',\n 'ieu-b-14',\n 'ieu-b-15',\n 'ieu-b-16',\n 'ieu-b-17',\n 'ieu-b-18',\n 'ieu-b-2',\n 'ieu-b-7',\n 'ieu-b-8',\n 'ieu-b-9',\n 'ocd_aug2017.txt.gz',\n 'pgc-bip2021-BDI.vcf.txt.gz',\n 'pgc-bip2021-BDII.vcf.txt.gz',\n 'pgc-bip2021-all.vcf.txt.gz',\n 'pgc.scz2',\n 'pgcAN2.2019-07.vcf.txt.gz',\n 'pts_all_freeze2_overall.txt.gz']\n\n\n\n\nSave SNP information\n\n\nCode\nassoc_df_fa_nsnp_nodup\n\n\n\n\n\n\n\n\n\nSNP\nA1\nA2\nCHR\nBP\nZ\nP\nBETA\nSE\nID\nTRAIT\n\n\n\n\n0\nrs10486722\nC\nT\nchr7\n41772310\n0.881222\n0.378198\n0.001757\n0.001994\nUKB_1160_Sleep_duration\nUKB_1160_Sleep_duration\n\n\n1\nrs10486722\nC\nT\nchr7\n41772310\n1.369354\n0.170889\n0.003476\n0.002538\nUKB_1180_Morning_or_evening_person_chronotype\nUKB_1180_Morning_or_evening_person_chronotype\n\n\n2\nrs10486722\nC\nT\nchr7\n41772310\n-0.138782\n0.889622\n-0.000257\n0.001850\nUKB_1200_Sleeplessness_or_insomnia\nUKB_1200_Sleeplessness_or_insomnia\n\n\n3\nrs10486722\nC\nT\nchr7\n41772310\n0.442336\n0.658246\n0.000035\n0.000080\nUKB_20002_1243_self_reported_psychological_or_...\nUKB_20002_1243_self_reported_psychological_or_...\n\n\n4\nrs10486722\nC\nT\nchr7\n41772310\n0.275442\n0.782977\n0.000030\n0.000110\nUKB_20002_1262_self_reported_parkinsons_disease\nUKB_20002_1262_self_reported_parkinsons_disease\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n676256\nrs2322\nC\nG\n4\n9701603\nNaN\n0.396100\n-0.077600\n0.091500\nieu-b-7\nParkinson's disease\n\n\n676257\nrs2322\nC\nG\n4\n9701603\nNaN\n0.107599\n0.024041\n0.014941\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\n\n\n676258\nrs2322\nC\nG\n4\n9701603\nNaN\n0.430338\n0.016141\n0.020468\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\n\n\n676259\nrs2322\nC\nG\n4\n9701603\nNaN\n0.068770\n0.049301\n0.027090\nMHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_F...\nMHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_F...\n\n\n676260\nrs2322\nC\nG\n4\n9701603\nNaN\n0.721340\n0.006329\n0.017746\nMHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filter...\nMHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filter...\n\n\n\n\n676229 rows × 11 columns\n\n\n\n\n\nCode\nall_snp_info = assoc_df_fa_nsnp_nodup[['SNP', 'CHR', 'BP', 'A1', 'A2']].copy()\nall_snp_info_nona = all_snp_info.dropna(axis = 0)\nall_snp_info_nona.loc[:, 'CHR'] = all_snp_info_nona.CHR.str.replace('chr', '')\nall_snp_info_nodup = all_snp_info_nona.drop_duplicates(subset=['SNP'])\n\n\n\n\nCode\nall_snp_info_nodup\n\n\n\n\n\n\n\n\n\nSNP\nCHR\nBP\nA1\nA2\n\n\n\n\n0\nrs10486722\n7\n41772310\nC\nT\n\n\n91\nrs2416745\n9\n120018976\nT\nG\n\n\n182\nrs9287971\n2\n174067024\nG\nA\n\n\n273\nrs12660608\n6\n123017369\nT\nC\n\n\n364\nrs1693523\n15\n52535142\nC\nT\n\n\n...\n...\n...\n...\n...\n...\n\n\n676161\nrs7199470\n16\n10137879\nT\nG\n\n\n676181\nrs77337722\n18\n74935886\nT\nC\n\n\n676201\nrs183211079\n2\n151553344\nG\nA\n\n\n676221\nrs78346960\n17\n9121155\nA\nC\n\n\n676241\nrs2322\n4\n9701603\nC\nG\n\n\n\n\n10068 rows × 5 columns\n\n\n\n\n\nCode\nsnp_info_filename = f\"{data_dir}/snp_info.pkl\"\nall_snp_info_nodup.to_pickle(snp_info_filename)"
  },
  {
    "objectID": "notebooks/develop/2023-07-19-weighted-nnm-python-class.html",
    "href": "notebooks/develop/2023-07-19-weighted-nnm-python-class.html",
    "title": "Python Class for NNM using Frank-Wolfe algorithm",
    "section": "",
    "text": "About\nHere I develop the Python class for weighted nuclear norm minimization using Frank-Wolfe algorithm. This is not properly documented, but I keep this notebook for future reference.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_true)\n\n\n40\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_cent)\n\n\n499\n\n\n\n\nCode\nnp.linalg.norm(Y_cent, ord = 'nuc')\n\n\n6916.216191017533\n\n\n\n\nCode\nnp.linalg.norm(Y_std, ord = 'nuc')\n\n\n13488.398215158779\n\n\n\n\nCode\nnp.linalg.norm(Y_true, ord = 'nuc')\n\n\n562.4856656633167\n\n\n\n\nFunctions for NNMWF\n\n\nCode\nfrom sklearn.utils.extmath import randomized_svd\n\ndef nuclear_norm(X):\n    '''\n    Nuclear norm of input matrix\n    '''\n    return np.sum(np.linalg.svd(X)[1])\n\ndef f_objective(X, Y, W = None, mask = None):\n    '''\n    Objective function\n    Y is observed, X is estimated\n    W is the weight of each observation.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    # The * operator can be used as a shorthand for np.multiply on ndarrays.\n    if Wmask is None:\n        f_obj = 0.5 * np.linalg.norm(Y - Xmask, 'fro')**2\n    else:\n        f_obj = 0.5 * np.linalg.norm(Wmask * (Y - Xmask), 'fro')**2\n    return f_obj\n\n\ndef f_gradient(X, Y, W = None, mask = None):\n    '''\n    Gradient of the objective function.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    if Wmask is None:\n        f_grad = Xmask - Y\n    else:\n        f_grad = np.square(Wmask) * (Xmask - Y)\n    \n    return f_grad\n\n\ndef linopt_oracle(grad, r = 1.0, max_iter = 10, method = 'power'):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a nuclear norm ball for some r\n    '''\n    if method == 'power':\n        U1, V1_T = singular_vectors_power_method(grad, max_iter = max_iter)\n    elif method == 'randomized':\n        U1, V1_T = singular_vectors_randomized_method(grad, max_iter = max_iter)\n    S = - r * U1 @ V1_T\n    return S\n\n\ndef singular_vectors_randomized_method(X, max_iter = 10):\n    u, s, vh = randomized_svd(X, n_components = 1, n_iter = max_iter,\n                              power_iteration_normalizer = 'none',\n                              random_state = 0)\n    return u, vh\n\n\ndef singular_vectors_power_method(X, max_iter = 10):\n    '''\n    Power method.\n        \n        Computes approximate top left and right singular vector.\n        \n    Parameters:\n    -----------\n        X : array {m, n},\n            input matrix\n        max_iter : integer, optional\n            number of steps\n            \n    Returns:\n    --------\n        u, v : (n, 1), (p, 1)\n            two arrays representing approximate top left and right\n            singular vectors.\n    '''\n    n, p = X.shape\n    u = np.random.normal(0, 1, n)\n    u /= np.linalg.norm(u)\n    v = X.T.dot(u)\n    v /= np.linalg.norm(v)\n    for _ in range(max_iter):      \n        u = X.dot(v)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)       \n    return u.reshape(-1, 1), v.reshape(1, -1)\n\n\ndef do_step_size(dg, D, W = None, old_step = None):\n    if W is None:\n        denom = np.linalg.norm(D, 'fro')**2\n    else:\n        denom = np.linalg.norm(W * D, 'fro')**2\n    step_size = dg / denom\n    step_size = min(step_size, 1.0)\n    if step_size &lt; 0:\n        print (\"Warning: Step Size is less than 0\")\n        if old_step is not None and old_step &gt; 0:\n            print (\"Using previous step size\")\n            step_size = old_step\n        else:\n            step_size = 1.0\n    return step_size\n\n\ndef frank_wolfe_minimize_step(X, Y, r, istep, W = None, mask = None, old_step = None, svd_iter = None, svd_method = 'power'):\n    \n    # 1. Gradient for X_(t-1)\n    G = f_gradient(X, Y, W = W, mask = mask)\n    # 2. Linear optimization subproblem\n    if svd_iter is None: \n        svd_iter = 10 + int(istep / 20)\n        svd_iter = min(svd_iter, 25)\n    S = linopt_oracle(G, r, max_iter = svd_iter, method = svd_method)\n    # 3. Define D\n    D = X - S\n    # 4. Duality gap\n    dg = np.trace(D.T @ G)\n    # 5. Step size\n    step = do_step_size(dg, D, W = W, old_step = old_step)\n    # 6. Update\n    Xnew = X - step * D\n    return Xnew, G, dg, step\n\n\ndef frank_wolfe_minimize(Y, r, X0 = None,\n                         weight = None,\n                         mask = None,\n                         max_iter = 1000,\n                         svd_iter = None,\n                         svd_method = 'power',\n                         tol = 1e-4, step_tol = 1e-3, rel_tol = 1e-8,\n                         return_all = True,\n                         debug = False, debug_step = 10):\n    \n    # Step 0\n    old_X = np.zeros_like(Y) if X0 is None else X0.copy()\n    dg = np.inf\n    step = 1.0\n\n    if return_all:\n        dg_list = [dg]\n        fx_list = [f_objective(old_X, Y, W = weight, mask = mask)]\n        st_list = [1]\n        \n    # Steps 1, ..., max_iter\n    for istep in range(max_iter):\n        X, G, dg, step = \\\n            frank_wolfe_minimize_step(old_X, Y, r, istep, W = weight, mask = mask, old_step = step, svd_iter = svd_iter, svd_method = svd_method)\n        f_obj = f_objective(X, Y, W = weight, mask = mask)\n        fx_list.append(f_obj)\n\n        if return_all:\n            dg_list.append(dg)\n            st_list.append(step)\n        \n        if debug:\n            if (istep % debug_step == 0):\n                print (f\"Iteration {istep}. Step size {step:.3f}. Duality Gap {dg:g}\")\n                \n        # Stopping criteria\n        # duality gap\n        if np.abs(dg) &lt;= tol:\n            break\n        # step size\n        if step &gt; 0 and step &lt;= step_tol:\n            break\n        # relative tolerance of objective function\n        f_rel = np.abs((f_obj - fx_list[-2]) / f_obj)\n        if f_rel &lt;= rel_tol:\n            break\n            \n        old_X = X.copy()\n        \n    if return_all:\n        return X, dg_list, fx_list, st_list\n    else:\n        return X\n\n\n\n\nClass for NNMWF\n\n\nCode\nfrom sklearn.utils.extmath import randomized_svd\n\nclass TopCompSVD():\n    \n    def __init__(self, method = 'power', max_iter = 10):\n        self._method = method\n        self._max_iter = max_iter\n        return\n        \n    def fit(self, X):\n        if self._method == 'power':\n            self._u1, self._v1h = self.fit_power(X)\n        elif self._method == 'randomized':\n            self._u1, self._v1h = self.fit_randomized(X)\n        return\n    \n    def fit_power(self, X):\n        n, p = X.shape\n        u = np.random.normal(0, 1, n)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)\n        for _ in range(self._max_iter):      \n            u = X.dot(v)\n            u /= np.linalg.norm(u)\n            v = X.T.dot(u)\n            v /= np.linalg.norm(v)       \n        return u.reshape(-1, 1), v.reshape(1, -1)\n    \n    def fit_randomized(self, X):\n        u, s, vh = sp.randomized_svd(X, \n                        n_components = 1, n_iter = self._max_iter,\n                        power_iteration_normalizer = 'none',\n                        random_state = 0)\n        return u, vh\n    \n    @property\n    def u1(self):\n        return self._u1\n    \n    @property\n    def v1_t(self):\n        return self._v1h\n        \n\nclass NNMFW():\n    \n    def __init__(self, max_iter = 1000,\n            svd_method = 'power', svd_max_iter = None,\n            stop_criteria = ['duality_gap', 'step_size', 'relative_objective'],\n            tol = 1e-3, step_tol = 1e-3, rel_tol = 1e-8, \n            show_progress = False, print_skip = None,\n            debug = True):\n        self._max_iter = max_iter\n        self._svd_method = svd_method\n        self._svd_max_iter = svd_max_iter\n        self._stop_criteria = stop_criteria\n        self._tol = tol\n        self._step_size_tol = step_tol\n        self._fxrel_tol = rel_tol\n        self._show_progress = show_progress\n        self._prog_step_skip = print_skip\n        if self._show_progress and self._prog_step_skip is None:\n            self._prog_step_skip = max(1, int(self._max_iter / 100)) * 10\n        self._debug = debug\n        return\n\n    \n    def f_objective(self, X):\n        '''\n        Objective function\n        Y is observed, X is estimated\n        W is the weight of each observation.\n        '''\n        Xmask = self.get_masked(X)    \n        # The * operator can be used as a shorthand for np.multiply on ndarrays.\n        if self._weight_mask is None:\n            fx = 0.5 * np.linalg.norm(self._Y - Xmask, 'fro')**2\n        else:\n            fx = 0.5 * np.linalg.norm(self._weight_mask * (self._Y - Xmask), 'fro')**2\n        return fx\n\n\n    def f_gradient(self, X):\n        '''\n        Gradient of the objective function.\n        '''\n        Xmask = self.get_masked(X)       \n        if self._weight_mask is None:\n            gx = Xmask - self._Y\n        else:\n            gx = np.square(self._weight_mask) * (Xmask - self._Y)    \n        return gx\n    \n    \n    def fw_step_size(self, dg, D):\n        if self._weight_mask is None:\n            denom = np.linalg.norm(D, 'fro')**2\n        else:\n            denom = np.linalg.norm(self._weight_mask * D, 'fro')**2\n        ss = dg / denom\n        ss = min(ss, 1.0)\n        if ss &lt; 0:\n            print(\"Step Size is less than 0. Using last valid step size.\")\n            ss = self._st_list[-1]\n        return ss\n\n    \n    def get_masked(self, X):\n        if self._mask is None or X is None:\n            return X\n        else:\n            return X * self._mask\n        \n        \n    def linopt_oracle(self, X):\n        '''\n        Linear optimization oracle,\n        where the feasible region is a nuclear norm ball for some r\n        '''\n        U1, V1_T = self.get_singular_vectors(X)\n        S = - self._rank * U1 @ V1_T\n        return S\n    \n    \n    def get_singular_vectors(self, X):\n        max_iter = self._svd_max_iter\n        if max_iter is None:\n            nstep = len(self._st_list) + 1\n            max_iter = 10 + int(nstep / 20)\n            max_iter = min(max_iter, 25)\n        svd = TopCompSVD(method = self._svd_method, max_iter = max_iter)\n        svd.fit(X)\n        return svd.u1, svd.v1_t\n\n        \n    def fit(self, Y, r, weight = None, mask = None, X0 = None):\n        \n        '''\n        Wrapper function for the minimization\n        '''\n        \n        n, p = Y.shape\n        \n        # Make some variables available for the class\n        self._weight = weight\n        self._mask = mask\n        self._weight_mask = self.get_masked(self._weight)\n        self._Y = Y\n        self._rank = r\n        \n        # Step 0\n        X = np.zeros_like(Y) if X0 is None else X0.copy()\n        dg = np.inf\n        step = 1.0\n        fx = self.f_objective(X)\n        \n        # Save relevant variables in list\n        self._dg_list = [dg]\n        self._fx_list = [fx]\n        self._st_list = [step]\n\n        # Steps 1, ..., max_iter\n        for i in range(self._max_iter):\n            \n            X, G, dg, step = self.fw_one_step(X)\n            fx = self.f_objective(X)\n            \n            self._fx_list.append(fx)\n            self._dg_list.append(dg)\n            self._st_list.append(step)\n\n            if self._show_progress:\n                if (i % self._prog_step_skip == 0):\n                    print (f\"Iteration {i}. Step size {step:.3f}. Duality Gap {dg:g}\")\n                    \n            if self.do_stop():\n                break\n                \n        self._X = X\n\n        return\n    \n    def fw_one_step(self, X):\n    \n        # 1. Gradient for X_(t-1)\n        G = self.f_gradient(X)\n        \n        # 2. Linear optimization subproblem\n        S = self.linopt_oracle(G)\n        \n        # 3. Define D\n        D = X - S\n        \n        # 4. Duality gap\n        dg = np.trace(D.T @ G)\n        \n        # 5. Step size\n        step = self.fw_step_size(dg, D)\n        \n        # 6. Update\n        Xnew = X - step * D\n        return Xnew, G, dg, step\n    \n    def do_stop(self):\n        # self._stop_criteria = ['duality_gap', 'step_size', 'relative_objective']\n        #\n        if 'duality_gap' in self._stop_criteria:\n            dg = self._dg_list[-1]\n            if np.abs(dg) &lt;= self._tol:\n                return True\n        #\n        if 'step_size' in self._stop_criteria:\n            ss = self._st_list[-1]\n            if ss &lt;= self._step_size_tol:\n                return True\n        #\n        if 'relative_objective' in self._stop_criteria:\n            fx = self._fx_list[-1]\n            fx0 = self._fx_list[-2]\n            fx_rel = np.abs((fx - fx0) / fx0)\n            if fx_rel &lt;= self._fxrel_tol:\n                return True\n        #\n        return False\n\n\n\n\nTry the functions\n\n\nCode\nX_opt, dg_list, fx_list, step_list = frank_wolfe_minimize(Y_cent, 40.0, max_iter = 1000, debug = True, debug_step = 100, step_tol = 1e-4, svd_iter=100)\n\n\nIteration 0. Step size 1.000. Duality Gap 2528.37\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nkp = len(step_list)\nfx_arr = np.array(fx_list)\nfx_rel_diff_log10 = np.log10(np.abs(np.diff(fx_arr) / fx_arr[1:]))\n\nax1.plot(np.arange(kp - 2), dg_list[2:kp])\n# ax2.plot(np.arange(kp - 1), np.log10(fx_list[1:kp]))\nax2.plot(np.arange(kp - 1), fx_rel_diff_log10)\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTry the Python Class\n\n\nCode\nnnm = NNMFW(show_progress = True, svd_max_iter = 50)\nnnm.fit(Y_cent, 40.0)\n\n\nIteration 0. Step size 1.000. Duality Gap 2528.37\nStep Size is less than 0. Using last valid step size.\n\n\n\n\nCode\nnnm._st_list\n\n\n[1.0,\n 1.0,\n 0.4059508526803153,\n 0.19713776422660786,\n 0.031601077477726426,\n 0.011408112527596072,\n 0.011435454280280217,\n 0.011435454280280217,\n 0.0003653454546090821]\n\n\n\n\nCode\nnnm._fx_list\n\n\n[68953.79597625589,\n 67225.42267046764,\n 66961.74891880063,\n 66914.56295271001,\n 66914.19909538413,\n 66914.12717521715,\n 66914.04739067073,\n 66914.16163671855,\n 66914.16150434242]\n\n\n\n\nCode\nfx_list\n\n\n[68953.79597625589,\n 67225.42267046703,\n 66961.74891743576,\n 66914.56295259335,\n 66914.22530475237,\n 66914.11898081755,\n 66914.28171754857,\n 66914.21086735667,\n 66914.20897738854,\n 66914.1850500866,\n 66914.17070820235,\n 66914.16008819439,\n 66914.13124317658,\n 66914.18277041374,\n 66914.15440484512,\n 66914.13133305841,\n 66914.12544153685,\n 66914.12542199122]\n\n\n\n\nCode\nU, S, Vt = np.linalg.svd(mpy_simulate.do_standardize(X_opt, scale = False), full_matrices=False)\nU1, S1, Vt1 = np.linalg.svd(mpy_simulate.do_standardize(nnm._X, scale = False), full_matrices=False)\n\nfig = plt.figure(figsize = (18, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, U @ np.diag(S), vmax = 0.005)\nmpy_plotfn.plot_covariance_heatmap(ax2, U1 @ np.diag(S1), vmax = 0.005)\n\nplt.tight_layout(w_pad=2.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\n\n\n\n\nCode\ndef psnr(original, recovered):\n    n, p = original.shape\n    maxsig2 = np.square(np.max(original) - np.min(original))\n    mse = np.sum(np.square(recovered - original)) / (n * p)\n    res = 10 * np.log10(maxsig2 / mse)\n    return res\n\n\n\n\nCode\npsnr(Y_true_cent, X_opt)\n\n\n21.65905757205794\n\n\n\n\nCode\npsnr(Y_true_cent, nnm._X)\n\n\n21.664554727300995\n\n\n\n\nCode\npsnr(Y_true_cent, Y_cent)\n\n\n9.943220233648011\n\n\n\n\nCode\nY_true_mean = np.mean(Y_true, axis = 0)\nY_obs_mean = np.mean(Y, axis = 0)\n\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\nax1.scatter(Y_true_mean, Y_obs_mean)\nplt.show()"
  },
  {
    "objectID": "notebooks/develop/2023-07-29-frank-wolfe-nnm-sparse.html",
    "href": "notebooks/develop/2023-07-29-frank-wolfe-nnm-sparse.html",
    "title": "Python implementation of Frank-Wolfe algorithm for NNM with sparse penalty",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_true)\n\n\n40\n\n\n\n\nCode\nnp.linalg.matrix_rank(Y_cent)\n\n\n499\n\n\n\n\nCode\nnp.linalg.norm(Y_cent, ord = 'nuc')\n\n\n6932.244652345258\n\n\n\n\nCode\nnp.linalg.norm(Y_std, ord = 'nuc')\n\n\n13368.766978646658\n\n\n\n\nCode\nnp.linalg.norm(Y_true, ord = 'nuc')\n\n\n584.4221920551713\n\n\n\n\nCode\nidx = np.unravel_index(np.argmax(Y_true), Y_true.shape)\n\n\n\n\nCode\nnp.max(Y_true)\n\n\n0.8948086870272697\n\n\n\n\nCode\nY_true[idx]\n\n\n0.8948086870272697\n\n\n\n\nCode\nfrom sklearn.utils.extmath import randomized_svd\n\ndef proj_simplex_sort(y, a = 1.0):\n    if np.sum(y) == a and np.alltrue(y &gt;= 0):\n        return y\n    u = np.sort(y)[::-1]\n    ukvals = (np.cumsum(u) - a) / np.arange(1, y.shape[0] + 1)\n    K = np.nonzero(ukvals &lt; u)[0][-1]\n    tau = ukvals[K]\n    x = np.clip(y - tau, a_min=0, a_max=None)\n    return x\n\ndef proj_l1ball_sort(y, a = 1.0):\n    if y.ndim == 2:\n        n, p = y.shape\n        yflat = y.flatten()\n        yproj = np.sign(y) * proj_simplex_sort(np.abs(yflat), a = a).reshape(n, p)\n    else:\n        yproj = np.sign(y) * proj_simplex_sort(np.abs(yflat), a = a).reshape(n, p)\n    return yproj\n\ndef l1_norm(x):\n    return np.sum(np.abs(x))\n\ndef nuclear_norm(X):\n    '''\n    Nuclear norm of input matrix\n    '''\n    return np.sum(np.linalg.svd(X)[1])\n\ndef f_objective(X, Y, W = None, mask = None):\n    '''\n    Objective function\n    Y is observed, X is estimated\n    W is the weight of each observation.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    # The * operator can be used as a shorthand for np.multiply on ndarrays.\n    if Wmask is None:\n        f_obj = 0.5 * np.linalg.norm(Y - Xmask, 'fro')**2\n    else:\n        f_obj = 0.5 * np.linalg.norm(Wmask * (Y - Xmask), 'fro')**2\n    return f_obj\n\n\ndef f_gradient(X, Y, W = None, mask = None):\n    '''\n    Gradient of the objective function.\n    '''\n    Xmask = X if mask is None else X * mask\n    Wmask = W if mask is None else W * mask\n    \n    if Wmask is None:\n        f_grad = Xmask - Y\n    else:\n        f_grad = np.square(Wmask) * (Xmask - Y)\n    \n    return f_grad\n\ndef linopt_oracle_l1norm(grad, r = 1.0, max_iter = 10):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a l1 norm ball for some r\n    '''\n    maxidx = np.unravel_index(np.argmax(grad), grad.shape)\n    S = np.zeros_like(grad)\n    S[maxidx] = - r\n    return S\n\n\ndef linopt_oracle_nucnorm(grad, r = 1.0, max_iter = 10, method = 'power'):\n    '''\n    Linear optimization oracle,\n    where the feasible region is a nuclear norm ball for some r\n    '''\n    if method == 'power':\n        U1, V1_T = singular_vectors_power_method(grad, max_iter = max_iter)\n    elif method == 'randomized':\n        U1, V1_T = singular_vectors_randomized_method(grad, max_iter = max_iter)\n    S = - r * U1 @ V1_T\n    return S\n\n\ndef singular_vectors_randomized_method(X, max_iter = 10):\n    u, s, vh = randomized_svd(X, n_components = 1, n_iter = max_iter,\n                              power_iteration_normalizer = 'none',\n                              random_state = 0)\n    return u, vh\n\n\ndef singular_vectors_power_method(X, max_iter = 10):\n    '''\n    Power method.\n        \n        Computes approximate top left and right singular vector.\n        \n    Parameters:\n    -----------\n        X : array {m, n},\n            input matrix\n        max_iter : integer, optional\n            number of steps\n            \n    Returns:\n    --------\n        u, v : (n, 1), (p, 1)\n            two arrays representing approximate top left and right\n            singular vectors.\n    '''\n    n, p = X.shape\n    u = np.random.normal(0, 1, n)\n    u /= np.linalg.norm(u)\n    v = X.T.dot(u)\n    v /= np.linalg.norm(v)\n    for _ in range(max_iter):      \n        u = X.dot(v)\n        u /= np.linalg.norm(u)\n        v = X.T.dot(u)\n        v /= np.linalg.norm(v)       \n    return u.reshape(-1, 1), v.reshape(1, -1)\n\n\ndef do_step_size(dg, D, W = None, old_step = None):\n    if W is None:\n        denom = np.linalg.norm(D, 'fro')**2\n    else:\n        denom = np.linalg.norm(W * D, 'fro')**2\n    step_size = dg / denom\n    step_size = min(step_size, 1.0)\n    if step_size &lt; 0:\n        print (\"Warning: Step Size is less than 0\")\n        if old_step is not None and old_step &gt; 0:\n            print (\"Using previous step size\")\n            step_size = old_step\n        else:\n            step_size = 1.0\n    return step_size\n\n\ndef frank_wolfe_minimize_step(L, M, Y, rl, rm, istep, W = None, mask = None, old_step = None, svd_iter = None, svd_method = 'power'):\n    #\n    # 1. Gradient for X_(t-1)\n    #\n    G = f_gradient(L + M, Y, W = W, mask = mask)\n    #\n    # 2. Linear optimization subproblem\n    #\n    if svd_iter is None: \n        svd_iter = 10 + int(istep / 20)\n        svd_iter = min(svd_iter, 25)\n    SL = linopt_oracle_nucnorm(G, rl, max_iter = svd_iter, method = svd_method)\n    SM = linopt_oracle_l1norm(G, rm)\n    #\n    # 3. Define D\n    #\n    DL = L - SL\n    DM = M - SM\n    #\n    # 4. Duality gap\n    #\n    dg = np.trace(DL.T @ G) + np.trace(DM.T @ G)\n    #\n    # 5. Step size\n    #\n    step = do_step_size(dg, DL + DM, W = W, old_step = old_step)\n    #\n    # 6. Update\n    #\n    Lnew = L - step * DL\n    Mnew = M - step * DM\n    #\n    # 7. l1 projection\n    #\n    G_half = f_gradient(Lnew + Mnew, Y, W = W, mask = mask)\n    Mnew = proj_l1ball_sort(Mnew - G_half, rm)\n    return Lnew, Mnew, G, dg, step\n\n\ndef frank_wolfe_minimize(Y, r, X0 = None,\n                         weight = None,\n                         mask = None,\n                         max_iter = 1000,\n                         svd_iter = None,\n                         svd_method = 'power',\n                         tol = 1e-4, step_tol = 1e-3, rel_tol = 1e-8,\n                         return_all = True,\n                         debug = False, debug_step = 10):\n    \n    rl, rm = r\n    \n    # Step 0\n    old_L = np.zeros_like(Y) if X0 is None else X0.copy()\n    old_M = np.zeros_like(Y)\n    dg = np.inf\n    step = 1.0\n\n    if return_all:\n        dg_list = [dg]\n        fx_list = [f_objective(old_L + old_M, Y, W = weight, mask = mask)]\n        fl_list = [f_objective(old_L, Y, W = weight, mask = mask)]\n        fm_list = [f_objective(old_M, Y, W = weight, mask = mask)]\n        st_list = [1]\n        \n    # Steps 1, ..., max_iter\n    for istep in range(max_iter):\n        L, M, G, dg, step = \\\n            frank_wolfe_minimize_step(old_L, old_M, Y, rl, rm, istep, W = weight, mask = mask, old_step = step, svd_iter = svd_iter, svd_method = svd_method)\n        f_obj = f_objective(L + M, Y, W = weight, mask = mask)\n        fl_obj = f_objective(L, Y, W = weight, mask = mask)\n        fm_obj = f_objective(M, Y, W = weight, mask = mask)\n        fx_list.append(f_obj)\n        fl_list.append(fl_obj)\n        fm_list.append(fm_obj)\n\n        if return_all:\n            dg_list.append(dg)\n            st_list.append(step)\n        \n        if debug:\n            if (istep % debug_step == 0):\n                print (f\"Iteration {istep}. Step size {step:.3f}. Duality Gap {dg:g}\")\n                \n        # Stopping criteria\n        # duality gap\n        if np.abs(dg) &lt;= tol:\n            break\n        # step size\n        if step &gt; 0 and step &lt;= step_tol:\n            break\n        # relative tolerance of objective function\n        f_rel = np.abs((f_obj - fx_list[-2]) / f_obj)\n        if f_rel &lt;= rel_tol:\n            break\n            \n        old_L = L.copy()\n        old_M = M.copy()\n        \n    if return_all:\n        return L, M, dg_list, fx_list, st_list, fl_list, fm_list\n    else:\n        return L, M\n\n\n\n\nCode\nL_opt, M_opt, dg_list, fx_list, step_list, fl_list, fm_list = \\\n    frank_wolfe_minimize(\n        Y_cent, (40.0, 10.0), max_iter = 1000, debug = True, debug_step = 100, step_tol = 1e-4, svd_iter=20)\n\n\nIteration 0. Step size 1.000. Duality Gap 3001.78\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\nWarning: Step Size is less than 0\nUsing previous step size\n\n\n\n\nCode\nl1_norm(M_opt)\n\n\n10.000000000000005\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\ndef relative_diff_log10(xlist):\n    x_arr = np.array(xlist)\n    x_ = np.log10(np.abs(np.diff(x_arr) / x_arr[1:]))\n    return x_\n\nkp = len(step_list)\n\nax1.plot(np.arange(kp - 2), dg_list[2:kp])\nax1.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\n\n# ax2.plot(np.arange(kp - 1), np.log10(fx_list[1:kp]))\nax2.plot(np.arange(kp - 1), relative_diff_log10(fx_list), label = \"L + M\")\nax2.plot(np.arange(kp - 1), relative_diff_log10(fl_list), label = \"L\")\nax2.plot(np.arange(kp - 1), relative_diff_log10(fm_list), label = \"M\")\nax2.set_xlabel(\"Number of iterations\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nax2.legend()\n\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nU1, S1, Vt1 = np.linalg.svd(mpy_simulate.do_standardize(L_opt, scale = False), full_matrices=False)\nU2, S2, Vt2 = np.linalg.svd(mpy_simulate.do_standardize(M_opt, scale = False), full_matrices=False)\n\nfig = plt.figure(figsize = (18, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, U1 @ np.diag(S1), vmax = 0.01)\nmpy_plotfn.plot_covariance_heatmap(ax2, U2 @ np.diag(S2), vmax = 0.0001)\n\nplt.tight_layout(w_pad=2.0)\nplt.show()"
  },
  {
    "objectID": "notebooks/develop/weighted-nnm.html",
    "href": "notebooks/develop/weighted-nnm.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\nfrom matplotlib.gridspec import GridSpec\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\ndata_dir = \"../data\"\nzscore_df_filename = f\"{data_dir}/ukbb_zscore_df2.pkl\"\nzscore_df = pd.read_pickle(zscore_df_filename)\nbeta_df_filename = f\"{data_dir}/ukbb_beta_df2.pkl\"\nbeta_df = pd.read_pickle(beta_df_filename)\nse_df_filename = f\"{data_dir}/ukbb_se_df2.pkl\"\nse_df = pd.read_pickle(se_df_filename)\n\nphenotype_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/npd_phenotypes_broad_categories.tsv\"\nphenotype_df = pd.read_csv(phenotype_metafile, sep=\"\\t\")\n\nn_signif_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/npd_n_signif.tsv\"\nn_signif_df = pd.read_csv(n_signif_metafile, sep=\"\\t\", header = None, names = ['phenotype', 'n_signif'])\n\n\n\n\nCode\nzscore_df = zscore_df.loc[:, n_signif_df.loc[n_signif_df['n_signif'] &gt;= 4, 'phenotype']]\nbeta_df   = beta_df.loc[:, n_signif_df.loc[n_signif_df['n_signif'] &gt;= 4, 'phenotype']]\nse_df     = se_df.loc[:, n_signif_df.loc[n_signif_df['n_signif'] &gt;= 4, 'phenotype']]\n\n\n\n\nCode\nphenotype_ids = list(zscore_df.columns)\nphenotype_names = [phenotype_df.loc[phenotype_df['Phenotype Code'] == x, 'Phenotype Name'].item() for x in phenotype_ids]\nphenotype_categories = [phenotype_df.loc[phenotype_df['Phenotype Code'] == x, 'Phenotype Class'].item() for x in phenotype_ids]\nunique_categories = list(set(phenotype_categories))\n\ntrait_indices = [np.array([i for i, x in enumerate(phenotype_categories) if x == catg]) for catg in unique_categories]\ntrait_colors  = {trait: color for trait, color in zip(unique_categories, (mpl_stylesheet.kelly_colors()))}\n\n\n\n\nCode\nX_nan = np.array(beta_df).T\nX_nan_cent = X_nan - np.nanmean(X_nan, axis = 0, keepdims = True)\nX_nan_mask = np.isnan(X_nan)\nX_cent = np.nan_to_num(X_nan_cent, copy = True, nan = 0.0)\n\nX_weights = 1/np.square(np.array(se_df)).T\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(X_nan_mask) / np.prod(X_cent.shape):.3f}\")\n\n\nWe have 81 samples (phenotypes) and 3387 features (variants)\nFraction of Nan entries: 0.000\n\n\n\n\nCode\nnnm_weighted = FrankWolfe(model = 'nnm', max_iter = 1000, svd_max_iter = 50, \n                        tol = 1e-8, step_tol = 1e-8, simplex_method = 'sort',\n                        show_progress = True, debug = True, print_skip = 100)\nnnm_weighted.fit(X_cent, 1024.0, weight = X_weights)\n\n\n2023-11-27 14:52:57,121 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 0.000. Duality Gap 2.15562e+19\n2023-11-27 14:53:00,854 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.000. Duality Gap 1.55525e+17\n2023-11-27 14:53:04,587 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.000. Duality Gap 4.3933e+16\n2023-11-27 14:53:08,351 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.000. Duality Gap 6.48602e+16\n2023-11-27 14:53:12,071 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 400. Step size 0.000. Duality Gap 3.2847e+16\n2023-11-27 14:53:15,751 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 500. Step size 0.000. Duality Gap 1.73102e+16\n2023-11-27 14:53:19,494 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 600. Step size 0.000. Duality Gap 1.35282e+16\n2023-11-27 14:53:23,159 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 700. Step size 0.000. Duality Gap 2.17447e+16\n2023-11-27 14:53:26,839 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 800. Step size 0.000. Duality Gap 9.20949e+15\n2023-11-27 14:53:30,606 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 900. Step size 0.000. Duality Gap 1.44776e+16\n\n\n\n\nCode\nwith open (f\"{data_dir}/ukbb_npd_lowrank_X_nnm_weighted.pkl\", 'wb') as handle:\n    pickle.dump(nnm_weighted.X_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n#with open (f\"{data_dir}/ukbb_npd_lowrank_E_nnm_weighted.pkl\", 'wb') as handle:\n#    pickle.dump(nnm_weighted.M_, handle, protocol=pickle.HIGHEST_PROTOCOL)"
  },
  {
    "objectID": "notebooks/index.html",
    "href": "notebooks/index.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "This is a collection of all Jupyter notebooks for this project. It may be easier to navigate through the subcategories of the navigation.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n \n\n\nGenerate a masked input\n\n\n\n\n23-12-29\n\n\nComparison of different methods using numerical experiments\n\n\n\n\n23-12-14\n\n\nComparing two different simulation strategies - Direct Model vs Genetics Model\n\n\n\n\n23-12-12\n\n\nPan-UKB Hidden Factors v01\n\n\n\n\n23-12-11\n\n\nPan-UKB Pleitropy of Diseases v01\n\n\n\n\n23-12-08\n\n\nPan-UKB Principal Components v01\n\n\n\n\n23-11-27\n\n\nLD filtering of UKBB data\n\n\n\n\n23-10-30\n\n\nPreprocessing UKBB data\n\n\n\n\n23-10-30\n\n\nFirst look at NPD phenotypes in the UKBB data\n\n\n\n\n23-10-23\n\n\nStructure plot from GWAS phenotypes\n\n\n\n\n23-09-25\n\n\nApplication of denoising methods on GWAS phenotypes\n\n\n\n\n23-08-09\n\n\nWhich noise model is the best to capture the distinct GWAS phenotypes?\n\n\n\n\n23-08-04\n\n\nDemonstration of the Frank-Wolfe methods\n\n\n\n\n23-08-02\n\n\nComparison of different noise models\n\n\n\n\n23-07-29\n\n\nPython implementation of Frank-Wolfe algorithm for NNM with sparse penalty\n\n\n\n\n23-07-28\n\n\nChoosing step size for Inexact ALM algorithm\n\n\n\n\n23-07-24\n\n\nPython Class for cross-validation of NNM-FW\n\n\n\n\n23-07-19\n\n\nPython Class for NNM using Frank-Wolfe algorithm\n\n\n\n\n23-07-05\n\n\nSimulation setup for benchmarking matrix factorization methods\n\n\n\n\n23-07-01\n\n\nHow to simulate ground truth for multi-phenotype z-scores?\n\n\n\n\n23-06-23\n\n\nMetrices for evaluating clusters given true labels\n\n\n\n\n23-06-05\n\n\nCan the low rank approximation capture the distinct GWAS phenotypes?\n\n\n\n\n23-05-23\n\n\nNuclear norm regularization using Frank-Wolfe algorithm\n\n\n\n\n23-05-16\n\n\nPCA of NPD summary statistics\n\n\n\n\n23-05-16\n\n\nRobust PCA implementation\n\n\n\n\n23-05-12\n\n\nPreprocess NPD summary statistics\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Network effects of neuropsychiatric disorders",
    "section": "",
    "text": "The project aims to use convex optimization methods to understand the network effects of neuropsychiatric disorders.\nMy daily researach journal at the NYGC is also available online. To learn more about my work and research interests, please visit my main portfolio website."
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Convex optimization for matrix factorization, Overleaf document (restricted access).\nLiterature review of GWAS on subphenotypes, curated by Shane Crinion and Niamh Mullins."
  },
  {
    "objectID": "resources/index.html#documents",
    "href": "resources/index.html#documents",
    "title": "Resources",
    "section": "",
    "text": "Convex optimization for matrix factorization, Overleaf document (restricted access).\nLiterature review of GWAS on subphenotypes, curated by Shane Crinion and Niamh Mullins."
  },
  {
    "objectID": "resources/index.html#github",
    "href": "resources/index.html#github",
    "title": "Resources",
    "section": "Github",
    "text": "Github\n\nSoftware for low rank matrix approximation by Saikat Banerjee\nDSC for numerical experiments by Saikat Banerjee\nNPD phenotypes from UK Biobank by Saikat Banerjee\nPilot NPD Sumstat Project by David Knowles."
  },
  {
    "objectID": "resources/index.html#online-articles",
    "href": "resources/index.html#online-articles",
    "title": "Resources",
    "section": "Online Articles",
    "text": "Online Articles\n\nTips for formatting GWAS summary statistics.\nMatti Pirinen’s course on GWAS."
  },
  {
    "objectID": "notebooks/develop/index.html",
    "href": "notebooks/develop/index.html",
    "title": "Develop",
    "section": "",
    "text": "Develop ideas into software.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n23-08-04\n\n\nDemonstration of the Frank-Wolfe methods\n\n\n\n\n23-07-29\n\n\nPython implementation of Frank-Wolfe algorithm for NNM with sparse penalty\n\n\n\n\n23-07-24\n\n\nPython Class for cross-validation of NNM-FW\n\n\n\n\n23-07-19\n\n\nPython Class for NNM using Frank-Wolfe algorithm\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks",
      "Subcategory",
      "Develop"
    ]
  },
  {
    "objectID": "notebooks/develop/2023-08-03-demonstration-frankwolfe-methods-simulation.html",
    "href": "notebooks/develop/2023-08-03-demonstration-frankwolfe-methods-simulation.html",
    "title": "Demonstration of the Frank-Wolfe methods",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import FrankWolfe\n\n\n\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\n\n\n\n\nCode\nunique_labels  = list(range(len(sample_indices)))\nclass_labels = [None for x in range(ngwas)]\nfor k, idxs in enumerate(sample_indices):\n    for i in idxs:\n        class_labels[i] = k\n\n\n\n\nCode\nnnm = FrankWolfe(show_progress = True, debug = True, benchmark = True, svd_max_iter = 50, model = 'nnm')\nnnm.fit(Y_cent, 64.0, Ytrue = Y_cent)\n\n\n2023-08-04 11:54:05,899 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 4832.89\n\n\n\n\nCode\nnnm_sparse = FrankWolfe(model = 'nnm-sparse', simplex_method = 'sort', \n                        svd_max_iter = 50, tol = 1e-3, step_tol = 1e-5, \n                        show_progress = True, debug = True, benchmark = True)\nnnm_sparse.fit(Y_cent, (64.0, 0.2), Ytrue = Y_cent)\n\n\n2023-08-04 11:56:43,131 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 657679\n2023-08-04 11:56:57,498 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.011. Duality Gap 16.3721\n2023-08-04 11:57:11,707 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.000. Duality Gap 1.55259\n2023-08-04 11:57:12,886 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-04 11:57:22,063 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-04 11:57:22,766 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-08-04 11:57:25,841 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.002. Duality Gap 6.5173\n\n\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(np.arange(1, len(nnm.steps)), nnm.cpu_time_[1:], 'o-', label = 'NNM')\nax1.plot(np.arange(1, len(nnm_sparse.steps)), nnm_sparse.cpu_time_[1:], 'o-', label = 'NNM-Sparse')\nax1.legend()\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"CPU Time for iteration\")\n\nax2.plot(np.cumsum(nnm.cpu_time_), nnm.rmse_, 'o-', label = 'NNM')\nax2.plot(np.cumsum(nnm_sparse.cpu_time_), nnm_sparse.rmse_, 'o-', label = 'NNM-Sparse')\nax2.legend()\nax2.set_xlabel(\"CPU Time\")\nax2.set_ylabel(\"RMSE\")\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()"
  },
  {
    "objectID": "notebooks/develop/2023-07-24-nnmfw-cross-validation-python-class.html",
    "href": "notebooks/develop/2023-07-24-nnmfw-cross-validation-python-class.html",
    "title": "Python Class for cross-validation of NNM-FW",
    "section": "",
    "text": "About\nI mask random elements of the input matrix and perform matrix completion using NNMFW. The minimum error for the masked elements in the recovered matrix is used for cross-validation.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import NNMFW_CV\nfrom nnwmf.optimize import NNMFW\n\n\n\n\nSimulate\n\n\nCode\nntrait = 4 # categories / class\nngwas  = 50 # N\nnsnp   = 100 # P\nnfctr  = 40 # K\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var, sample_indices = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, std = 0.5, do_shift_mean = False)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\nY_std  = mpy_simulate.do_standardize(Y)\n\n\n\n\nNNMFW\n\n\nCode\nnnm = NNMFW(show_progress = True, svd_max_iter = 50, debug = True)\nnnm.fit(Y_cent, 40.0)\n\n\n2023-07-24 16:02:19,996 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 0.554. Duality Gap 886.279\n2023-07-24 16:02:20,308 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.007. Duality Gap 11.7604\n2023-07-24 16:02:20,597 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n2023-07-24 16:02:20,610 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.004. Duality Gap 6.56283\n2023-07-24 16:02:20,636 | nnwmf.optimize.frankwolfe                | WARNING | Step Size is less than 0. Using last valid step size.\n\n\n\n\nCode\nnnm._convergence_msg\n\n\n'Step size converged below tolerance.'\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nkp = len(nnm._st_list)\nfx_arr = np.array(nnm._fx_list)\nfx_rel_diff_log10 = np.log10(np.abs(np.diff(fx_arr) / fx_arr[1:]))\n\nax1.plot(np.arange(kp - 2), nnm._dg_list[2:kp])\n# ax2.plot(np.arange(kp - 1), np.log10(fx_list[1:kp]))\nax2.plot(np.arange(kp - 1), fx_rel_diff_log10)\nax3.plot(np.arange(kp), nnm._st_list)\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFunctions for cross-validation\n\n\nCode\ndef psnr(original, recovered):\n    n, p = original.shape\n    maxsig2 = np.square(np.max(original) - np.min(original))\n    mse = np.sum(np.square(recovered - original)) / (n * p)\n    res = 10 * np.log10(maxsig2 / mse)\n    return res\n\ndef masked_rmse(original, recovered, mask):\n    n = np.sum(mask)\n    mse = np.sum(np.square((original - recovered) * mask)) / n\n    return np.sqrt(mse)\n\ndef generate_rseq(Y):\n    r_min = 1\n    r_max = np.linalg.norm(Y, 'nuc')\n    nseq  = int(np.floor(np.log2(r_max)) + 1) + 1\n    r_seq = np.logspace(0, nseq - 1, num = nseq, base = 2.0)\n    return r_seq\n\ndef generate_mask(Y, folds = 1, test_size = 0.33):\n    n, p = Y.shape\n    O = np.ones(n * p)\n    ntest = int(test_size * n * p)\n    O[:ntest] = 0\n    np.random.shuffle(O)\n    return O.reshape(n, p) == 0\n\ndef generate_fold_labels(Y, folds = 2, test_size = None, shuffle = True):\n    n, p = Y.shape\n    fold_labels = np.ones(n * p)\n    ntest = int ((n * p) / folds) if test_size is None else int(test_size * n * p)\n    for k in range(1, folds):\n        start = k * ntest\n        end = (k + 1) * ntest\n        fold_labels[start: end] = k + 1\n    if shuffle:\n        np.random.shuffle(fold_labels)\n    return fold_labels.reshape(n, p)\n\ndef generate_masked_input(Y, mask):\n    Ymiss_nan = Y.copy()\n    Ymiss_nan[mask] = np.nan\n    Ymiss_nan_cent = Ymiss_nan - np.nanmean(Ymiss_nan, axis = 0, keepdims = True)\n    Ymiss_nan_cent[mask] = 0.0\n    return Ymiss_nan_cent\n\ndef nnmfw_cv(Y, folds = 2, r_seq = None):\n    if r_seq is None:\n        r_seq = generate_rseq(Y)\n    rmse_dict = {r: list() for r in r_seq}\n    fold_labels = generate_fold_labels(Y, folds = folds)\n    for fold in range(folds):\n        mask = fold_labels == fold + 1\n        Ymiss = generate_masked_input(Y, mask)\n        for r in r_seq:\n            nnm_cv = NNMFW(suppress_warnings = True)\n            nnm_cv.fit(Ymiss, r, mask = mask)\n            rmse = masked_rmse(Y, nnm_cv._X, mask)\n            rmse_dict[r].append(rmse)\n    return rmse_dict\n\n\n\n\nCode\nrmse_dict = nnmfw_cv(Y_cent, folds = 5)\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(4):\n    ax1.plot(np.log10(list(rmse_dict.keys())), [x[k] for x in rmse_dict.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nmean_err = {k: np.mean(v) for k,v in rmse_dict.items()}\n\n\n\n\nCode\nmin(mean_err, key=mean_err.get)\n\n\n64.0\n\n\n\n\nClass for cross-validation\n\n\nCode\nnnmcv = NNMFW_CV(chain_init = True, reverse_path = False, debug = True, kfolds = 2)\nnnmcv.fit(Y_cent)\n\n\n2023-07-24 16:02:56,005 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Cross-validation over 10 ranks.\n2023-07-24 16:02:56,007 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 1 ...\n2023-07-24 16:02:58,362 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 2 ...\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(2):\n    #ax1.plot(np.log10(list(nnmcv.training_error.keys())), [x[k] for x in nnmcv.training_error.values()], 'o-')\n    ax1.plot(np.log10(list(nnmcv.test_error.keys())), [x[k] for x in nnmcv.test_error.values()], 'o-')\n    ax1.plot(np.log10(list(rmse_dict.keys())), [x[k] for x in rmse_dict.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-06-05-broad-trait-clusters.html",
    "href": "notebooks/explore/2023-06-05-broad-trait-clusters.html",
    "title": "Can the low rank approximation capture the distinct GWAS phenotypes?",
    "section": "",
    "text": "About\nHere, I try to qualitatively compare the different dimensionality reduction methods in terms of their ability to distinguish the different traits. Suppose \\mathbf{X} is the N \\times P input matrix, with N traits and P associated variants. The dimensionality reduction methods decompose the input matrix into a sparse low rank component, \\mathbf{L} and a background \\mathbf{E}, \n\\mathbf{Y} \\sim \\mathbf{X} + \\mathbf{E}\n We perform PCA on the low rank matrix \\mathbf{X} using the SVD, \n\\mathbf{X} = \\mathbf{U}\\mathbf{S}\\mathbf{V}^{\\intercal}\n Then, the principal components are given by the columns of \\mathbf{U}\\mathbf{S}. The traits are broadly classified into NPD phenotypes. For each “broad phenotype” T and principal component k, we define the trait-wise PC score as, \nV_{tk} = \\sum_{t \\in T}(U_{tk}S_k)^2\n Note, the total variance explained by the component is \\sum_{i}(U_{ik}S_k)^2 = S_k^2.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nfrom nnwmf.functions.frankwolfe import frank_wolfe_minimize, frank_wolfe_cv_minimize\nfrom nnwmf.functions.robustpca import RobustPCA\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nData\nSummary statistics data for NPD is collected from PGC, OpenGWAS and GTEx. See previous work for data cleaning and filtering. Our input is the Z-Score matrix for N diseases and P variants.\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\nselect_ids = beta_df.columns\n\nX = np.array(zscore_df.replace(np.nan, 0)[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\nWe perform PCA (using SVD) on the raw input data (mean centered). In Figure 1, we look at the proportion of variance explained by each principal component.\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices = False)\nS2 = np.square(S)\npcomp = U @ np.diag(S)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(np.arange(S.shape[0]), np.cumsum(S2 / np.sum(S2)), 'o-')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Proportion of variance explained by the principal components of the input matrix\n\n\n\n\n\n\n\nTrait-wise PC score\nWe break down the total variance V_1 explained by the first principal component for each trait. In Figure 2, we show $V_{t1} / V_1, and note that the first component explains the variance in SZ and BD.\n\n\nCode\nfig = plt.figure(figsize = (12, 6))\nax1 = fig.add_subplot(111)\nnsample = pcomp.shape[0]\nntrait  = len(unique_labels)\n\n\n\npcidx = 0\ntot_variance  = np.square(S[pcidx])\n\ntrait_indices = [np.array([i for i, x in enumerate(labels) if x == label]) for label in unique_labels]\ntrait_pcomps  = [np.square(pcomp[idx, pcidx]) / tot_variance for idx in trait_indices]\ntrait_colors  = {trait: color for trait, color in zip(unique_labels, (mpl_stylesheet.kelly_colors())[:ntrait])}\n\ndef rand_jitter(n, d = 0.1):\n    return np.random.randn(n) * d\n\nfor ilbl, label in enumerate(unique_labels):\n    xtrait = trait_pcomps[ilbl]\n    nsample = xtrait.shape[0]\n    \n    boxcolor = trait_colors[label]\n    boxface = f'#{boxcolor[1:]}80' #https://stackoverflow.com/questions/15852122/hex-transparency-in-colors\n    medianprops = dict(linewidth=0, color = boxcolor)\n    whiskerprops = dict(linewidth=2, color = boxcolor)\n    boxprops = dict(linewidth=2, color = boxcolor, facecolor = boxface)\n    flierprops = dict(marker='o', markerfacecolor=boxface, markersize=3, markeredgecolor = boxcolor)\n    \n    ax1.boxplot(xtrait, positions = [ilbl],\n                showcaps = False, showfliers = False,\n                widths = 0.7, patch_artist = True, notch = False,\n                flierprops = flierprops, boxprops = boxprops,\n                medianprops = medianprops, whiskerprops = whiskerprops)\n    \n    ax1.scatter(ilbl + rand_jitter(nsample), xtrait, edgecolor = boxcolor, facecolor = boxface, linewidths = 1)\n\n\nax1.axhline(y = 0, ls = 'dotted', color = 'grey')\nax1.set_xticks(np.arange(len(unique_labels)))\nax1.set_xticklabels(unique_labels, rotation = 90)\nax1.set_ylabel(f\"PC{pcidx + 1:d}\")\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Trait-wise PVE by the first principal component of the input matrix\n\n\n\n\n\n\n\nCode\ndef plot_traitwise_pc_scores(ax, U, S, unique_labels, trait_colors, min_idx = 0, max_idx = 20, alpha = 0.6,\n                             use_proportion = True):\n    trait_pcomps_all = dict()\n    pcindices = np.arange(min_idx, max_idx)\n    pcomp = U @ np.diag(S)\n\n    for pcidx in pcindices:\n        tot_variance = np.square(S[pcidx])\n        if use_proportion:\n            trait_pcomps_all[pcidx] = [np.square(pcomp[idx, pcidx]) / tot_variance for idx in trait_indices]\n        else:\n            trait_pcomps_all[pcidx] = [np.square(pcomp[idx, pcidx]) for idx in trait_indices]\n    \n    comp_weights = {\n        trait: [np.sum(trait_pcomps_all[pcidx][ilbl]) for pcidx in pcindices] for ilbl, trait in enumerate(unique_labels)\n    }\n\n    bar_width = 1.0\n    bottom = np.zeros(len(pcindices))\n\n    for trait, comp_weight in comp_weights.items():\n        ax.bar(pcindices, comp_weight, bar_width, label=trait, bottom=bottom, color = trait_colors[trait], alpha = alpha)\n        bottom += comp_weight\n\n    ax.set_xticks(pcindices)\n    ax.set_xticklabels([f\"{i + 1}\" for i in pcindices])\n\n    for side, border in ax.spines.items():\n        border.set_visible(False)\n\n    ax.tick_params(bottom = True, top = False, left = False, right = False,\n                   labelbottom = True, labeltop = False, labelleft = False, labelright = False)\n    \n    return\n\n\nWe can do the same for each principal component and show the trait-wise PC-score for each component, as shown in the top panel of Figure 3. In such representation, we lose the information of the total variance by the component, but it gives an idea of the ability of the component to distinguish different phenotypes. In the bottom panel, we retain the information of the total variance explained (by avoiding the scaling to 1.0) but it is difficult to see the utility of the components with lower variance.\n\n\nCode\nfig = plt.figure(figsize = (12, 10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\nplot_traitwise_pc_scores(ax1, U, S, unique_labels, trait_colors, max_idx = 25)\nplot_traitwise_pc_scores(ax2, U, S, unique_labels, trait_colors, max_idx = 25, use_proportion = False)\n\nax2.set_xlabel(\"Principal Components\")\nax1.set_ylabel(\"Trait-wise scores for each PC (scaled)\")\nax2.set_ylabel(\"Trait-wise scores for each PC (scaled)\")\nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Trait-wise PVE by the first principal component of the input matrix\n\n\n\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\nsvd_pc1 = pcomp[:, idx1]\nsvd_pc2 = pcomp[:, idx2]\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Comparison of the first two principal components of input matrix\n\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 50, alpha = 0.7, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pcomp)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Comparison of the first 6 principal components of the input matrix\n\n\n\n\n\n\n\nExperiment\nHere, we perform the low-rank approximation using two methods: - Robust PCA - Nuclear Norm Matrix Factorization using Frank-Wolfe Algorithm\n\n\nCode\nrpca = RobustPCA(lmb=0.0095, max_iter=1000)\nL_rpca, M_rpca = rpca.fit(Xcent)\nnp.linalg.matrix_rank(L_rpca)\n\n\n24\n\n\n\n\nCode\nr_opt = 4096.\nL_cvopt, _, _ = frank_wolfe_minimize(Xcent, np.ones(Xcent.shape), r_opt)\nM_cvopt = Xcent - L_cvopt\n\n\n\n\nCode\nX_rpca = L_rpca + M_rpca\n\nL_rpca_cent = L_rpca - np.mean(L_rpca, axis = 0, keepdims = True)\nM_rpca_cent = M_rpca - np.mean(M_rpca, axis = 0, keepdims = True)\nX_rpca_cent = X_rpca - np.mean(X_rpca, axis = 0, keepdims = True)\n\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(L_rpca_cent, full_matrices = False)\n\nL_cvopt_cent = L_cvopt - np.mean(L_cvopt, axis = 0, keepdims = True)\nM_cvopt_cent = M_cvopt - np.mean(M_cvopt, axis = 0, keepdims = True)\n\nU_cvopt, S_cvopt, Vt_cvopt = np.linalg.svd(L_cvopt_cent, full_matrices = False)\n\npc_cvopt = U_cvopt @ np.diag(S_cvopt)\npc_rpca  = U_rpca  @ np.diag(S_rpca)\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 50, alpha = 0.7, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pc_cvopt)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Comparison of the first 6 principal components of the low rank approximation\n\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pc_rpca)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Comparison of the first 6 principal components of the Robust PCA low rank\n\n\n\n\n\nIn Figure 8, we compare the trait-wise PC scores obtained from the two methods. Note that, I am not showing the first component because the first component is very similar for the input data and the low-rank approximations. Neglecting the first component also allows me to look at the unscaled component-wise PC scores without losing too much information about the components with lower eigenvalues.\n\n\nCode\nfig = plt.figure(figsize = (12, 10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\n\nplot_traitwise_pc_scores(ax1, U_cvopt, S_cvopt, unique_labels, trait_colors, min_idx = 0, max_idx = 9, use_proportion = True)\n#plot_traitwise_pc_scores(ax2, U_cvopt2, S_cvopt2, unique_labels, trait_colors, max_idx = 10, use_proportion = False)\nplot_traitwise_pc_scores(ax2, U_rpca, S_rpca, unique_labels, trait_colors, min_idx = 0, max_idx = 9, use_proportion = True)\n\nax1.set_title(\"NNWMF\")\nax2.set_title(\"Robust PCA\")\n\nax2.set_xlabel(\"Principal Components\")\nax1.set_ylabel(\"Trait-wise PC score\")\nax2.set_ylabel(\"Trait-wise PC score\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Trait-wise PC scores for the low rank approximation of the input matric\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 10))\nax1 = fig.add_subplot(211)\nax2 = fig.add_subplot(212)\n\nplot_traitwise_pc_scores(ax1, U_cvopt, S_cvopt, unique_labels, trait_colors, min_idx = 1, max_idx = 9, use_proportion = False)\n#plot_traitwise_pc_scores(ax2, U_cvopt2, S_cvopt2, unique_labels, trait_colors, max_idx = 10, use_proportion = False)\nplot_traitwise_pc_scores(ax2, U_rpca, S_rpca, unique_labels, trait_colors, min_idx = 1, max_idx = 9, use_proportion = False)\n\nax1.set_title(\"NNWMF\")\nax2.set_title(\"Robust PCA\")\n\nax2.set_xlabel(\"Principal Components\")\nax1.set_ylabel(\"Trait-wise PC score\")\nax2.set_ylabel(\"Trait-wise PC score\")\n\nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Trait-wise PC scores for the low rank approximation of the input matric\n\n\n\n\n\n\n\nCode\nnp.linalg.matrix_rank(Xcent)\n\n\n68\n\n\n\n\nCode\nnp.linalg.matrix_rank(L_rpca)\n\n\n18\n\n\n\n\nCode\nnp.linalg.matrix_rank(L_cvopt)\n\n\n66"
  },
  {
    "objectID": "notebooks/explore/index.html",
    "href": "notebooks/explore/index.html",
    "title": "Explore",
    "section": "",
    "text": "This is a daily workbench, mostly fot prototyping ideas, exploring data, implementing methods, debugging stuff and everything else.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n \n\n\nGenerate a masked input\n\n\n\n\n23-12-29\n\n\nComparison of different methods using numerical experiments\n\n\n\n\n23-12-14\n\n\nComparing two different simulation strategies - Direct Model vs Genetics Model\n\n\n\n\n23-10-23\n\n\nStructure plot from GWAS phenotypes\n\n\n\n\n23-09-25\n\n\nApplication of denoising methods on GWAS phenotypes\n\n\n\n\n23-08-09\n\n\nWhich noise model is the best to capture the distinct GWAS phenotypes?\n\n\n\n\n23-08-02\n\n\nComparison of different noise models\n\n\n\n\n23-07-28\n\n\nChoosing step size for Inexact ALM algorithm\n\n\n\n\n23-07-05\n\n\nSimulation setup for benchmarking matrix factorization methods\n\n\n\n\n23-07-01\n\n\nHow to simulate ground truth for multi-phenotype z-scores?\n\n\n\n\n23-06-23\n\n\nMetrices for evaluating clusters given true labels\n\n\n\n\n23-06-05\n\n\nCan the low rank approximation capture the distinct GWAS phenotypes?\n\n\n\n\n23-05-23\n\n\nNuclear norm regularization using Frank-Wolfe algorithm\n\n\n\n\n23-05-16\n\n\nPCA of NPD summary statistics\n\n\n\n\n23-05-16\n\n\nRobust PCA implementation\n\n\n\n\n23-05-12\n\n\nPreprocess NPD summary statistics\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks",
      "Subcategory",
      "Explore"
    ]
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "",
    "text": "David proposed a convex optimization algorithm for nuclear norm minimization of weighted matrix factorization. This is a pilot implementation of the algorithm, following his implementation see here. I was not sure if the weighted matrix factorization was working. Therefore, I implemented the matrix completion to make sure that the algorithm is working to find the missing data.\nIn this proof-of-concept, we show that the NNWMF (we need a better acronym) indeed can find the missing data despite the noise in real data using an unit weight matrix.\nHere are some references I used for the Frank-Wolfe algorithm: - Martin Jaggi, “Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization”, PMLR 28(1):427-435, 2013 - Martin Jaggi and Marek Sulovská, “A Simple Algorithm for Nuclear Norm Regularized Problems”, ICML 2010 - Fabian Pedregosa, “Notes on the Frank-Wolfe Algorithm, Part I”, Personal Blog, 2018 - Moritz Hardt, Lecture Notes on “Convex Optimization and Approximation”, 2018\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.extmath import randomized_svd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')"
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html#simplex-projection",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html#simplex-projection",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "Simplex projection",
    "text": "Simplex projection\nProject the input matrix on nuclear norm ball, using an algorithm proposed by Duchi et. al. in “Efficient projections onto the l1-ball for learning in high dimensions”, Proc. 25th ICML, pages 272–279. ACM, 2008. This is computationally expensive but will help to compare the results from Frank-Wolfe algorithm.\n\n\nCode\nXproj = nuclear_projection(Xcent, r = 1000)\nXproj = Xproj - np.mean(Xproj, axis = 0, keepdims = True)\nU, S, Vt = np.linalg.svd(Xproj)\nprint(f\"Nuclear norm of projected input matrix is {np.sum(S)}\")\n\n\nNuclear norm of projected input matrix is 1000.0000000000019"
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html#frank-wolfe",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html#frank-wolfe",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "Frank-Wolfe",
    "text": "Frank-Wolfe\n\n\nCode\nX_opt, gs, fx = frank_wolfe_minimize(Xcent, weight, 1000.0, max_iter = 500, debug = False)\n\n\nIn Figure 2, we show the progress of the optimization. On the left hand side, we show the duality gap and on the right hand side, we show the objective function.\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nkp = 100\nax1.plot(np.arange(kp), gs[:kp])\nax2.plot(np.arange(kp), fx[:kp])\n\nax1.set_xlabel(\"Number of iterations\")\nax2.set_xlabel(\"Number of iterations\")\nax1.set_ylabel(r\"Duality gap, $g_t$\")\nax2.set_ylabel(r\"Objective function, $f(\\mathbf{X})$\")\nfig.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Frank-Wolfe optimization statistic\n\n\n\n\n\n\n\nCode\nX_opt_cent = X_opt - np.mean(X_opt, axis = 0, keepdims = True)\nU_fw, S_fw, Vt_fw = np.linalg.svd(X_opt_cent)\n\n\nIn Figure 3, we compare the singular values of the output signal from the nuclear norm projection and the Frank-Wolfe algorithm. This gives us confidence with the implementation.\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nkp = 22\nax1.plot(np.arange(kp), S[:kp], 'o-', label = 'Simplex Projection')\nax1.plot(np.arange(kp), S_fw[:kp], 'o-', label = 'Frank-Wolfe')\n\nax1.legend()\nax1.set_xlabel(\"Component\")\nax1.set_ylabel(\"Eigenvalue (S)\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Eigenvalues obtained using SVD of the projected input matrix"
  },
  {
    "objectID": "notebooks/explore/2023-05-23-frank-wolfe.html#compare-the-principal-components-of-the-recovered-matrix",
    "href": "notebooks/explore/2023-05-23-frank-wolfe.html#compare-the-principal-components-of-the-recovered-matrix",
    "title": "Nuclear norm regularization using Frank-Wolfe algorithm",
    "section": "Compare the principal components of the recovered matrix",
    "text": "Compare the principal components of the recovered matrix\n\n\nCode\nU_input, S_input, Vt_input = np.linalg.svd(Xcent, full_matrices=False)\npca_input = U_input @ np.diag(S_input)\n\n\n\n\nCode\nX_cvopt_cent = X_cvopt - np.mean(X_cvopt, axis = 0, keepdims = True)\nU_cvopt, S_cvopt, Vt_cvopt = np.linalg.svd(X_cvopt_cent, full_matrices = False)\npca_cvopt = U_cvopt @ np.diag(S_cvopt)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\n\nsvd_pc1 = pca_input[:, idx1]\nsvd_pc2 = pca_input[:, idx2]\nwmf_pc1 = pca_cvopt[:, idx1]\nwmf_pc2 = pca_cvopt[:, idx2]\n\nfig = plt.figure(figsize = (16, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.5, label = label)\n    ax2.scatter(wmf_pc1[idx], wmf_pc2[idx], s = 100, alpha = 0.5, label = label)\n    \nax2.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nax2.set_xlabel(f\"Component {idx1}\")\nax2.set_ylabel(f\"Component {idx2}\")\nax1.set_title(\"Raw input\", pad = 20.0)\nax2.set_title(\"Low rank approximation\", pad = 20.)\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Comparison of the first two principal components of input matrix and low rank approximation\n\n\n\n\n\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.7, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, pca_cvopt)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Comparison of the first 6 principal components of the low rank approximation\n\n\n\n\n\n\n\nCode\ntrain_error\n\n\n{8.0: 1.0445314519131719,\n 16.0: 1.041037978310444,\n 32.0: 1.0342448061985816,\n 64.0: 1.0212031961118844,\n 128.0: 0.9980512728715728,\n 256.0: 0.9592552598273123,\n 512.0: 0.8958869142145245,\n 1024.0: 0.7931029154998555,\n 2048.0: 0.6284386790184624,\n 4096.0: 0.3992465647184987,\n 5000.0: 0.36700702863471724,\n 6000.0: 0.37273215971181584,\n 7000.0: 0.39743960536609935,\n 8000.0: 0.46060258805130466}\n\n\n\n\nCode\ntest_error\n\n\n{8.0: 0.7379283863527464,\n 16.0: 0.7360553648577052,\n 32.0: 0.7323992229046496,\n 64.0: 0.7252923894633748,\n 128.0: 0.7126499267407608,\n 256.0: 0.6929325616806638,\n 512.0: 0.6650417778754745,\n 1024.0: 0.6294795062102471,\n 2048.0: 0.5984243261174017,\n 4096.0: 0.5813444546875715,\n 5000.0: 0.5978681673912616,\n 6000.0: 0.6282368213911593,\n 7000.0: 0.6693131025636734,\n 8000.0: 0.6992722149861493}"
  },
  {
    "objectID": "notebooks/explore/2023-05-16-pca-sumstat.html",
    "href": "notebooks/explore/2023-05-16-pca-sumstat.html",
    "title": "PCA of NPD summary statistics",
    "section": "",
    "text": "About\nFor multi-trait analysis of GWAS summary statistics, we look at one of the simplest models - the principal component analysis (PCA). The goal is to characterize the latent components of genetic associations. We apply PCA to the matrix of summary statistics derived from GWAS across 80 NPD phenotypes from various sources – namely, GTEx, OpenGWAS and PGC. For a similar comprehensive study with the UK Biobank data, see Tanigawa et al., Nat. Comm. 2019.\nThe \\mathbf{X} matrix of size N \\times P for the PCA is characterized by N phenotypes (samples) and P variants (features).\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nfrom sklearn.decomposition import PCA\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\n\n\n\nRead summary statistics\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\nAfter filtering, there are 100068 variants for 69 phenotypes, summarized below.\n\nbeta_df\n\n\n\n\n\n\n\n\nAD_sumstats_Jansenetal_2019sept.txt.gz\nCNCR_Insomnia_all\nGPC-NEO-NEUROTICISM\nIGAP_Alzheimer\nJones_et_al_2016_Chronotype\nJones_et_al_2016_SleepDuration\nMDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_...\nMDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUK...\nMHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered...\nMHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Onl...\n...\nieu-b-7\nieu-b-8\nieu-b-9\nocd_aug2017.txt.gz\npgc-bip2021-BDI.vcf.txt.gz\npgc-bip2021-BDII.vcf.txt.gz\npgc-bip2021-all.vcf.txt.gz\npgc.scz2\npgcAN2.2019-07.vcf.txt.gz\npts_all_freeze2_overall.txt.gz\n\n\n\n\nrs1000031\n-0.002240\n-0.003273\n0.1289\n0.0072\n-0.000698\n0.000712\n0.001170\n0.005629\n-0.016636\n-0.046173\n...\n0.0124\nNaN\nNaN\n-0.006995\n0.012896\n-0.005596\n0.012798\n-0.004802\n-0.022505\n-0.022603\n\n\nrs1000269\n-0.002631\n-0.009901\n0.0708\n-0.0286\n-0.010576\n-0.004509\n0.001181\n0.000982\n0.025008\n0.044356\n...\n0.0373\n-0.005293\n-0.008693\n0.003400\n-0.002603\n0.006598\n-0.008597\n0.008801\n0.009202\n-0.008702\n\n\nrs10003281\n-0.005380\n0.060423\n-0.4361\n-0.0037\n-0.011744\n0.022551\nNaN\nNaN\nNaN\nNaN\n...\n-0.0344\n0.111759\n0.103478\n0.097997\n0.011296\n-0.091501\n-0.006099\n0.000500\nNaN\n-0.036700\n\n\nrs10004866\n0.000037\n0.019230\n0.2035\n-0.0294\n-0.000277\n-0.008780\n-0.021749\n-0.023538\n-0.000016\n-0.034849\n...\n-0.0316\n-0.027825\n-0.011386\n-0.035900\n0.016601\n0.036197\n0.027002\n-0.027703\n-0.021801\n0.015499\n\n\nrs10005235\n0.002221\n-0.007645\n0.1255\n0.0108\n-0.016947\n0.011787\n0.003163\n0.002522\n0.016984\n0.004132\n...\n0.0159\n-0.004489\n-0.021064\n0.010604\n-0.028399\n0.068602\n-0.021499\n0.045102\n0.025605\n-0.006400\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrs9989571\n0.000085\n-0.002726\nNaN\n0.0184\n0.002450\n-0.010379\n-0.001487\n-0.009087\n0.024389\n0.023753\n...\n-0.0383\n-0.008719\n0.012676\n-0.100198\n-0.014302\n-0.054097\n-0.037401\n0.047999\n0.028502\n-0.021595\n\n\nrs9991694\n-0.004170\n-0.011018\nNaN\n-0.0150\n0.007666\n-0.013183\n0.031620\n0.034180\n-0.062177\n-0.058765\n...\n0.0021\nNaN\nNaN\n-0.185500\n-0.015103\n0.000800\n-0.015103\n0.033102\nNaN\n0.017299\n\n\nrs9992763\n0.001502\n-0.000098\nNaN\n-0.0067\n-0.000568\n0.002275\n0.000704\n0.002954\n0.013290\n0.001284\n...\n0.0033\n-0.000539\n0.012932\n-0.007899\n-0.000600\n0.015401\n0.000100\n0.004898\n-0.006896\n-0.018802\n\n\nrs9993607\n-0.003670\n-0.003879\nNaN\n0.0005\n0.000622\n-0.000486\n0.031370\n0.036475\n-0.044404\n-0.045986\n...\n-0.0161\n0.001483\n0.003172\n-0.027897\n-0.010697\n-0.011901\n-0.007700\n-0.000700\n0.003396\n-0.002597\n\n\nrs999494\n-0.000829\n0.010356\n-0.0274\n0.0274\n0.000675\n0.006377\n-0.014707\n-0.014568\n0.021090\n-0.000227\n...\n-0.0126\n0.002342\n-0.014567\n-0.060596\n0.013202\n0.019204\n0.016100\n-0.070304\n-0.019803\n0.010100\n\n\n\n\n10068 rows × 69 columns\n\n\n\n\nselect_ids = beta_df.columns\n\n\n\nPCA\nWe use the z-score as the observation statistic for each phenotype and variant. For PCA, we center the columns of the X matrix.\n\n\nCode\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n#phenotype_dict\n\n\n\n\nCode\nX = np.array(zscore_df.replace(np.nan, 0)[select_ids]).T\ncolmeans = np.mean(X, axis = 0, keepdims = True)\nXcent = X - colmeans\n\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\n\nprint (f\"We have {Xcent.shape[0]} samples (phenotypes) and {Xcent.shape[1]} features (variants)\")\n\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\n\n\n\n\nCode\nncomp = min(Xcent.shape)\npca = PCA(n_components = ncomp)\npca.fit(Xcent)\nbeta_pcs = pca.fit_transform(Xcent)\nbeta_eig  = pca.components_\n\n\nThe variance explained by each principal component is shown in Figure 1\n\n\nCode\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\n#ax1.scatter(np.arange(ncomp), pca.explained_variance_ratio_, s = 100, alpha = 0.7)\nax1.plot(np.arange(ncomp), np.cumsum(pca.explained_variance_ratio_), marker = 'o')\nax1.set_ylabel(\"Fraction of variance explained\")\nax1.set_xlabel(\"Principal Components\")\n\nax2.plot(np.arange(ncomp), pca.explained_variance_ratio_, marker = 'o')\nax2.set_ylabel(\"Variance explained by each component\")\nax2.set_xlabel(\"Principal Components\")\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Variance explained by the Principal Components\n\n\n\n\n\nWe plot the first 6 principal components against each other in Figure 2. Each dot is a sample (phenotype) colored by their broad label of NPD.\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.8, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, beta_pcs)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: First 6 principal components compared against each other.\n\n\n\n\n\n\n\nPCA using SVD\nJust to make sure that I understand things correctly, I am redoing the PCA using SVD. It must yield the same results as sklearn PCA. Indeed, it does as shown in Figure 3.\n\n\nCode\nU, S, Vt = np.linalg.svd(Xcent, full_matrices=False)\nsvd_pcs = U @ np.diag(S)\n\n\n\n\nCode\nidx1 = 0\nidx2 = 1\nsvd_pc1 = svd_pcs[:, idx1]\nsvd_pc2 = svd_pcs[:, idx2]\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor label in unique_labels:\n    idx = np.array([i for i, x in enumerate(labels) if x == label])\n    ax1.scatter(svd_pc1[idx], svd_pc2[idx], s = 100, alpha = 0.7, label = label)\n    \n# idxs = np.array([i for i, x in enumerate(labels) if x == 'Sleep'])\n# for idx in idxs:\n#     pid = select_ids[idx]\n#     ax1.annotate(pid, (svd_pc1[idx], svd_pc2[idx]))\n\n    \nax1.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\nax1.set_xlabel(f\"Component {idx1}\")\nax1.set_ylabel(f\"Component {idx2}\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Check PCA using SVD.\n\n\n\n\n\n\n\nWeighted PCA\nHere, I implement the weighted covariance eigendecomposition approach algorithm proposed by L. Delchambre (2014). I used 3 different versions of the weight matrix:\n\nthe identity matrix (to check that the implementation is correct).\nthe precision matrix (as suggested by David in the project proposal).\nsquare root of the precision (ad-hoc idea to reduce the weights on each observation)\n\n\n# W = np.ones(X.shape)\n# W = np.array(prec_df[select_ids]).T\nW = np.sqrt(np.array(prec_df.replace(np.nan, 0)[select_ids]).T)\n\n\n\nCode\ndef weighted_mean(x, w=None, axis=None):\n    \"\"\"Compute the weighted mean along the given axis\n\n    The result is equivalent to (x * w).sum(axis) / w.sum(axis),\n    but large temporary arrays are not created.\n\n    Parameters\n    ----------\n    x : array_like\n        data for which mean is computed\n    w : array_like (optional)\n        weights corresponding to each data point. If supplied, it must be the\n        same shape as x\n    axis : int or None (optional)\n        axis along which mean should be computed\n\n    Returns\n    -------\n    mean : np.ndarray\n        array representing the weighted mean along the given axis\n    \"\"\"\n    if w is None:\n        return np.mean(x, axis)\n\n    x = np.asarray(x)\n    w = np.asarray(w)\n\n    if x.shape != w.shape:\n        raise NotImplementedError(\"Broadcasting is not implemented: \"\n                                  \"x and w must be the same shape.\")\n\n    if axis is None:\n        wx_sum = np.einsum('i,i', np.ravel(x), np.ravel(w))\n    else:\n        try:\n            axis = tuple(axis)\n        except TypeError:\n            axis = (axis,)\n\n        if len(axis) != len(set(axis)):\n            raise ValueError(\"duplicate value in 'axis'\")\n\n        trans = sorted(set(range(x.ndim)).difference(axis)) + list(axis)\n        operand = \"...{0},...{0}\".format(''.join(chr(ord('i') + i)\n                                                 for i in range(len(axis))))\n        wx_sum = np.einsum(operand,\n                           np.transpose(x, trans),\n                           np.transpose(w, trans))\n\n    return wx_sum / np.sum(w, axis)\n\ndef weighted_pca_delchambre(X, W, n_components = None, regularization = None):\n    \n    import scipy as sp\n    \n    weights = weighted_mean(X, W, axis = 0).reshape(1, -1)\n    _X = (X - weights) *  weights\n    _covar = np.dot(_X.T, _X)\n    _covar /= np.dot(weights.T, weights)\n    _covar[np.isnan(_covar)] = 0\n\n    n_components = 20\n    eigvals = (_X.shape[1] - n_components, _X.shape[1] - 1)\n    evals, evecs = sp.linalg.eigh(_covar, subset_by_index = eigvals)\n\n    components = evecs[:, ::-1].T\n    explained_variance = evals[::-1]\n    Y = np.zeros((_X.shape[0], components.shape[0]))\n    for i in range(_X.shape[0]):\n        cW = components * weights[0, i]\n        cWX = np.dot(cW, _X[i])\n        cWc = np.dot(cW, cW.T)\n        if regularization is not None:\n            cWc += np.diag(regularization / explained_variance)\n        Y[i] = np.linalg.solve(cWc, cWX)\n    return Y, explained_variance\n\n\n\n\nCode\nweighted_pcs, explained_variance = weighted_pca_delchambre(Xcent, W, n_components = 20, regularization = None)\n\n\nAs before, the variance explained by each principal component is shown in Figure 4\n\n\nCode\nwpca_tot_explained_variance = np.sum(explained_variance)\nwpca_explained_variance_ratio = explained_variance / wpca_tot_explained_variance\n\nfig = plt.figure(figsize = (14, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(np.arange(20), np.cumsum(wpca_explained_variance_ratio), marker = 'o')\nax1.set_ylabel(\"Fraction of variance explained\")\nax1.set_xlabel(\"Principal Components\")\n\nax2.plot(np.arange(20), wpca_explained_variance_ratio, marker = 'o')\nax2.set_ylabel(\"Variance explained by each component\")\nax2.set_xlabel(\"Principal Components\")\n\nplt.tight_layout(w_pad = 3)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Variance explained by the weighted Principal Components\n\n\n\n\n\nWe plot the first 6 principal components against each other in Figure 5. Each dot is a sample (phenotype) colored by their broad label of NPD.\n\n\nCode\nplot_ncomp = 6\nsubplot_h = 2.0\n\nnrow = plot_ncomp - 1\nncol = plot_ncomp - 1\nfigw = ncol * subplot_h + (ncol - 1) * 0.3 + 1.2\nfigh = nrow * subplot_h + (nrow - 1) * 0.3 + 1.5\nbgcolor = '#F0F0F0'\n\n\ndef make_plot_principal_components(ax, i, j, comp):\n    pc1 = comp[:, j]\n    pc2 = comp[:, i]\n    for label in unique_labels:\n        idx = np.array([k for k, x in enumerate(labels) if x == label])\n        ax.scatter(pc1[idx], pc2[idx], s = 30, alpha = 0.8, label = label)\n    return\n\nfig = plt.figure(figsize = (figw, figh))\naxmain = fig.add_subplot(111)\n\nfor i in range(1, nrow + 1):\n    for j in range(ncol):\n        ax = fig.add_subplot(nrow, ncol, ((i - 1) * ncol) + j + 1)\n        \n        ax.tick_params(bottom = False, top = False, left = False, right = False,\n                       labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n        if j == 0: ax.set_ylabel(f\"PC{i + 1}\")\n        if i == nrow: ax.set_xlabel(f\"PC{j + 1}\")\n        if i &gt; j:\n            ax.patch.set_facecolor(bgcolor)\n            ax.patch.set_alpha(0.3)\n            make_plot_principal_components(ax, i, j, weighted_pcs)\n            for side, border in ax.spines.items():\n                border.set_color(bgcolor)\n        else:\n            ax.patch.set_alpha(0.)\n            for side, border in ax.spines.items():\n                border.set_visible(False)\n                \n        if i == 1 and j == 0:\n            mhandles, mlabels = ax.get_legend_handles_labels()\n\naxmain.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\nfor side, border in axmain.spines.items():\n    border.set_visible(False)\naxmain.legend(handles = mhandles, labels = mlabels, loc = 'upper right', bbox_to_anchor = (0.9, 0.9))\n          \n        \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: First 6 weighted principal components compared against each other.\n\n\n\n\n\n\n\nFurther Reading\n\nComments on PCA by Alex Williams"
  },
  {
    "objectID": "notebooks/explore/2024-01-29-simulation-comparisons.html",
    "href": "notebooks/explore/2024-01-29-simulation-comparisons.html",
    "title": "Comparison of different methods using numerical experiments",
    "section": "",
    "text": "Getting Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport sys\nimport os\nimport dsc\nfrom dsc.query_engine import Query_Processor as dscQP\nfrom dsc import dsc_io\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120)\n\n\n\n\nCode\ndef stratify_dfcol(df, colname, value):\n    #return pd_utils.select_dfrows(df, [f\"$({colname}) == {value}\"])\n    return df.loc[df[colname] == value]\n\ndef stratify_dfcols(df, condition_list):\n    for (colname, value) in condition_list:\n        df = stratify_dfcol(df, colname, value)\n    return df\n\ndef stratify_dfcols_in_list(df, colname, values):\n    return df.loc[df[colname].isin(values)]\n\n\n\n\nRead Simulation Results\n\n\nCode\ndsc_output = \"/gpfs/commons/groups/knowles_lab/sbanerjee/low_rank_matrix_approximation_numerical_experiments/blockdiag\"\ndsc_fname  = os.path.basename(os.path.normpath(dsc_output))\ndb = os.path.join(dsc_output, dsc_fname + \".db\")\ndscoutpkl = os.path.join(\"/gpfs/commons/home/sbanerjee/work/npd/lrma-dsc/dsc/results\", dsc_fname + \"_dscout.pkl\")\ndscout    = pd.read_pickle(dscoutpkl)\ndscout\n\n\n\n\n\n\n\n\n\nDSC\nsimulate\nsimulate.n\nsimulate.p\nsimulate.k\nsimulate.h2\nsimulate.h2_shared_frac\nsimulate.aq\nlowrankfit\nmatfactor\nscore.L_rmse\nscore.F_rmse\nscore.Z_rmse\nscore.L_psnr\nscore.F_psnr\nscore.Z_psnr\nscore.adj_MI\n\n\n\n\n0\n1\nblockdiag\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.6\nidentical\ntruncated_svd\n0.244982\n0.485097\n0.005098\n28.429593\n24.712971\n21.552452\n0.018100\n\n\n1\n2\nblockdiag\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.6\nidentical\ntruncated_svd\n0.264152\n0.483567\n0.005141\n28.688465\n24.252872\n21.767697\n0.015638\n\n\n2\n3\nblockdiag\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.6\nidentical\ntruncated_svd\n0.252526\n0.499034\n0.005091\n28.831802\n23.687408\n22.263407\n0.654857\n\n\n3\n4\nblockdiag\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.6\nidentical\ntruncated_svd\n0.285154\n0.484862\n0.005550\n28.881899\n24.282865\n23.215231\n0.561981\n\n\n4\n5\nblockdiag\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.6\nidentical\ntruncated_svd\n0.292962\n0.474069\n0.005080\n27.246965\n24.492696\n22.070442\n0.024386\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n895\n8\nblockdiag_aq\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.8\nidentical\nfactorgo\n0.597619\n0.720961\n0.002017\n20.790216\n21.536737\n30.477673\n0.010297\n\n\n896\n9\nblockdiag_aq\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.4\nidentical\nfactorgo\n0.201241\n0.400942\n0.002174\n30.378105\n26.809769\n29.938074\n-0.009727\n\n\n897\n9\nblockdiag_aq\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.8\nidentical\nfactorgo\n0.509286\n0.669955\n0.002038\n21.469111\n20.631187\n28.405981\n0.003175\n\n\n898\n10\nblockdiag_aq\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.4\nidentical\nfactorgo\n0.270847\n0.429577\n0.002189\n28.086705\n25.500348\n30.518931\n0.022548\n\n\n899\n10\nblockdiag_aq\n200.0\n2000.0\n10.0\n0.2\n0.5\n0.8\nidentical\nfactorgo\n0.626669\n0.756680\n0.002087\n19.320562\n20.277492\n30.701310\n-0.004610\n\n\n\n\n900 rows × 17 columns\n\n\n\n\n\nCompare Methods\nWe use several metrics to compare the different methods.\n\nmethods = {\n    \"rpca\" : [\"rpca\", \"truncated_svd\"],\n    \"nnm\"  : [\"nnm\", \"truncated_svd\"],\n    \"nnm_sparse\" : [\"nnm_sparse\", \"truncated_svd\"],\n    \"truncated_svd\" : [\"identical\", \"truncated_svd\"],\n    \"factorgo\" : [\"identical\", \"factorgo\"],\n}\nmethod_labels = {\n    \"rpca\" : \"RPCA-IALM\",\n    \"nnm\" : \"NN-FW\",\n    \"nnm_sparse\" : \"NN-Sparse-FW\",\n    \"truncated_svd\": \"tSVD\",\n    \"factorgo\": \"FactorGO\",\n}\n\nmethod_colors = {\n    \"rpca\" : '#FF6800', # Vivid Orange\n    \"nnm\" : '#C10020', # Vivid Red\n    \"nnm_sparse\" : '#803E75', # Strong Purple\n    \"truncated_svd\" : '#535154', # gray\n    \"factorgo\" : '#A6BDD7', # Very Light Blue\n}\n\n# Base parameters\nsimparams = {'p': 2000, 'k': 10, 'h2': 0.2, 'h2_shared_frac': 0.5, 'aq': 0.6}\nscore_names = {\n    'L_rmse': r\"$\\| L - \\hat{L}\\|_F$\",\n    'F_rmse': r\"$\\| F - \\hat{F}\\|_F$\",\n    'Z_rmse': r\"$\\| LF^{T} - \\hat{L}\\hat{F}^{T}\\|_F$\",\n    'adj_MI': \"Adjusted Mutual Information Score\",\n}\n\n# score_names = {\n#     'L_psnr': r\"$\\| L - \\hat{L}\\|_F$\",\n#     'F_psnr': r\"$\\| F - \\hat{F}\\|_F$\",\n#     'Z_psnr': r\"$\\| LF^{T} - \\hat{L}\\hat{F}^{T}\\|_F$\",\n#     'adj_MI': \"Adjusted Mutual Information Score\",\n# }\n\n\n\nCode\ndef get_simulation_with_variable(df, var_name, var_values):\n    condition = [(f'simulate.{k}', v) for k, v in simparams.items() if k != var_name]\n    df1 = stratify_dfcols(df, condition)\n    df2 = stratify_dfcols_in_list(df1, f'simulate.{var_name}', var_values)\n    return df2\n\ndef get_scores_from_dataframe(df, score_name, variable_name, variable_values, \n        methods = methods):\n    simdf = get_simulation_with_variable(df, variable_name, variable_values)\n    scores = {key: list() for key in methods.keys()}\n    for method, mlist in methods.items():\n        mrows = stratify_dfcols(simdf, [('lowrankfit', mlist[0]), ('matfactor', mlist[1])])\n        for value in variable_values:\n            vrows = stratify_dfcol(mrows, f'simulate.{variable_name}', value)\n            scores[method].append(vrows[f'score.{score_name}'].to_numpy())\n    return scores\n\n\n\n\nCode\ndef random_jitter(xvals, yvals, d = 0.1):\n    xjitter = [x + np.random.randn(len(y)) * d for x, y in zip(xvals, yvals)]\n    return xjitter\n\ndef boxplot_scores(variable, variable_values, \n        methods = methods, score_names = score_names,\n        dscout = dscout, method_colors = method_colors):\n    \n    nmethods = len(methods)\n    nvariables = len(variable_values)\n    nscores = len(score_names)\n    \n    figh = 6\n    figw = (nscores * figh) + (nscores - 1)\n    fig = plt.figure(figsize = (figw, figh))\n    axs = [fig.add_subplot(1, nscores, x+1) for x in range(nscores)]\n    boxs = {x: None for x in methods.keys()}\n    \n    for i, (score_name, score_label) in enumerate(score_names.items()):\n        scores = get_scores_from_dataframe(dscout, score_name, variable, variable_values)\n        for j, mkey in enumerate(methods.keys()):\n            boxcolor = method_colors[mkey]\n            boxface = f'#{boxcolor[1:]}80'\n            medianprops = dict(linewidth=0, color = boxcolor)\n            whiskerprops = dict(linewidth=2, color = boxcolor)\n            boxprops = dict(linewidth=2, color = boxcolor, facecolor = boxface)\n            flierprops = dict(marker='o', markerfacecolor=boxface, markersize=3, markeredgecolor = boxcolor)\n\n            xpos = [x * (nmethods + 1) + j for x in range(nvariables)]\n            boxs[mkey] = axs[i].boxplot(scores[mkey], positions = xpos,\n                showcaps = False, showfliers = False,\n                widths = 0.7, patch_artist = True, notch = False,\n                flierprops = flierprops, boxprops = boxprops,\n                medianprops = medianprops, whiskerprops = whiskerprops)\n            \n            axs[i].scatter(random_jitter(xpos, scores[mkey]), scores[mkey], \n                           edgecolor = boxcolor, facecolor = boxface, linewidths = 1, \n                           s = 10)\n\n        xcenter = [x * (nmethods + 1) + (nmethods - 1) / 2 for x in range(nvariables)]\n        axs[i].set_xticks(xcenter)\n        axs[i].set_xticklabels(variable_values)\n        axs[i].set_xlabel(variable)\n        axs[i].set_ylabel(score_label)\n        xlim_low = 0 - (nvariables - 1) / 2\n        #xlim_high = (nvariables - 1) * (nmethods + 1) + (nmethods - 1) + (nvariables - 1) / 2\n        xlim_high = (nmethods + 1.5) * nvariables - 2.5\n        axs[i].set_xlim( xlim_low, xlim_high )\n\n    plt.tight_layout()\n    return axs, boxs\n\nvariable = 'p'\nvariable_values = [500, 1000, 2000, 5000, 10000]\n\naxs, boxs = boxplot_scores(variable, variable_values)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (6, 2))\nax1 = fig.add_subplot(111)\nhandles = [boxs[mkey][\"boxes\"][0] for mkey in methods.keys()]\nlabels = [method_labels[mkey] for mkey in methods.keys()]\nax1.legend(handles = handles, labels = labels, loc = 'upper left', frameon = False, handlelength = 4, ncol = 2)\n\nfor side, border in ax1.spines.items():\n    border.set_visible(False)\nax1.tick_params(bottom = False, top = False, left = False, right = False,\n                   labelbottom = False, labeltop = False, labelleft = False, labelright = False)\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nvariable = 'k'\nvariable_values = [2,5,10,15,20]\nboxplot_scores(variable, variable_values)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nvariable = 'h2'\nvariable_values = [0.05, 0.1, 0.2, 0.3, 0.4]\nboxplot_scores(variable, variable_values)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nvariable = 'h2_shared_frac'\nvariable_values = [0.2, 0.5, 0.8, 1.0]\nboxplot_scores(variable, variable_values)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nvariable = 'aq'\nvariable_values = [0.4, 0.6, 0.8]\nboxplot_scores(variable, variable_values)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPSNR\nInstead of the Frobenius norm, we can also use the Peak Signal-to-Noise Ratio (PSNR) for comparing the methods.\n\n\nCode\npsnr_score_names = {\n    'L_psnr': r\"PSNR ($L$)\",\n    'F_psnr': r\"PSNR ($F$)\",\n    'Z_psnr': r\"PSNR ($LF^T$)\",\n}\n\nvariable = 'p'\nvariable_values = [500, 1000, 2000, 5000, 10000]\n\naxs, boxs = boxplot_scores(variable, variable_values, score_names = psnr_score_names)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/Untitled.html",
    "href": "notebooks/explore/Untitled.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\nfrom scipy import stats as spstats\n\ndef distribute_samples_to_classes(Q, n, shuffle = False):\n    ''' \n    Distribute n samples to Q classes\n    '''\n    rs = 0.6 * np.random.rand(Q) + 0.2 # random sample from [0.2, 0.8)\n    z = np.array(np.round((rs / np.sum(rs)) * n), dtype = int)\n    z[-1] = n - np.sum(z[:-1])\n    tidx = np.arange(n)\n    if shuffle:\n        np.random.shuffle(tidx)\n    bins = np.zeros(Q + 1, dtype = int)\n    bins[1:] = np.cumsum(z)\n    idx_groups  = [np.sort(tidx[bins[i]:bins[i+1]]) for i in range(Q)]\n    labels = [i for idx in range(n) for i in range(Q) if idx in idx_groups[i]]\n    return idx_groups, labels\n\ndef get_blockdiag_matrix(n, rholist, rhobg, idx_groups):\n    ''' \n    Generate a block diagonal matrix of size n x n.\n    S_ij = 1, if i = j\n         = rholist[q],  if i,j \\in idx_groups[q]\n         = rhobg, otherwise\n    '''\n    R = np.ones((n, n)) * rhobg\n\n    for i, (idx, rho) in enumerate(zip(idx_groups, rholist)):\n        nblock = idx.shape[0]\n        xblock = np.ones((nblock, nblock)) * rho \n        R[np.ix_(idx, idx)] = xblock\n\n    R[np.diag_indices_from(R)] = 1.0 \n\n    return R\n\ndef effect_size(n, p, k, Q, h2, g2, \n        aq, a0, nsample, \n        cov_design = 'blockdiag',\n        shuffle = False,\n        seed = None):\n    ''' \n    Get Y = LF' + M + E  where columns of F are orthonormal,\n    and L is a blockdiagonal matrix.\n    LF' correspond to the shared component of effect sizes,\n    the distinct components are given by M, which is sampled\n    from a Laplace distribution.\n    The noise in the estimate of the effect sizes is given by E.\n    '''\n    if seed is not None: np.random.seed(seed)\n    C_ixgrp, C = distribute_samples_to_classes(Q, n, shuffle = shuffle)\n    ggT  = np.sqrt(np.einsum('i,j-&gt;ij', g2, g2))\n    if cov_design == 'blockdiag':\n        rho  = [aq for _ in range(Q)]\n        covL = get_blockdiag_matrix(n, rho, a0, C_ixgrp) * ggT \n    else:\n        covL = np.eye(n) * ggT \n    # normalize L for correct variance.\n    L  = np.random.multivariate_normal(np.zeros(n), covL, size = k).T \n    L /= np.sqrt(k)\n    F  = spstats.ortho_group.rvs(p)[:, :k] \n    scaleM = np.sqrt((h2 - g2) * 0.5 / p)\n    M = np.random.laplace(np.zeros(n), scaleM, size = (p, n)).T\n    # obtain the matrix\n    Y0 = L @ F.T \n    covE = np.eye(p) / nsample\n    noise = np.random.multivariate_normal(np.zeros(p), covE, size = n)\n    Y = Y0 + noise\n    return Y, L, F, M, C\n\n\n\n\nCode\nn = 500\np = 1000\nk = 100\nQ = 3\nh2 = 0.6\nh2_shared_frac = 0.6\naq = 0.6\na0 = 0.2\nnsample = 10000\n\nh2array = np.ones(n) * h2\ng2array = np.ones(n) * h2 * h2_shared_frac\nY, L, F, M, C = effect_size(\n        n, p, k, Q, h2array, g2array, aq, a0, nsample,\n        cov_design = 'blockdiag', shuffle = False,\n        seed = None)\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\n#idxchoice = np.random.choice(nmax, size = nfctr, replace = False)\n# diag(cov(Y)) = h2 / P. [see for e.g. eq.58]\n# scale Y with P / h2 so that the diagonal elements are 1.\nmpy_plotfn.plot_covariance_heatmap(ax1, Y * np.sqrt(p))\n#mpy_plotfn.plot_heatmap(ax1, np.cov(Y) / h2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.random.laplace(np.zeros(n), scaleM, size = (p, n))\n\n\narray([[ 0.00497709,  0.00168154,  0.01172249, ..., -0.07249525,\n         0.00590398,  0.00444817],\n       [-0.00709314, -0.00984046, -0.00792401, ..., -0.00028352,\n        -0.00931836,  0.00040385],\n       [ 0.00559331, -0.00085078, -0.00108788, ...,  0.00122074,\n        -0.00827317,  0.00978397],\n       ...,\n       [ 0.01005872,  0.00483947, -0.00194775, ..., -0.01352505,\n        -0.00310229,  0.00225305],\n       [ 0.00132675, -0.00347866,  0.01516312, ...,  0.00833575,\n        -0.01608544,  0.00901879],\n       [-0.03345688,  0.00950119,  0.00123566, ..., -0.02009901,\n        -0.01008799,  0.00456026]])\n\n\n\n\nCode\nxfin = 1000000\nn = 200\n\nxinit = np.linspace(1000, 100000, 100)\nr = np.exp(np.log(xfin / xinit)/n) - 1\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(xinit, r * 100)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nxinit = 40000\nr = 0.01\nn = np.linspace(10,200,1000)\nxfin = xinit * np.power((1. + r), n)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(n, xfin)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nxfin[-1]\n\n\n146320.35703659907"
  },
  {
    "objectID": "notebooks/explore/2023-07-01-how-to-simulate.html",
    "href": "notebooks/explore/2023-07-01-how-to-simulate.html",
    "title": "How to simulate ground truth for multi-phenotype z-scores?",
    "section": "",
    "text": "About\nHere, I try to develop a generative model to create a feature matrix with realistic ground truth in order to benchmark different methods.\nMatrix factorization methods represent an observed N \\times P data matrix \\mathbf{Y} as:\n\n\\mathbf{Y} = \\mathbf{L}\\mathbf{F}^{\\intercal} + \\mathbf{E}\n\nwhere \\mathbf{L} is an N \\times K matrix, \\mathbf{F} is a P \\times K matrix, and \\mathbf{E} is an N \\times P matrix of residuals. For consistency, we adopt the notation and terminology of factor analysis, and refer to \\mathbf{L} as the loadings and \\mathbf{F} as the factors.\nDifferent matrix factorization methods assume different constraints on \\mathbf{L} and \\mathbf{F}. For example, PCA assumes that the columns of \\mathbf{L} are orthogonal and the columns of \\mathbf{F} are orthonormal. For the purpose of generating a ground truth, we will use a generative model,\n\n\\mathbf{Y} = \\mathbf{M} + \\mathbf{L}\\mathbf{F}^{\\intercal} + \\mathbf{E}\\,,\n where every row of \\mathbf{M} is equal to the mean value for each feature. The noise \\mathbf{E} is sampled from \\mathcal{N}(0, \\sigma^2 \\mathbf{I}). Equivalently, for sample (phenotype) i and feature (SNP) j, \ny_{ij} = m_j + \\sum_{k = 1}^K l_{ik}f_{jk} + e_j\n\nWe will make some realistic assumptions of \\mathbf{M}, \\mathbf{L}, \\mathbf{F} and \\mathbf{E}, as discussed below.\n\n\nGetting setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colormaps as mpl_cmaps\nimport matplotlib.colors as mpl_colors\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\n\n\n\n\nInput Dimensions\n\nntrait = 4 # categories / class\nngwas  = 100 # N\nnsnp   = 1000 # P\nnfctr  = 20 # K\n\n\n\nGenerate latent factors\nThe matrix \\mathbf{F} of latent factors, size P \\times K. Properties: - Semi-orthogonal matrix - Columns are K orthonormal vectors (all orthogonal to each other: ortho; all of unit length: **normal”)\n\n\nCode\nfrom scipy import stats as spstats\nF = spstats.ortho_group.rvs(nsnp)[:, :nfctr]\n\n# Quickly check if vectors are orthonormal\nnp.testing.assert_array_almost_equal(F.T @ F, np.identity(nfctr))\n\n\n\n\nCode\ndef do_standardize(Z, axis = 0, center = True, scale = True):\n    '''\n    Standardize (divide by standard deviation)\n    and/or center (subtract mean) of a given numpy array Z\n    \n    axis: the direction along which the std / mean is aggregated.\n        In other words, this axis is collapsed. For example,\n        axis = 0, means the rows will aggregated (collapsed).\n        In the output, the mean will be zero and std will be 1\n        along the remaining axes.\n        For a 2D array (matrix), use axis = 0 for column standardization\n        (with mean = 0 and std = 1 along the columns, axis = 1).\n        Simularly, use axis = 1 for row standardization\n        (with mean = 0 and std = 1 along the rows, axis = 0).\n        \n    center: whether or not to subtract mean.\n    \n    scale: whether or not to divide by std.\n    '''\n    dim = Z.ndim\n    \n    if scale:\n        Znew = Z / np.std(Z, axis = axis, keepdims = True)\n    else:\n        Znew = Z.copy()\n        \n    if center:\n        Znew = Znew - np.mean(Znew, axis = axis, keepdims = True)\n\n    return Znew\n\ndef get_equicorr_feature(n, p, rho = 0.8, seed = None, standardize = True):\n    '''\n    Return a matrix X of size n x p with correlated features.\n    The matrix S = X^T X has unit diagonal entries and constant off-diagonal entries rho.\n    \n    '''\n    if seed is not None: np.random.seed(seed)\n    iidx = np.random.normal(size = (n , p))\n    comR = np.random.normal(size = (n , 1))\n    x    = comR * np.sqrt(rho) + iidx * np.sqrt(1 - rho)\n\n    # standardize if required\n    if standardize:\n        x = do_standardize(x)\n\n    return x\n\ndef get_blockdiag_features(n, p, rholist, groups, rho_bg = 0.0, seed = None, standardize = True):\n    '''\n    Return a matrix X of size n x p with correlated features.\n    The matrix S = X^T X has unit diagonal entries and \n    k blocks of matrices, whose off-diagonal entries \n    are specified by elements of `rholist`.\n    \n    rholist: list of floats, specifying the correlation within each block\n    groups: list of integer arrays, each array contains the indices of the blocks.\n    '''\n    np.testing.assert_equal(len(rholist), len(groups))\n    \n    if seed is not None: np.random.seed(seed)\n    iidx = get_equicorr_feature(n, p, rho = rho_bg)\n\n    # number of blocks\n    k = len(rholist)\n    \n    # zero initialize\n    x = iidx.copy() #np.zeros_like(iidx)\n    \n    for rho, grp in zip(rholist, groups):\n        comR = np.random.normal(size = (n, 1))\n        x[:, grp] = np.sqrt(rho) * comR + np.sqrt(1 - rho) * iidx[:, grp]\n\n    # standardize if required\n    if standardize:\n        x = do_standardize(x)\n\n    return x\n\n\ndef get_blockdiag_matrix(n, rholist, groups):\n    R = np.ones((n, n))\n\n    for i, (idx, rho) in enumerate(zip(groups, rholist)):\n        nblock = idx.shape[0]\n        xblock = np.ones((nblock, nblock)) * rho\n        R[np.ix_(idx, idx)] = xblock\n        \n    return R\n\n# def get_correlated_features (n, p, R, seed = None, standardize = True, method = 'cholesky'):\n#     '''\n#     method: Choice of method, cholesky or eigenvector or blockdiag.\n#     '''\n\n#     # Generate samples from independent normally distributed random\n#     # variables (with mean 0 and std. dev. 1).\n#     x = norm.rvs(size=(p, n))\n\n#     # We need a matrix `c` for which `c*c^T = r`.  We can use, for example,\n#     # the Cholesky decomposition, or the we can construct `c` from the\n#     # eigenvectors and eigenvalues.\n\n#     if method == 'cholesky':\n#         # Compute the Cholesky decomposition.\n#         c = cholesky(r, lower=True)\n#     else:\n#         # Compute the eigenvalues and eigenvectors.\n#         evals, evecs = eigh(r)\n#         # Construct c, so c*c^T = r.\n#         c = np.dot(evecs, np.diag(np.sqrt(evals)))\n#     # Convert the data to correlated random variables. \n#     y = np.dot(c, x)\n#     return y\n\ndef get_sample_indices(ntrait, ngwas, shuffle = True):\n    '''\n    Distribute the samples in the categories (classes)\n    '''\n    rs = 0.6 * np.random.rand(ntrait) + 0.2 # random sample from [0.2, 0.8)\n    z = np.array(np.round((rs / np.sum(rs)) * ngwas), dtype = int)\n    z[-1] = ngwas - np.sum(z[:-1])\n    tidx = np.arange(ngwas)\n    if shuffle:\n        np.random.shuffle(tidx)\n    bins = np.zeros(ntrait + 1, dtype = int)\n    bins[1:] = np.cumsum(z)\n    sdict = {i : np.sort(tidx[bins[i]:bins[i+1]]) for i in range(ntrait)}\n    return sdict\n\ndef plot_covariance_heatmap(ax, X):\n    return plot_heatmap(ax, np.cov(X))\n\ndef plot_heatmap(ax, X):\n    '''\n    Helps to plot a heatmap\n    '''\n    cmap1 = mpl_cmaps.get_cmap(\"YlOrRd\").copy()\n    cmap1.set_bad(\"w\")\n    norm1 = mpl_colors.TwoSlopeNorm(vmin=0., vcenter=0.5, vmax=1.)\n    im1 = ax.imshow(X.T, cmap = cmap1, norm = norm1, interpolation='nearest', origin = 'upper')\n\n    ax.tick_params(bottom = False, top = True, left = True, right = False,\n                    labelbottom = False, labeltop = True, labelleft = True, labelright = False)\n\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.2)\n    cbar = plt.colorbar(im1, cax=cax, fraction = 0.1)\n    return\n\ndef reduce_dimension_svd(X, nfeature = None):\n    if k is None: k = int(X.shape[1] / 10)\n    k = max(1, k)\n    U, S, Vt = np.linalg.svd(X)\n    Uk = U[:, :k]\n    Sk = S[:k]\n    return Uk @ Sk\n\n# L_qr_ortho, L_qr_eig = orthogonalize_qr(do_standardize(L_full))\n# #idsort = np.argsort(L_eig)[::-1]\n# #idselect = idsort[:nfctr]\n# idselect = np.arange(nfctr)\n# L_qr =  L_qr_ortho[:, idselect] @ np.diag(L_qr_eig[idselect])\n\n# L_svd_ortho, L_svd_eig = orthogonalize_svd(do_standardize(L_full), k = nfctr)\n# L_svd = L_svd_ortho @ np.diag(L_svd_eig)\n\n\n\n\nGenerate loadings of each factor\nThe matrix \\mathbf{L} of loadings, size N \\times K. It encapsulates the similarity and distinctness of the samples in the latent space. We design \\mathbf{L} to contain the a covariance structure similar to the realistic data. The covariance of \\mathbf{L} is shown in Figure 1\nTo-Do: Is there a way to enforce the columns of \\mathbf{L} to be orthogonal? This can be both good and bad. Good, because many matrix factorization methods like PCA, etc assume that W is orthogonal. Bad, because real data may not be orthogonal.\n\n\nCode\nsample_dict = get_sample_indices(ntrait, ngwas, shuffle = False)\nsample_indices = [x for k, x in sample_dict.items()]\nrholist = [0.7 for x in sample_indices]\nrholist = [0.9, 0.5, 0.6, 0.9]\n\n\n\n\nCode\nL_full = get_blockdiag_features(1000, ngwas, rholist, sample_indices, rho_bg = 0.0, seed = 2000).T\nL = L_full[:, :nfctr]\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nplot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Covariance structure of the loadings\n\n\n\n\n\nThe covariance of \\mathbf{Y}_{\\mathrm{true}} = \\mathbf{L}\\mathbf{F}^{\\intercal} is shown in Figure 2 (left panel). In the right panel, we show the covariance of \\mathbf{Y}_{\\mathrm{true}}\\mathbf{F}.\n\n\nCode\nfig = plt.figure(figsize = (16, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nY_true = L @ F.T\nY_true_proj = Y_true @ F\n\nplot_covariance_heatmap(ax1, Y_true)\nplot_covariance_heatmap(ax2, Y_true_proj)\n\nax1.set_title(\"Covariance of Y in feature space\", pad = 50)\nax2.set_title(\"Covariance of Y after projection to latent space\", pad = 50)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Visualization of the true input matrix\n\n\n\n\n\n\nApproximate methods to obtain loadings decomposition\nEnforcing orthogonality on the loading will naturally overshrink the similarity and enhance the distinctness. Still, we can reduce the dimension of L_full using truncated SVD and use the projection of L_full on the first K components of the right singular vector.\nAnother option is to use a QR decomposition to obtain orthogonal columns, scaled by some approximate eigenvalues: 1. Transpose L_full and use QR decomposition. 2. \\mathbf{s}_{qr} \\leftarrow eigenvalues (diagonal of \\mathbf{R}). 3. Define \\mathbf{s}' such that s'_i = 1 if s_{qr} &gt; 0 and s'_i = -1 if s_{qr} &lt; 0. 4. Choose K orthogonal vectors from \\mathbf{Q} multiplied by \\mathrm{diag}(\\mathbf{s}'). This does not work numerically, as shown below in Figure 3\n\n\nCode\ndef orthogonalize_qr(X):\n    Q, R = np.linalg.qr(X)\n    eigv = np.diag(R).copy()\n    eigv[eigv &gt; 0] = 1.\n    eigv[eigv &lt; 0] = -1.\n    U = Q @ np.diag(eigv)\n    #return U, np.diag(R)\n    return Q, np.abs(np.diag(R))\n\ndef orthogonalize_svd(X, k = None):\n    if k is None:  k = X.shape[1]\n    U, S, Vt = np.linalg.svd(X)\n    Uk = U[:, :k]\n    Sk = S[:k]\n    return Uk, Sk\n\nL_qr_ortho, L_qr_eig = orthogonalize_qr(do_standardize(L_full))\n#idsort = np.argsort(L_eig)[::-1]\n#idselect = idsort[:nfctr]\nidselect = np.arange(nfctr)\nL_qr =  L_qr_ortho[:, idselect] @ np.diag(L_qr_eig[idselect])\n\nL_svd_ortho, L_svd_eig = orthogonalize_svd(do_standardize(L_full), k = nfctr)\nL_svd = L_svd_ortho @ np.diag(L_svd_eig)\n\n\nfig = plt.figure(figsize = (16, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nplot_covariance_heatmap(ax1, L_qr)\nplot_covariance_heatmap(ax2, L_svd)\n\nax1.set_title(\"Covariance of loadings after QR decomposition\", pad = 50)\nax2.set_title(\"Covariance of loadings after SVD decomposition\", pad = 50)\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Approximate methods for loading decomposition\n\n\n\n\n\n\n\n\nGenerate Data\n\n\n\n\n\n\n\nvariable\ndata\n\n\n\n\nY_true\nGround truth\n\n\nY_true_proj\nProjection of ground truth on the latent factors\n\n\nY\nObserved data\n\n\nY_std\nObserved data, standardized to mean 0 and std 1 for each feature (column)\n\n\nY_std_proj\nProjection of Y_std on the latent factors\n\n\n\n\n\nCode\n# fixed noise for every SNP\nsigma2 = np.random.uniform(1e-2, 5.0, nsnp)\nnoise = np.random.multivariate_normal(np.zeros(nsnp), np.diag(sigma2), size = ngwas)\nmeanshift = np.random.normal(0, 10, size = (1, nsnp))\n\n# Generate data and its projections on true F\nY_true = L @ F.T\nY_true_proj = Y_true @ F\nY = Y_true + meanshift + noise\nY_std = do_standardize(Y)\nY_std_proj = Y_std @ F\n\n\nWe obtain the observed \\mathbf{Y} by adding noise and mean to \\mathbf{Y}_{\\mathrm{true}}. The covariance of \\mathbf{Y} is shown in the left panel of (left panel). In the right panel, we show the covariance of \\mathbf{Y}\\mathbf{F}.\n\n\nCode\nfig = plt.figure(figsize = (16, 8))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nplot_covariance_heatmap(ax1, Y_std)\nplot_covariance_heatmap(ax2, Y_std_proj)\n\nax1.set_title(\"Covariance of Y in feature space\", pad = 50)\nax2.set_title(\"Covariance of Y after projection to latent space\", pad = 50)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Visualization of the observed input matrix\n\n\n\n\n\n\n\nPlot distribution of each feature\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\nfor i in range(Y.shape[1]):\n    x = Y_std[:, i]\n    outlier_mask = mpy_histogram.iqr_outlier(x, axis = 0, bar = 5)\n    data = x[~outlier_mask]\n    xmin, xmax, bins, xbin = mpy_histogram.get_bins(data, 100, None, None)\n    curve = mpy_histogram.get_density(xbin, data)\n    ax1.plot(xbin, curve)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Distribution of z-scores for all phenotypes\n\n\n\n\n\n\n\nCan SVD decomposition capture the signal?\nIn Figure 6, we look at the separation of the underlying features using - projection of \\mathbf{Y}_{\\mathrm{true}} on \\mathbf{F} (left panel). - projection of \\mathbf{Y} on \\mathbf{F} (center panel). - principal components obtained from SVD of \\mathbf{Y} (right panel).\n\n\nCode\nU, S, Vt = np.linalg.svd(Y_std, full_matrices=False)\npcomps_svd = U[:, :nfctr] @ np.diag(S[:nfctr])\n\n\n\n\nCode\nfig = plt.figure(figsize = (18, 6))\n\nidx1 = 1\nidx2 = 2\n\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\n\nmcolors = mpl_stylesheet.kelly_colors()\nfor i, grp in enumerate(sample_indices):\n    ax1.scatter(Y_true_proj[grp, idx1], Y_true_proj[grp, idx2], color = mcolors[i])\n    ax2.scatter(Y_std_proj[grp, idx1], Y_std_proj[grp, idx2], color = mcolors[i])\n    ax3.scatter(pcomps_svd[grp, idx1], pcomps_svd[grp, idx2], color = mcolors[i])\n    \nax1.set_title (\"Projection of ground truth on latent factors\", pad = 20)\nax2.set_title (\"Projection of observed data on latent factors\", pad = 20)\nax3.set_title (\"Principal components of observed data\", pad = 20)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Separation of components in different spaces\n\n\n\n\n\n\n\nCode\nYpred_svd = pcomps_svd @ Vt[:nfctr, :]\nY_true_std = do_standardize(Y_true)\n\nnp.sqrt(np.sum(np.square(Ypred_svd - Y_true) / np.square(Y_true)))\n\n\n190115.21364169012"
  },
  {
    "objectID": "notebooks/explore/manhattan_plot_to_tsv.html",
    "href": "notebooks/explore/manhattan_plot_to_tsv.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom scipy.stats import pearsonr\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\n\n\n\nCode\ndata_dir = \"../data\"\nbeta_df_filename   = f\"{data_dir}/beta_df.pkl\"\nprec_df_filename   = f\"{data_dir}/prec_df.pkl\"\nse_df_filename     = f\"{data_dir}/se_df.pkl\"\nzscore_df_filename = f\"{data_dir}/zscore_df.pkl\"\nsnp_info_filename  = f\"{data_dir}/snp_info.pkl\"\n\n'''\nData Frames for beta, precision, standard error and zscore.\n'''\n\nbeta_df   = pd.read_pickle(beta_df_filename)\nprec_df   = pd.read_pickle(prec_df_filename)\nse_df     = pd.read_pickle(se_df_filename)\nzscore_df = pd.read_pickle(zscore_df_filename)\nsnp_info  = pd.read_pickle(snp_info_filename)\n\ntrait_df = pd.read_csv(f\"{data_dir}/trait_meta.csv\")\nphenotype_dict = trait_df.set_index('ID')['Broad'].to_dict()\n\n\n\n\nCode\nX_nan = np.array(zscore_df).T\nX_nan_cent = X_nan - np.nanmean(X_nan, axis = 0, keepdims = True)\nX_nan_mask = np.isnan(X_nan)\nX_cent = np.nan_to_num(X_nan_cent, copy = True, nan = 0.0)\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(X_nan_mask) / np.prod(X_cent.shape):.3f}\")\n\n\nWe have 69 samples (phenotypes) and 10068 features (variants)\nFraction of Nan entries: 0.193\n\n\n\n\nCode\nselect_ids = zscore_df.columns\nlabels = [phenotype_dict[x] for x in select_ids]\nunique_labels = list(set(labels))\nnsample = X_cent.shape[0]\nntrait  = len(unique_labels)\n\ntrait_indices = [np.array([i for i, x in enumerate(labels) if x == label]) for label in unique_labels]\ntrait_colors  = {trait: color for trait, color in zip(unique_labels, (mpl_stylesheet.kelly_colors())[:ntrait])}\n\n\n\n\nCode\nmf_methods = ['ialm', 'nnm', 'nnm_sparse']\nlowrank_X = dict()\n\nfor method in mf_methods:\n    with open (f\"{data_dir}/lowrank_X_{method}.pkl\", 'rb') as handle:\n        lowrank_X[method] = pickle.load(handle)\n\n\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps, S\n\nloadings  = dict()\npcomps    = dict()\neigenvals = dict()\n\nloadings['tsvd'], pcomps['tsvd'], eigenvals['tsvd'] = get_principal_components(X_cent)\nfor m in mf_methods:\n    loadings[m], pcomps[m], eigenvals[m] = get_principal_components(lowrank_X[m])\n\n\n\n\nCode\nphenotype_dict_readable = {\n    'AD_sumstats_Jansenetal_2019sept.txt.gz' : 'AD_Jansen_2019',\n    'anxiety.meta.full.cc.txt.gz' : 'anxiety',\n    'anxiety.meta.full.fs.txt.gz' : 'anxiety',\n    'CNCR_Insomnia_all' : 'Insomnia',\n    'daner_adhd_meta_filtered_NA_iPSYCH23_PGC11_sigPCs_woSEX_2ell6sd_EUR_Neff_70.txt.gz' : 'ADHD_Daner',\n    'daner_PGC_BIP32b_mds7a_0416a.txt.gz' : 'BD_Daner_PGC',\n    'daner_PGC_BIP32b_mds7a_mds7a_BD1.0416a_INFO6_A5_NTOT.txt.gz' : 'BD1_Daner_PGC',\n    'daner_PGC_BIP32b_mds7a_mds7a_BD2.0416a_INFO6_A5_NTOT.txt.gz' : 'BD2_Daner_PGC',\n    'ENIGMA_Intracraneal_Volume' : 'Intracraneal_Volume',\n    'ieu-a-1000' : 'Neuroticism',\n    'ieu-a-1041' : 'Intracranial volume',\n    'ieu-a-1042' : 'Nucleus accumbens volume',\n    'ieu-a-1043' : 'Amygdala volume',\n    'ieu-a-1044' : 'Caudate volume',\n    'ieu-a-1045' : 'Hippocampus volume',\n    'ieu-a-1046' : 'Pallidum volume',\n    'ieu-a-1047' : 'Putamen volume',\n    'ieu-a-1048' : 'Thalamus volume',\n    'ieu-a-1085' : 'Amyotrophic lateral sclerosis',\n    'ieu-a-118' : 'Neuroticism',\n    'ieu-a-1183' : 'ADHD',\n    'ieu-a-1184' : 'Autism Spectrum Disorder',\n    'ieu-a-1185' : 'Autism Spectrum Disorder',\n    'ieu-a-1186' : 'Anorexia Nervosa',\n    'ieu-a-1188' : 'Major Depressive Disorder',\n    'ieu-a-1189' : 'Obsessive Compulsive Disorder',\n    'ieu-a-22' : 'Schizophrenia',\n    'ieu-a-297' : 'Alzheimers disease',\n    'ieu-a-806' : 'Autism',\n    'ieu-a-990' : 'Bulimia nervosa',\n    'ieu-b-10' : 'Focal epilepsy',\n    'ieu-b-11' : 'Focal epilepsy',\n    'ieu-b-12' : 'Juvenile absence epilepsy',\n    'ieu-b-13' : 'Childhood absence epilepsy',\n    'ieu-b-14' : 'Focal epilepsy',\n    'ieu-b-15' : 'Focal epilepsy',\n    'ieu-b-16' : 'Generalized epilepsy',\n    'ieu-b-17' : 'Juvenile myoclonic epilepsy',\n    'ieu-b-18' : 'Multiple sclerosis',\n    'ieu-b-2' : 'Alzheimers disease',\n    'ieu-b-41' : 'Bipolar Disorder',\n    'ieu-b-42' : 'Schizophrenia',\n    'ieu-b-7' : 'Parkinsons',\n    'ieu-b-8' : 'Epilepsy',\n    'ieu-b-9' : 'Generalized epilepsy',\n    'IGAP_Alzheimer' : 'IGAP_Alzheimer',\n    'iPSYCH-PGC_ASD_Nov2017.txt.gz' : 'ASD_PGC_Nov2017',\n    'Jones_et_al_2016_Chronotype' : 'Chronotype_Jones_2016',\n    'Jones_et_al_2016_SleepDuration' : 'Sleep_duration_Jones_2016',\n    'MDD_MHQ_BIP_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz' : 'MDD_BIP_no23andMe_noUKBB',\n    'MDD_MHQ_METACARPA_INFO6_A5_NTOT_no23andMe_noUKBB.txt.gz' : 'MDD_METACARPA_no23andMe_noUKBB',\n    'MHQ_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Depression',\n    'MHQ_Recurrent_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Recurrent_Depression',\n    'MHQ_Single_Depression_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Single_Depression',\n    'MHQ_Subthreshold_WG_MAF1_INFO4_HRC_Only_Filtered_Dups_FOR_METACARPA_INFO6_A5_NTOT.txt.gz' : 'Subthreshold_WG',\n    'ocd_aug2017.txt.gz' : 'OCD_aug2017',\n    'PGC_ADHD_EUR_2017' : 'ADHD_PGC_EUR_2017',\n    'PGC_ASD_2017_CEU' : 'ASD_PGC_2017_CEU',\n    'pgc-bip2021-all.vcf.txt.gz' : 'BD_PGC_all_2021',\n    'pgc-bip2021-BDI.vcf.txt.gz' : 'BDI_PGC_2021',\n    'pgc-bip2021-BDII.vcf.txt.gz' : 'BDII_PGC_2021',\n    'pgc.scz2' : 'Schizophrenia_PGC_2',\n    'PGC3_SCZ_wave3_public.v2.txt.gz' : 'Schizophrenia_PGC_3',\n    'pgcAN2.2019-07.vcf.txt.gz' : 'pgcAN2.2019-07.vcf.txt.gz',\n    'pts_all_freeze2_overall.txt.gz' : 'pts_all_freeze2',\n    'SSGAC_Depressive_Symptoms' : 'SSGAC_Depressive_Symptoms',\n    'SSGAC_Education_Years_Pooled' : 'SSGAC_Education_Years_Pooled',\n    'UKB_1160_Sleep_duration' : 'UKB_1160_Sleep_duration',\n    'UKB_1180_Morning_or_evening_person_chronotype' : 'UKB_1180_Morning_or_evening_person_chronotype',\n    'UKB_1200_Sleeplessness_or_insomnia' : 'UKB_1200_Sleeplessness_or_insomnia',\n    'UKB_20002_1243_self_reported_psychological_or_psychiatric_problem' : 'UKB_20002_1243_self_reported_psychological_or_psychiatric_problem',\n    'UKB_20002_1262_self_reported_parkinsons_disease' : 'UKB_20002_1262_self_reported_parkinsons_disease',\n    'UKB_20002_1265_self_reported_migraine' : 'UKB_20002_1265_self_reported_migraine',\n    'UKB_20002_1289_self_reported_schizophrenia' : 'UKB_20002_1289_self_reported_schizophrenia',\n    'UKB_20002_1616_self_reported_insomnia' : 'UKB_20002_1616_self_reported_insomnia',\n    'UKB_20016_Fluid_intelligence_score' : 'UKB_20016_Fluid_intelligence_score',\n    'UKB_20127_Neuroticism_score' : 'UKB_20127_Neuroticism_score',\n    'UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy' : 'UKB_G40_Diagnoses_main_ICD10_G40_Epilepsy',\n    'UKB_G43_Diagnoses_main_ICD10_G43_Migraine' : 'UKB_G43_Diagnoses_main_ICD10_G43_Migraine',\n    'ieu-b-5070' : 'Schizophrenia',\n    'GPC-NEO-NEUROTICISM' : 'GPC-NEO-NEUROTICISM',\n    'ieu-a-1009' : 'Subjective well being',\n    'ieu-a-1018' : 'Subjective well being',\n    'ieu-a-1019' : 'Migraine in bipolar disorder',\n    'ieu-a-1029' : 'Internalizing problems',\n    'ieu-a-1061' : 'G speed factor',\n    'ieu-a-1062' : 'Symbol search',\n    'ieu-a-1063' : '8-choice reaction time',\n    'ieu-a-1064' : '2-choice reaction time',\n    'ieu-a-1065' : 'Inspection time',\n    'ieu-a-1066' : 'Simple reaction time',\n    'ieu-a-1067' : 'Digit symbol',\n    'ieu-a-1068' : '4-choice reaction time',\n    'ieu-a-45' : 'Anorexia nervosa',\n    'ieu-a-298' : 'Alzheimers Disease',\n    'ieu-a-808' : 'Bipolar Disorder',\n    'ieu-a-810' : 'Schizophrenia',\n    'ieu-a-812' : 'Parkinsons',\n    'ieu-a-818' : 'Parkinsons',\n    'ieu-a-824' : 'Alzheimers Disease',\n    'ieu-b-43' : 'frontotemporal dementia',\n    'ILAE_Genetic_generalised_epilepsy' : 'ILAE_Genetic_generalised_epilepsy'\n}\n\nlabels_readable = [phenotype_dict_readable[x] for x in select_ids]\n\n\n\n\nCode\nplot_methods = ['tsvd'] + mf_methods\nplot_methods_names = {\n    'tsvd' : 'Raw Data',\n    'ialm' : 'RPCA-IALM',\n    'nnm'  : 'NNM-FW',\n    'nnm_sparse' : 'NNM-Sparse-FW',\n}\n\n\n\n\nCode\ncorrs = dict()\npvals = dict()\n\n# for m in plot_methods:\n#     corrs[m], pvals[m] = get_corrmat(pcomps[m], X_cent, ncomp = 20)\n#     with open (f\"{data_dir}/loading_corr_{m}.pkl\", 'wb') as handle:\n#         pickle.dump(corrs[m], handle, protocol=pickle.HIGHEST_PROTOCOL)\n#     with open (f\"{data_dir}/loading_corr_pval_{m}.pkl\", 'wb') as handle:\n#         pickle.dump(pvals[m], handle, protocol=pickle.HIGHEST_PROTOCOL)\n\nfor m in plot_methods:\n    with open (f\"{data_dir}/loading_corr_{m}.pkl\", 'rb') as handle:\n        corrs[m] = pickle.load(handle)\n    with open (f\"{data_dir}/loading_corr_pval_{m}.pkl\", 'rb') as handle:\n        pvals[m] = pickle.load(handle)\n\n\n\n\nCode\ndef pval_to_manhattan_data(pval_data, pcidx):\n    data = {i+1: dict() for i in range(22)}\n    for i, val in enumerate(pval_data[:, pcidx]):\n        rsid = rsid_list[i]\n        chrm = int(snp_info_dict[rsid]['CHR'])\n        bppos = snp_info_dict[rsid]['BP']\n        data[chrm][bppos] = - np.log10(val)\n    return data\n\n\n\n\nCode\ndef get_total_snps(sdict):\n    stot = {i + 1 : 0 for i in range(22)}\n    for snp, info in sdict.items():\n        chrm = int(info['CHR'])\n        bppos = info['BP']\n        if bppos &gt; stot[chrm]:\n            stot[chrm] = bppos\n    return stot\n\nrsid_list = zscore_df.index\nsnp_info_dict = snp_info.set_index('SNP').to_dict(orient = 'index')\nsnp_tot = get_total_snps(snp_info_dict)\n\n\n\n\nCode\nwhichmethod = 'nnm_sparse'\nnpcomp = 10\nfor icomp in range(npcomp):\n    pval_data = pval_to_manhattan_data(pvals[whichmethod], icomp)\n\n\n\n\nCode\npvals['nnm_sparse'].shape\n\n\n(10068, 20)\n\n\n\n\nCode\ndf = pd.DataFrame(pvals['nnm_sparse'], columns=[f\"PC{(i + 1):02d}\" for i in range(20)], index=rsid_list)\n\n\n\n\nCode\nchrm_list = [snp_info_dict[rsid]['CHR'] for rsid in rsid_list]\nbppos_list = [snp_info_dict[rsid]['BP'] for rsid in rsid_list]\na1_list = [snp_info_dict[rsid]['A1'] for rsid in rsid_list]\na2_list = [snp_info_dict[rsid]['A2'] for rsid in rsid_list]\n\n\n\n\nCode\ndf.insert(0, 'CHR', chrm_list)\ndf.insert(1, 'BP', bppos_list)\ndf.insert(2, 'A1', a1_list)\ndf.insert(3, 'A2', a2_list)\n\n\n\n\nCode\ndf\n\n\n\n\n\n\n\n\n\nCHR\nBP\nA1\nA2\nPC01\nPC02\nPC03\nPC04\nPC05\nPC06\n...\nPC11\nPC12\nPC13\nPC14\nPC15\nPC16\nPC17\nPC18\nPC19\nPC20\n\n\n\n\nrs1000031\n18\n48835070\nG\nA\n7.422631e-02\n0.097972\n0.247711\n0.476767\n0.082436\n5.433708e-01\n...\n0.935240\n0.941811\n0.914811\n0.581951\n0.920382\n0.403505\n0.249194\n0.979514\n0.752308\n0.430816\n\n\nrs1000269\n20\n20317839\nG\nA\n5.375219e-01\n0.502609\n0.022620\n0.000109\n0.293267\n3.003434e-01\n...\n0.921333\n0.634377\n0.572828\n0.158633\n0.349089\n0.254829\n0.424526\n0.668366\n0.954586\n0.964721\n\n\nrs10003281\n4\n135966929\nT\nC\n3.098978e-01\n0.954710\n0.211275\n0.893236\n0.068240\n1.853400e-08\n...\n0.676377\n0.720226\n0.469461\n0.947054\n0.722806\n0.138866\n0.822534\n0.430424\n0.562946\n0.444230\n\n\nrs10004866\n4\n65474274\nT\nG\n3.068253e-03\n0.918855\n0.297918\n0.265726\n0.015150\n3.711352e-02\n...\n0.431861\n0.369428\n0.166726\n0.002138\n0.295246\n0.756696\n0.433845\n0.100954\n0.075662\n0.507525\n\n\nrs10005235\n4\n74772610\nT\nC\n2.457373e-01\n0.095638\n0.288727\n0.322219\n0.561365\n8.401277e-01\n...\n0.057075\n0.597793\n0.973229\n0.195882\n0.379469\n0.414009\n0.968203\n0.993873\n0.748008\n0.305201\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nrs9989571\n18\n74972059\nC\nT\n6.318590e-06\n0.056867\n0.543619\n0.245213\n0.143151\n1.071744e-02\n...\n0.316641\n0.895875\n0.489673\n0.387509\n0.675797\n0.784278\n0.322042\n0.743365\n0.878251\n0.882301\n\n\nrs9991694\n4\n83078972\nT\nC\n3.752557e-07\n0.006464\n0.579804\n0.056797\n0.010551\n1.643510e-02\n...\n0.743803\n0.927547\n0.613604\n0.883619\n0.452764\n0.525951\n0.557713\n0.398007\n0.964597\n0.439896\n\n\nrs9992763\n4\n108137562\nG\nT\n9.895024e-01\n0.828606\n0.754621\n0.486959\n0.678962\n3.847490e-01\n...\n0.926779\n0.998543\n0.216710\n0.802382\n0.745869\n0.095897\n0.997941\n0.025970\n0.071930\n0.946161\n\n\nrs9993607\n4\n58503142\nC\nT\n1.660853e-06\n0.000227\n0.982201\n0.669206\n0.000183\n9.630507e-01\n...\n0.478628\n0.478179\n0.808850\n0.164558\n0.878914\n0.655345\n0.416203\n0.511181\n0.896615\n0.757639\n\n\nrs999494\n2\n72930266\nC\nT\n3.347897e-06\n0.758201\n0.127911\n0.655321\n0.155012\n1.133519e-03\n...\n0.403499\n0.887910\n0.727940\n0.421461\n0.790410\n0.223866\n0.659267\n0.475587\n0.967269\n0.829686\n\n\n\n\n10068 rows × 24 columns\n\n\n\n\n\nCode\ndf.to_csv(\"variant_contribution_to_PC.csv\")"
  },
  {
    "objectID": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html",
    "href": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html",
    "title": "Choosing step size for Inexact ALM algorithm",
    "section": "",
    "text": "There are two alternate proposed methods for updating the penalty \\mu on the residual. The penalty is equivalent to the step size. Here, we compare the two methods on a simulated dataset. The proposed methods are:\n\nEq. 25 of Lin et. al.\nEq. 3.13 of Boyd et. al.\n\nTo-Do: Check stopping criterion of Boyd et. al. using Eq. 3.12."
  },
  {
    "objectID": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#default",
    "href": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#default",
    "title": "Choosing step size for Inexact ALM algorithm",
    "section": "Default",
    "text": "Default\n\n\nCode\nY_rpca_cent = mpy_simulate.do_standardize(rpca.L_, scale = False)\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Y_rpca_cent, full_matrices = False)\npcomps_rpca = U_rpca @ np.diag(S_rpca)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#admm",
    "href": "notebooks/explore/2023-07-28-choosing-penalty-update-method.html#admm",
    "title": "Choosing step size for Inexact ALM algorithm",
    "section": "ADMM",
    "text": "ADMM\n\n\nCode\nY_rpca_cent = mpy_simulate.do_standardize(rpca_admm.L_, scale = False)\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Y_rpca_cent, full_matrices = False)\npcomps_rpca = U_rpca @ np.diag(S_rpca)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-12-14-comparison-of-simulation-strategies.html",
    "href": "notebooks/explore/2023-12-14-comparison-of-simulation-strategies.html",
    "title": "Comparing two different simulation strategies - Direct Model vs Genetics Model",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe_CV\nfrom nnwmf.optimize import FrankWolfe\n\n\n\nntrait = 3 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 100 # K\n\n\n\nCode\nh2     = np.ones(ngwas) * 0.6\nhg2    = np.ones(ngwas) * 0.2\nhm2    = h2 - hg2\n\n\n\n\nCode\ndef get_sample_indices(nclass, n, shuffle = True):\n    ''' \n    Distribute the samples in the categories (classes)\n    '''\n    rs = 0.6 * np.random.rand(nclass) + 0.2 # random sample from [0.2, 0.8)\n    z = np.array(np.round((rs / np.sum(rs)) * n), dtype = int)\n    z[-1] = n - np.sum(z[:-1])\n    tidx = np.arange(n)\n    if shuffle:\n        np.random.shuffle(tidx)\n    bins = np.zeros(nclass + 1, dtype = int)\n    bins[1:] = np.cumsum(z)\n    #sdict = {i : np.sort(tidx[bins[i]:bins[i+1]]) for i in range(nclass)}\n    C = [np.sort(tidx[bins[i]:bins[i+1]]) for i in range(nclass)]\n    return C\n\n\n\n\nCode\nC = get_sample_indices(3, 20, shuffle = True)\n\n\n\n\nCode\nnp.eye(10)\n\n\narray([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n\n\n\n\nCode\n[i for idx in range(20) for i in range(3) if idx in C[i]]\n\n\n[1, 0, 2, 0, 1, 1, 2, 2, 0, 0, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1]\n\n\n\n\nCode\nsample_dict = mpy_simulate.get_sample_indices(ntrait, ngwas, shuffle = False)\nsample_indices = [x for _, x in sample_dict.items()]\nunique_labels  = [k for k, _ in sample_dict.items()]\nclass_labels = [None for x in range(ngwas)]\nfor k, x in sample_dict.items():\n    for i in x:\n        class_labels[i] = k\n\n\n\n\nCode\nfrom scipy import stats as spstats\n\ndef sample_covariance_matrix (covX, size):\n    n, p = size\n    L = np.linalg.cholesky(covX)\n    Z = np.random.normal(0, 1, size = n * p).reshape(n, p)\n    Zstd = mpy_simulate.do_standardize(Z, axis = 1)\n    return L @ Zstd\n\n\nrholist = [0.8 for _ in unique_labels]\nggT = np.sqrt(np.einsum('i,j-&gt;ij', hg2, hg2))\nF = spstats.ortho_group.rvs(nsnp)[:, :nfctr]\ncovL = mpy_simulate.get_blockdiag_matrix(ngwas, rholist, sample_indices, bg = 0.1) * ggT\nL = np.random.multivariate_normal(np.zeros(ngwas), covL, size = nfctr).T / np.sqrt(nfctr)\n#L_ = sample_covariance_matrix(covL, (ngwas, nfctr))\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\n#idxchoice = np.random.choice(nmax, size = nfctr, replace = False)\n#mpy_plotfn.plot_covariance_heatmap(ax1, L * np.sqrt(nfctr))\nmpy_plotfn.plot_heatmap(ax1, covL)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nY0 = L @ F.T\nprint (\"Rows of Y0\")\nprint (f\"Median expectation: {np.median(np.mean(Y0, axis = 1)):g}\")\nprint (f\"Median variance (times P): {np.median(np.var(Y0, axis = 1) * nsnp):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(Y0), axis = 1)):g}\")\nprint ()\nprint (\"Columns of Y0\")\nprint (f\"Median expectation: {np.median(np.mean(Y0, axis = 0)):g}\")\nprint (f\"Median variance (times N): {np.median(np.var(Y0, axis = 0) * ngwas):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(Y0), axis = 0)):g}\")\nprint (f\"Median L1 norm: {np.median(np.sum(np.abs(Y0), axis = 0)):g}\")\nprint ()\n\nnrow = ngwas\nncol = nsnp\nscaleM = np.sqrt(hm2 * 0.5 / ncol)\nM = np.random.laplace(np.zeros(nrow), scaleM, size = (ncol, nrow)).T\n# r_M = 1.\n# for i in range(nsnp):\n#     M[:, i] = M[:, i] * r_M / np.linalg.norm(M[:, i], ord = 1)\nprint (\"Rows of M\")\nprint (f\"Median expectation: {np.median(np.mean(M, axis = 1)):g}\")\nprint (f\"Median variance (times P): {np.median(np.var(M, axis = 1) * ncol):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(M), axis = 1)):g}\")\nprint ()\nprint (\"Columns of M\")\nprint (f\"Median expectation: {np.median(np.mean(M, axis = 0)):g}\")\nprint (f\"Median variance (times N): {np.median(np.var(M, axis = 0) * nrow):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(M), axis = 0)):g}\")\nprint (f\"Median L1 norm: {np.median(np.sum(np.abs(M), axis = 0)):g}\")\nprint ()\n\nY = Y0 + M\nprint (\"Rows of Y\")\nprint (f\"Median expectation: {np.median(np.mean(Y, axis = 1)):g}\")\nprint (f\"Median variance (times P): {np.median(np.var(Y, axis = 1) * nsnp):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(Y), axis = 1)):g}\")\nprint ()\nprint (\"Columns of Y\")\nprint (f\"Median expectation: {np.median(np.mean(Y, axis = 0)):g}\")\nprint (f\"Median variance (times N): {np.median(np.var(Y, axis = 0) * ngwas):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(Y), axis = 0)):g}\")\nprint (f\"Median L1 norm: {np.median(np.sum(np.abs(Y), axis = 0)):g}\")\nprint ()\n\n\nRows of Y0\nMedian expectation: -0.000155426\nMedian variance (times P): 0.224106\nMedian L2 norm squared: 0.224177\n\nColumns of Y0\nMedian expectation: 0.000209719\nMedian variance (times N): 0.0512845\nMedian L2 norm squared: 0.0865713\nMedian L1 norm: 5.42823\n\nRows of M\nMedian expectation: -1.05241e-05\nMedian variance (times P): 0.396502\nMedian L2 norm squared: 0.396703\n\nColumns of M\nMedian expectation: 6.99139e-05\nMedian variance (times N): 0.197444\nMedian L2 norm squared: 0.197776\nMedian L1 norm: 7.05178\n\nRows of Y\nMedian expectation: -0.000146142\nMedian variance (times P): 0.620688\nMedian L2 norm squared: 0.621036\n\nColumns of Y\nMedian expectation: 0.000142319\nMedian variance (times N): 0.252043\nMedian L2 norm squared: 0.287131\nMedian L1 norm: 9.1555\n\n\n\n\n\nCode\ncovE = np.eye(nsnp) * 1 / 40000\nnoise = np.random.multivariate_normal(np.zeros(nsnp), covE, size = ngwas)\nprint (\"Rows of noise\")\nprint (f\"Median expectation: {np.median(np.mean(noise, axis = 1)):g}\")\nprint (f\"Median variance (times P): {np.median(np.var(noise, axis = 1) * nsnp):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(noise), axis = 1)):g}\")\nprint ()\nprint (\"Columns of noise\")\nprint (f\"Median expectation: {np.median(np.mean(noise, axis = 0)):g}\")\nprint (f\"Median variance: {np.median(np.var(noise, axis = 0)):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(noise), axis = 0)):g}\")\nprint (f\"Median L1 norm: {np.median(np.sum(np.abs(noise), axis = 0)):g}\")\nprint ()\n\n\nRows of noise\nMedian expectation: -1.09343e-05\nMedian variance (times P): 0.0249726\nMedian L2 norm squared: 0.0249982\n\nColumns of noise\nMedian expectation: -1.26038e-05\nMedian variance: 2.49111e-05\nMedian L2 norm squared: 0.012479\nMedian L1 norm: 1.99437\n\n\n\n\n\nCode\nY * np.sqrt(nsnp / h2.reshape(ngwas, 1))\n\n\narray([[-1.33357538, -0.95947936,  0.17148265, ..., -0.4863197 ,\n        -0.71514046, -0.15900849],\n       [-0.04274935, -0.6466637 ,  0.469406  , ...,  0.64127725,\n        -0.30498921,  0.20851449],\n       [-1.69318498, -0.23030792,  0.4263881 , ...,  0.00532535,\n        -1.32942984, -1.62658069],\n       ...,\n       [ 0.85148603, -0.79558655, -0.48725268, ..., -1.88284527,\n         0.47964622, -1.33434093],\n       [ 0.68072411, -0.18798221, -0.12279653, ..., -0.76358145,\n         0.1272466 , -1.33720936],\n       [-0.01052967,  0.16498775,  1.41944669, ..., -0.10789497,\n        -0.16660526, -2.33952439]])\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\n#idxchoice = np.random.choice(nmax, size = nfctr, replace = False)\n# diag(cov(Y)) = h2 / P. [see for e.g. eq.58]\n# scale Y with P / h2 so that the diagonal elements are 1.\nmpy_plotfn.plot_covariance_heatmap(ax1, Y * np.sqrt(nsnp))\n#mpy_plotfn.plot_heatmap(ax1, np.cov(Y) / h2)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint (\"Rows of F\")\nprint (f\"Median expectation: {np.median(np.mean(F, axis = 1)):g}\")\nprint (f\"Median variance (times K): {np.median(np.var(F, axis = 1) * nfctr):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(F), axis = 1)):g}\")\nprint ()\nprint (\"Columns of F\")\nprint (f\"Median expectation: {np.median(np.mean(F, axis = 0)):g}\")\nprint (f\"Median variance (times P): {np.median(np.var(F, axis = 0) * nsnp):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(F), axis = 0)):g}\")\n\n\nRows of F\nMedian expectation: -1.93028e-06\nMedian variance (times K): 0.0990139\nMedian L2 norm squared: 0.0999616\n\nColumns of F\nMedian expectation: -5.1289e-05\nMedian variance (times P): 0.999559\nMedian L2 norm squared: 1\n\n\n\n\nCode\nprint (\"Rows of L\")\nprint (f\"Median expectation: {np.median(np.mean(L, axis = 1)):g}\")\nprint (f\"Median variance (times K): {np.median(np.var(L, axis = 1) * nfctr):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(L), axis = 1)):g}\")\nprint ()\nprint (\"Columns of L\")\nprint (f\"Median expectation: {np.median(np.mean(L, axis = 0)):g}\")\nprint (f\"Median variance (times N): {np.median(np.var(L, axis = 0) * ngwas):g}\")\nprint (f\"Median L2 norm squared: {np.median(np.sum(np.square(L), axis = 0)):g}\")\n\n\nRows of L\nMedian expectation: -9.67536e-05\nMedian variance (times K): 0.223455\nMedian L2 norm squared: 0.224177\n\nColumns of L\nMedian expectation: -0.00185697\nMedian variance (times N): 0.526052\nMedian L2 norm squared: 0.825194\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nax1.hist(np.var(Y0, axis = 0))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\n#idxchoice = np.random.choice(nmax, size = nfctr, replace = False)\nmpy_plotfn.plot_covariance_heatmap(ax1, Y0 * np.sqrt(nsnp))\n#mpy_plotfn.plot_heatmap(ax1, covL)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.linalg.norm(Y0, ord = 1)\n\n\n16.89425850071007\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\n#idxchoice = np.random.choice(nmax, size = nfctr, replace = False)\nmpy_plotfn.plot_covariance_heatmap(ax1, mpy_simulate.do_standardize(Y))\n#mpy_plotfn.plot_heatmap(ax1, covL)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nhg2 = 0.9\nmaf = np.random.uniform(0.05, 0.5, nsnp)\nvar_Lb = hg2 / (nsnp * nfctr * 2.0 * maf * (1 - maf))\n\n\n\n\nCode\nLb = np.zeros((nsnp,nfctr))\nfor k in range(nfctr):\n    Lb[:,k] = np.random.normal(0, np.sqrt(var_Lb))\nFb = np.random.normal(0, 1, ngwas * nfctr).reshape(ngwas,nfctr)\n\n\n\n\nCode\nbetab = np.zeros((ngwas, nsnp))\nfor i in range(ngwas):\n    betab[i,:] = np.dot(Lb, Fb[i, :])\n\n\n\n\nCode\nvar_Xbetab = np.square(betab * 2.0 * maf * (1 - maf))\nvar_Yb = np.sum(var_Xbetab, axis = 1) / hg2\nresiduals_b = var_Yb.reshape(-1, 1) - var_Xbetab\n\n\n\n\nCode\nnsample = 10000\n\nstderr2_b = residuals_b / (nsample * (2.0 * maf * (1 - maf) + np.square(2.0 * maf)))\nZ_true = betab / np.sqrt(stderr2_b)\nZ_true_cent = mpy_simulate.do_standardize(Z_true, scale = False)\n\n\n\n\nCode\nZ = np.zeros_like(Z_true)\nfor i in range(ngwas):\n    Sigma_est = np.zeros((nsnp, nsnp))\n    Sigma_est[np.diag_indices(nsnp)] = stderr2_b[i,:]\n    beta_est = np.random.multivariate_normal(betab[i,:], Sigma_est)\n    Z[i, :] = beta_est / np.sqrt(stderr2_b[i, :])\nZ_cent = mpy_simulate.do_standardize(Z, scale = False)\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, Z_cent)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef truncated_SVD(X, thres = 0.9):\n    U, S, Vt = np.linalg.svd(X, full_matrices = False)\n    k = np.where(np.cumsum(S / np.sum(S)) &gt;= thres)[0][0]\n    pcomps = U[:, :k] @ np.diag(S[:k])\n    return U, S, Vt, pcomps\n\n_, S_tsvd, _, pcomps_tsvd = truncated_SVD(Z_cent)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_tsvd, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nrpca = IALM()\nrpca.fit(Y)\nnp.linalg.matrix_rank(rpca.L_)\n\n\n290\n\n\n\n\nCode\nY_rpca_cent = mpy_simulate.do_standardize(rpca.L_, scale = False)\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Y_rpca_cent, full_matrices = False)\npcomps_rpca = U_rpca @ np.diag(S_rpca)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\n#idxchoice = np.random.choice(nmax, size = nfctr, replace = False)\nmpy_plotfn.plot_covariance_heatmap(ax1, mpy_simulate.do_standardize(Y_rpca_cent))\n#mpy_plotfn.plot_heatmap(ax1, covL)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef truncated_SVD(X, thres = 0.9):\n    U, S, Vt = np.linalg.svd(X, full_matrices = False)\n    k = np.where(np.cumsum(S / np.sum(S)) &gt;= thres)[0][0]\n    pcomps = U[:, :k] @ np.diag(S[:k])\n    return U, S, Vt, pcomps\n\n_, S_tsvd, _, pcomps_tsvd = truncated_SVD(mpy_simulate.do_standardize(Y))\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_tsvd, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnnm_sparse = FrankWolfe(model = 'nnm-sparse', max_iter = 1000, svd_max_iter = 50, \n                        tol = 1e-3, step_tol = 1e-5, simplex_method = 'sort',\n                        show_progress = True, debug = True, print_skip = 100)\n#nnm_sparse.fit(np.nanmean(X_nan, axis = 0, keepdims = True), (300.0, 0.5))\nnnm_sparse.fit(mpy_simulate.do_standardize(Y, axis=0), (300.0, 0.5))\n\n\n2024-01-11 11:00:37,502 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 2.10098e+06\n2024-01-11 11:00:49,306 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.001. Duality Gap 45.1048\n2024-01-11 11:01:00,465 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.000. Duality Gap 22.8586\n2024-01-11 11:01:11,607 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.001. Duality Gap 30.7189\n2024-01-11 11:01:22,735 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 400. Step size 0.001. Duality Gap 29.6669\n2024-01-11 11:01:33,922 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 500. Step size 0.000. Duality Gap 13.3876\n2024-01-11 11:01:45,169 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 600. Step size 0.000. Duality Gap 19.7748\n2024-01-11 11:01:57,191 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 700. Step size 0.001. Duality Gap 29.0264\n2024-01-11 11:02:09,342 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 800. Step size 0.000. Duality Gap 27.5278\n2024-01-11 11:02:20,460 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 900. Step size 0.000. Duality Gap 16.3087\n\n\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps\n\nloadings_nnm_sparse, pcomps_nnm_sparse = get_principal_components(nnm_sparse.X)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()"
  },
  {
    "objectID": "notebooks/explore/2023-07-05-simulation-setup.html",
    "href": "notebooks/explore/2023-07-05-simulation-setup.html",
    "title": "Simulation setup for benchmarking matrix factorization methods",
    "section": "",
    "text": "About\nHere, I check if the simulation benchmarking makes sense using simple examples. The idea is to run large scale simulations using pipelines. Before that, I want to look at simple examples.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\n\nimport sys\nsys.path.append(\"../utils/\")\nimport histogram as mpy_histogram\nimport simulate as mpy_simulate\nimport plot_functions as mpy_plotfn\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe_CV\nfrom nnwmf.optimize import FrankWolfe\n\n\n\nntrait = 4 # categories / class\nngwas  = 500 # N\nnsnp   = 1000 # P\nnfctr  = 40 # K\n\n\n\nCode\nsample_dict = mpy_simulate.get_sample_indices(ntrait, ngwas, shuffle = False)\nsample_indices = [x for _, x in sample_dict.items()]\nunique_labels  = [k for k, _ in sample_dict.items()]\nclass_labels = [None for x in range(ngwas)]\nfor k, x in sample_dict.items():\n    for i in x:\n        class_labels[i] = k\n\n\n\n\nCode\nY, Y_true, L, F, mean, noise_var = mpy_simulate.simulate(ngwas, nsnp, ntrait, nfctr, sample_groups = sample_indices, std = 1.0)\nY_cent = mpy_simulate.do_standardize(Y, scale = False)\n\nfig = plt.figure(figsize = (8, 8))\nax1 = fig.add_subplot(111)\n\nmpy_plotfn.plot_covariance_heatmap(ax1, L)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTrue components\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(L, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nTruncated SVD\n\n\nCode\ndef truncated_SVD(X, thres = 0.9):\n    U, S, Vt = np.linalg.svd(X, full_matrices = False)\n    k = np.where(np.cumsum(S / np.sum(S)) &gt;= thres)[0][0]\n    pcomps = U[:, :k] @ np.diag(S[:k])\n    return U, S, Vt, pcomps\n\n_, S_tsvd, _, pcomps_tsvd = truncated_SVD(Y_cent)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_tsvd, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nS2 = np.square(S_tsvd)\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(np.arange(S2.shape[0]), np.cumsum(S2 / np.sum(S2)), 'o-')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nNuclear Norm Minimization using Frank-Wolfe algorithm\n\n\nCode\nnnmcv = FrankWolfe_CV(chain_init = True, reverse_path = False, kfolds = 2)\nnnmcv.fit(Y_cent)\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(2):\n    #ax1.plot(np.log10(list(nnmcv.training_error.keys())), [x[k] for x in nnmcv.training_error.values()], 'o-')\n    ax1.plot(np.log10(list(nnmcv.test_error.keys())), [x[k] for x in nnmcv.test_error.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nr_opt = 32.0\n\nnnm = FrankWolfe(show_progress = True, svd_max_iter = 50, debug = True, suppress_warnings = True)\nnnm.fit(Y_cent, r_opt)\n\nY_nnm_cent = mpy_simulate.do_standardize(nnm.X, scale = False)\nU_nnm, S_nnm, Vt_nnm = np.linalg.svd(Y_nnm_cent, full_matrices = False)\npcomps_nnm = U_nnm @ np.diag(S_nnm)\n\n\n2023-07-28 11:16:31,147 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 1.000. Duality Gap 2742.83\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_nnm, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWeighted Nuclear Norm Minimization\n\n\nCode\nsnp_weights = 1 / np.sqrt(noise_var)\nweight = np.column_stack([snp_weights for _ in range(ngwas)]).T\n\nwnnmcv = FrankWolfe_CV(chain_init = True, reverse_path = False, kfolds = 5, debug = True)\nwnnmcv.fit(Y_cent, weight = weight)\n\n\n2023-07-28 11:16:55,580 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Cross-validation over 15 ranks.\n2023-07-28 11:16:55,604 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 1 ...\n2023-07-28 11:17:13,426 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 2 ...\n2023-07-28 11:17:35,075 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 3 ...\n2023-07-28 11:17:57,035 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 4 ...\n2023-07-28 11:18:16,387 | nnwmf.optimize.frankwolfe_cv             | DEBUG   | Fold 5 ...\n\n\n\n\nCode\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nfor k in range(5):\n    #ax1.plot(np.log10(list(nnmcv.training_error.keys())), [x[k] for x in nnmcv.training_error.values()], 'o-')\n    ax1.plot(np.log10(list(wnnmcv.test_error.keys())), [x[k] for x in wnnmcv.test_error.values()], 'o-')\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log2')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nr_opt = 32.0\n\nwnnm = FrankWolfe(show_progress = True, svd_max_iter = 50, debug = True)\nwnnm.fit(Y_cent, r_opt, weight = weight)\n\nY_wnnm_cent = mpy_simulate.do_standardize(wnnm.X, scale = False)\nU_wnnm, S_wnnm, Vt_wnnm = np.linalg.svd(Y_wnnm_cent, full_matrices = False)\npcomps_wnnm = U_wnnm @ np.diag(S_wnnm)\n\n\n2023-07-28 11:18:52,909 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 0. Step size 0.154. Duality Gap 7.2392e+06\n2023-07-28 11:18:55,519 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 100. Step size 0.003. Duality Gap 100806\n2023-07-28 11:18:58,015 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 200. Step size 0.003. Duality Gap 42452.7\n2023-07-28 11:19:00,500 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 300. Step size 0.002. Duality Gap 31460.9\n2023-07-28 11:19:02,997 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 400. Step size 0.005. Duality Gap 20861.9\n2023-07-28 11:19:05,500 | nnwmf.optimize.frankwolfe                | INFO    | Iteration 500. Step size 0.004. Duality Gap 16232.8\n\n\n\n\nCode\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_wnnm, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nRobust PCA\n\n\nCode\nrpca = IALM()\nrpca.fit(Y_cent)\nnp.linalg.matrix_rank(rpca.L_)\n\n\nNameError: name 'lmb' is not defined\n\n\n\n\nCode\nY_rpca_cent = mpy_simulate.do_standardize(L_rpca, scale = False)\nU_rpca, S_rpca, Vt_rpca = np.linalg.svd(Y_rpca_cent, full_matrices = False)\npcomps_rpca = U_rpca @ np.diag(S_rpca)\naxmain, axs = mpy_plotfn.plot_principal_components(pcomps_rpca, class_labels, unique_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef get_rmse(original, recovered, mask = None):\n    if mask is None:\n        mask = np.ones(original.shape)\n    n = np.sum(mask)\n    mse = np.sum(np.square((original - recovered) * mask)) / n\n    return np.sqrt(mse)\n    \nY_true_cent = mpy_simulate.do_standardize(Y_true, scale = False)\nY_rpca_cent = mpy_simulate.do_standardize(L_rpca, scale = False)\n\nrmse_nnm = get_rmse(Y_true_cent, Y_nnm_cent)\nrmse_wnnm = get_rmse(Y_true_cent, Y_wnnm_cent)\nrmse_rpca = get_rmse(Y_true_cent, Y_rpca_cent)\n\nprint (f\"{rmse_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{rmse_wnnm:.4f}\\tWeighted Nuclear Norm Minimization\")\nprint (f\"{rmse_rpca:.4f}\\tRobust PCA\")\n\n\n0.1448  Nuclear Norm Minimization\n0.1427  Weighted Nuclear Norm Minimization\n0.1054  Robust PCA\n\n\n\n\nCode\ndef get_psnr(original, recovered):\n    n, p = original.shape\n    maxsig2 = np.square(np.max(original) - np.min(original))\n    mse = np.sum(np.square(recovered - original)) / (n * p)\n    res = 10 * np.log10(maxsig2 / mse)\n    return res\n\npsnr_nnm = get_psnr(Y_true_cent, Y_nnm_cent)\npsnr_wnnm = get_psnr(Y_true_cent, Y_wnnm_cent)\npsnr_rpca = get_psnr(Y_true_cent, Y_rpca_cent)\n\nprint (f\"{psnr_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{psnr_wnnm:.4f}\\tWeighted Nuclear Norm Minimization\")\nprint (f\"{psnr_rpca:.4f}\\tRobust PCA\")\n\n\n21.2372 Nuclear Norm Minimization\n21.3642 Weighted Nuclear Norm Minimization\n23.9949 Robust PCA\n\n\n\n\nCode\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn import metrics as skmetrics\n\ndef get_adjusted_MI_score(pcomp, class_labels):\n    distance_matrix = skmetrics.pairwise.pairwise_distances(pcomp, metric='euclidean')\n    model = AgglomerativeClustering(n_clusters = 4, linkage = 'average', metric = 'precomputed')\n    class_pred = model.fit_predict(distance_matrix)\n    return skmetrics.adjusted_mutual_info_score(class_labels, class_pred)\n\nadjusted_mi_nnm = get_adjusted_MI_score(pcomps_nnm,   class_labels)\nadjusted_mi_wnnm = get_adjusted_MI_score(pcomps_wnnm, class_labels)\nadjusted_mi_rpca = get_adjusted_MI_score(pcomps_rpca, class_labels)\n\nprint (f\"{adjusted_mi_nnm:.4f}\\tNuclear Norm Minimization\")\nprint (f\"{adjusted_mi_wnnm:.4f}\\tWeighted Nuclear Norm Minimization\")\nprint (f\"{adjusted_mi_rpca:.4f}\\tRobust PCA\")\n\n\n-0.0006 Nuclear Norm Minimization\n1.0000  Weighted Nuclear Norm Minimization\n1.0000  Robust PCA"
  },
  {
    "objectID": "notebooks/ukbb/2023-11-27-snp-position-filter.html",
    "href": "notebooks/ukbb/2023-11-27-snp-position-filter.html",
    "title": "LD filtering of UKBB data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as sc_stats\nimport collections\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\n\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120)\n\n\n\n\nCode\nphenotype_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/npd_phenotypes.tsv\"\nvariants_metafile = \"/gpfs/commons/home/sbanerjee/work/npd/UKBB/metadata/significant_variants.tsv\"\ndata_dir = \"/gpfs/commons/home/sbanerjee/npddata/ukbb.imputed_v3.neale/3_all_assoc\"\n\n\n\n\nCode\nphenotype_df = pd.read_csv(phenotype_metafile, sep = '\\t')\nphenotype_df\n\n\n\n\n\n\n\n\n\nphenotype\ndescription\nvariable_type\nsource\nn_non_missing\nn_missing\nn_controls\nn_cases\nPHESANT_transformation\nnotes\n\n\n\n\n0\n1160\nSleep duration\nordinal\nphesant\n359020\n2174\nNaN\nNaN\n1160_0|| INTEGER || reassignments: -1=NA|-3=NA...\nACE touchscreen question \"About how many hours...\n\n\n1\n1200\nSleeplessness / insomnia\nordinal\nphesant\n360738\n456\nNaN\nNaN\n1200_0|| CAT-SINGLE || Inc(&gt;=10): 3(102157) ||...\nACE touchscreen question \"Do you have trouble ...\n\n\n2\n1220\nDaytime dozing / sleeping (narcolepsy)\nordinal\nphesant\n359752\n1442\nNaN\nNaN\n1220_0|| CAT-SINGLE || reassignments: 3=2 || I...\nACE touchscreen question \"How likely are you t...\n\n\n3\n1920\nMood swings\nbinary\nphesant\n352604\n8590\n193622.0\n158982.0\n1920_0|| CAT-SINGLE || Inc(&gt;=10): 0(193622) ||...\nACE touchscreen question \"Does your mood often...\n\n\n4\n1970\nNervous feelings\nbinary\nphesant\n351829\n9365\n268709.0\n83120.0\n1970_0|| CAT-SINGLE || Inc(&gt;=10): 0(268709) ||...\nACE touchscreen question \"Would you call yours...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n312\nT79\nDiagnoses - main ICD10: T79 Certain early comp...\nbinary\nicd10\n361194\n0\n360989.0\n205.0\nNaN\nNaN\n\n\n313\nTRAUMBRAIN_NONCONCUS\nsevere traumatic brain injury, does not includ...\nbinary\nfinngen\n361194\n0\n360631.0\n563.0\nNaN\nNaN\n\n\n314\nVI_NERVOUS\nDiseases of the nervous system\nbinary\nfinngen\n361194\n0\n339871.0\n21323.0\nNaN\nNaN\n\n\n315\nV_MENTAL_BEHAV\nMental and behavioural disorders\nbinary\nfinngen\n361194\n0\n356892.0\n4302.0\nNaN\nNaN\n\n\n316\nZ43\nDiagnoses - main ICD10: Z43 Attention to artif...\nbinary\nicd10\n361194\n0\n359838.0\n1356.0\nNaN\nNaN\n\n\n\n\n317 rows × 10 columns\n\n\n\n\n\nCode\nphenotype_ids = phenotype_df['phenotype'].to_list()\nassoc_file = {}\nfor s in phenotype_ids:\n    assoc_file[s] = f\"{data_dir}/{s}.tsv\"\n\n\n\n\nCode\nassoc_df = pd.read_pickle(\"../data/ukbb_assoc.pkl\")\nassoc_df.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 13311781 entries, 0 to 41992\nData columns (total 7 columns):\n #   Column          Dtype  \n---  ------          -----  \n 0   variant         object \n 1   low_confidence  bool   \n 2   beta            float64\n 3   se              float64\n 4   tstat           float64\n 5   pval            float64\n 6   phenotype_id    object \ndtypes: bool(1), float64(4), object(2)\nmemory usage: 723.6+ MB\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df['variant'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df['phenotype_id'].unique())}\")\n\n\nNumber of unique SNPs: 41993\nNumber of unique studies: 317\n\n\n\n\nCode\nvariant_colnames = ['variant', 'chr', 'pos', 'ref', 'alt', 'rsid', 'varid', \n                    'consequence', 'consequence_category', 'info', 'call_rate',\n                    'AC', 'AF', 'minor_allele', 'minor_AF', 'p_hwe', \n                    'n_called', 'n_not_called', 'n_hom_ref', 'n_het', 'n_hom_var', 'n_non_ref',\n                    'r_heterozygosity', 'r_het_hom_var', 'r_expected_het_frequency']\nvariant_df = pd.read_csv(variants_metafile, sep = '\\t', names = variant_colnames)\n\ndef count_nucleotides(row):\n    return len(row['ref']) + len(row['alt'])\n\nvariant_df['ntcount'] = variant_df.apply(count_nucleotides, axis = 1)\nvariant_df_filtered = variant_df[(variant_df['ntcount'] == 2) \n                               & (variant_df['minor_AF'] &gt;= 0.05)\n                               & (variant_df['info'] &gt;= 0.8)].drop(columns = ['ntcount'])\n\n\n\n\nCode\nvariant_df_filtered.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 37395 entries, 9 to 41991\nData columns (total 25 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   variant                   37395 non-null  object \n 1   chr                       37395 non-null  int64  \n 2   pos                       37395 non-null  int64  \n 3   ref                       37395 non-null  object \n 4   alt                       37395 non-null  object \n 5   rsid                      37395 non-null  object \n 6   varid                     37395 non-null  object \n 7   consequence               37395 non-null  object \n 8   consequence_category      37395 non-null  object \n 9   info                      37395 non-null  float64\n 10  call_rate                 37395 non-null  float64\n 11  AC                        37395 non-null  int64  \n 12  AF                        37395 non-null  float64\n 13  minor_allele              37395 non-null  object \n 14  minor_AF                  37395 non-null  float64\n 15  p_hwe                     37395 non-null  float64\n 16  n_called                  37395 non-null  int64  \n 17  n_not_called              37395 non-null  int64  \n 18  n_hom_ref                 37395 non-null  int64  \n 19  n_het                     37395 non-null  int64  \n 20  n_hom_var                 37395 non-null  int64  \n 21  n_non_ref                 37395 non-null  int64  \n 22  r_heterozygosity          37395 non-null  float64\n 23  r_het_hom_var             37395 non-null  float64\n 24  r_expected_het_frequency  37395 non-null  float64\ndtypes: float64(8), int64(9), object(8)\nmemory usage: 7.4+ MB\n\n\n\n\nCode\ndef ldclump_snppos(xarr, pos_cutoff=100000):\n    xclump = list()\n    xsorted = np.sort(xarr)\n    xlast = xsorted[0]\n    xclump.append(xlast)\n    for x in xsorted[1:]:\n        if (x - xlast &gt; pos_cutoff):\n            xclump.append(x)\n            xlast = x\n    return np.array(xclump)\n\n\n\n\nCode\nldfilter_dict = []\nfor chrm in range(1,23):\n    chr_snppos = variant_df_filtered[variant_df_filtered['chr'] == chrm]['pos'].values\n    chr_snppos_filtered = ldclump_snppos(chr_snppos, pos_cutoff = 10000)\n    ldfilter_dict += [{'chr':chrm, 'pos':x} for x in chr_snppos_filtered]\n\n\n\n\nCode\nlen(ldfilter_dict)\n\n\n3386\n\n\n\n\nCode\ndf_ldfilter = pd.DataFrame.from_dict(ldfilter_dict)\nvariant_df_ldfiltered = df_ldfilter.merge(variant_df_filtered, on=['chr', 'pos'], how='inner')\n\n\n\n\nCode\nvariant_df_ldfiltered.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3387 entries, 0 to 3386\nData columns (total 25 columns):\n #   Column                    Non-Null Count  Dtype  \n---  ------                    --------------  -----  \n 0   chr                       3387 non-null   int64  \n 1   pos                       3387 non-null   int64  \n 2   variant                   3387 non-null   object \n 3   ref                       3387 non-null   object \n 4   alt                       3387 non-null   object \n 5   rsid                      3387 non-null   object \n 6   varid                     3387 non-null   object \n 7   consequence               3387 non-null   object \n 8   consequence_category      3387 non-null   object \n 9   info                      3387 non-null   float64\n 10  call_rate                 3387 non-null   float64\n 11  AC                        3387 non-null   int64  \n 12  AF                        3387 non-null   float64\n 13  minor_allele              3387 non-null   object \n 14  minor_AF                  3387 non-null   float64\n 15  p_hwe                     3387 non-null   float64\n 16  n_called                  3387 non-null   int64  \n 17  n_not_called              3387 non-null   int64  \n 18  n_hom_ref                 3387 non-null   int64  \n 19  n_het                     3387 non-null   int64  \n 20  n_hom_var                 3387 non-null   int64  \n 21  n_non_ref                 3387 non-null   int64  \n 22  r_heterozygosity          3387 non-null   float64\n 23  r_het_hom_var             3387 non-null   float64\n 24  r_expected_het_frequency  3387 non-null   float64\ndtypes: float64(8), int64(9), object(8)\nmemory usage: 661.7+ KB\n\n\n\n\nCode\nassoc_df_fvar = variant_df_ldfiltered[['variant', 'rsid']].merge(assoc_df, on = ['variant'], how = 'inner')\n\n\n\n\nCode\nprint (f\"Number of unique SNPs: {len(assoc_df_fvar['variant'].unique())}\")\nprint (f\"Number of unique studies: {len(assoc_df_fvar['phenotype_id'].unique())}\")\n\n\nNumber of unique SNPs: 3387\nNumber of unique studies: 317\n\n\n\n\nCode\nzscore_df = assoc_df_fvar[['variant', 'phenotype_id', 'tstat']].pivot(index = 'variant', columns = 'phenotype_id', values = 'tstat').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nbeta_df   = assoc_df_fvar[['variant', 'phenotype_id', 'beta']].pivot(index = 'variant', columns = 'phenotype_id', values = 'beta').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\nse_df     = assoc_df_fvar[['variant', 'phenotype_id', 'se']].pivot(index = 'variant', columns = 'phenotype_id', values = 'se').rename_axis(None, axis = 0).rename_axis(None, axis = 1)\n\n\n\n\nCode\nzscore_df\n\n\n\n\n\n\n\n\n\n1160\n1200\n1220\n1920\n1970\n1980\n20002_1123\n20002_1240\n20002_1243\n20002_1246\n...\nR53\nR56\nR90\nSFN\nSLEEP\nT79\nTRAUMBRAIN_NONCONCUS\nVI_NERVOUS\nV_MENTAL_BEHAV\nZ43\n\n\n\n\n10:100010186:A:G\n-0.501456\n0.468240\n-1.401350\n-2.319730\n-4.101760\n-2.469050\n1.373340\n0.807527\n-1.483770\n-0.983242\n...\n-1.047450\n-0.541606\n-1.559310\n0.746273\n0.644985\n-0.028021\n-0.159701\n3.711700\n-1.380550\n-0.903030\n\n\n10:100020572:T:G\n0.687418\n-0.769290\n0.041449\n-2.215650\n-1.051370\n-0.254845\n0.022909\n-0.961978\n-0.344766\n-1.693950\n...\n-0.750566\n0.989090\n0.516234\n-0.152721\n-0.266356\n1.091550\n-0.054509\n3.134090\n-1.631720\n-0.641998\n\n\n10:100602545:C:G\n0.197858\n0.137792\n0.064242\n-1.027020\n-0.967476\n-0.528904\n1.598920\n-0.491628\n-0.103273\n-0.099966\n...\n1.198700\n0.925530\n-0.146606\n1.605810\n0.764948\n2.064470\n1.832110\n1.723660\n-0.391214\n0.789003\n\n\n10:100612750:G:C\n0.243301\n0.144469\n0.043636\n-0.979825\n-0.982524\n-0.524519\n1.652070\n-0.498177\n0.032667\n-0.131171\n...\n1.208040\n1.012120\n-0.168484\n1.617130\n0.776149\n2.232480\n1.825990\n1.762370\n-0.351120\n0.801818\n\n\n10:100627302:G:C\n0.303313\n0.288102\n-0.243662\n-1.241620\n-0.917032\n-0.624787\n1.616470\n-0.359382\n0.038058\n-0.138088\n...\n0.959963\n1.038140\n-0.187544\n1.637230\n0.858652\n1.866400\n1.898110\n1.759880\n-0.374304\n0.609954\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9:98225056:G:T\n-0.609864\n0.191835\n2.357520\n3.899610\n3.187290\n5.256400\n0.402311\n1.592520\n-0.382107\n1.365880\n...\n1.017950\n1.900860\n2.127600\n0.056486\n0.692330\n-0.307143\n-0.591845\n0.056605\n-0.247403\n-1.640570\n\n\n9:98235310:T:C\n-0.731896\n-0.239943\n1.869780\n3.750520\n3.260280\n5.254040\n-0.255022\n0.183105\n0.344827\n1.237990\n...\n0.913732\n1.789060\n1.465640\n0.407725\n0.614795\n-0.168933\n-0.381017\n0.452154\n0.714727\n-1.272540\n\n\n9:98247204:C:T\n-0.627788\n0.057694\n2.106230\n4.309430\n3.180430\n4.965740\n0.358445\n0.787971\n-0.052250\n1.509800\n...\n0.796777\n1.726200\n1.235950\n-0.052887\n1.313870\n-0.505464\n-0.119510\n-0.124091\n1.065030\n-1.160720\n\n\n9:98262178:G:A\n-0.619834\n-0.240170\n2.556200\n4.230840\n3.632350\n4.855730\n0.002387\n0.601861\n0.154543\n1.272620\n...\n0.684594\n1.329930\n1.257680\n0.055839\n0.996216\n-0.217356\n-0.178359\n-0.336342\n1.148090\n-1.059110\n\n\n9:98273305:T:G\n-0.846092\n0.070976\n2.650630\n4.323630\n3.354090\n5.113290\n-0.051482\n0.623136\n0.203378\n1.743840\n...\n0.865338\n1.432780\n1.166460\n0.018348\n0.479160\n-0.233870\n-0.179549\n-0.483875\n0.839336\n-0.996326\n\n\n\n\n3387 rows × 317 columns\n\n\n\n\n\nCode\nmean_se  = se_df.median(axis = 0, skipna = True)\nmean_se  = pd.DataFrame(mean_se).set_axis([\"mean_se\"], axis = 1)\nbeta_std = beta_df.std(axis = 0, skipna = True)\nbeta_std = pd.DataFrame(beta_std).set_axis([\"beta_std\"], axis = 1)\nerror_df = pd.concat([mean_se, beta_std], axis = 1)\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(np.log10(error_df['beta_std']), np.log10(error_df['mean_se']), alpha = 0.5, s = 100)\nmpl_utils.set_xticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.set_yticks(ax1, scale = 'log10', spacing = 'log10')\nmpl_utils.plot_diag(ax1)\n\nkeep_columns = error_df.query(\"mean_se &lt;= 0.2\").index\nfor pid in error_df.index.to_list():\n    if pid not in keep_columns:\n        pid_text = f\"{pid} / {phenotype_dict[pid]}\"\n        xval = np.log10(error_df.loc[pid]['beta_std'])\n        yval = np.log10(error_df.loc[pid]['mean_se'])\n        ax1.annotate(pid_text, (xval, yval))\n\nax1.set_xlabel(r\"Standard Deviation of mean $\\beta$\")\nax1.set_ylabel(r\"Median of SE\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Calibration of SE against std of beta\n\n\n\n\n\n\n\nCode\nzscore_df.to_pickle(\"../data/ukbb_zscore_df2.pkl\")\nbeta_df.to_pickle(\"../data/ukbb_beta_df2.pkl\")\nse_df.to_pickle(\"../data/ukbb_se_df2.pkl\")"
  },
  {
    "objectID": "notebooks/ukbb/2023-12-12-hidden-factors-pan-ukb.html",
    "href": "notebooks/ukbb/2023-12-12-hidden-factors-pan-ukb.html",
    "title": "Pan-UKB Hidden Factors v01",
    "section": "",
    "text": "Getting Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\nfrom matplotlib.gridspec import GridSpec\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport simulate as mpy_simulate\n\n\n\n\nLoad data and results\n\n\nCode\ndata_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/data\"\nresult_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/nnwmf\"\nzscore_filename = f\"{data_dir}/GWAS_Zscore.tsv\"\ntrait_filename = f\"{data_dir}/trait_manifest_TableS6_no_readme.tsv\"\nzscore_df = pd.read_csv(zscore_filename, sep = '\\t')\ntrait_df = pd.read_csv(trait_filename, sep = '\\t')\n\n# remove extra columns from trait_df\n\ncolnames = trait_df.columns.tolist()\ncolnames[0] = \"zindex\"\ntrait_df.columns = colnames\ntrait_df_mod = trait_df.drop(labels = ['coding', 'modifier', 'coding_description', 'filename', 'aws_link'], axis=1)\ntrait_df_mod['trait_name'] = trait_df_mod['description']\ntrait_df_mod['trait_name'] = trait_df_mod['trait_name'].fillna(trait_df_mod['phenocode'])\n\n\n\n\nCode\nX_nan = np.array(zscore_df.loc[:, zscore_df.columns!='rsid']).T\nX_nan_cent = X_nan - np.nanmean(X_nan, axis = 0, keepdims = True)\nX_nan_mask = np.isnan(X_nan)\nX_cent = np.nan_to_num(X_nan_cent, copy = True, nan = 0.0)\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(X_nan_mask) / np.prod(X_cent.shape):.3f}\")\n\n\nWe have 2483 samples (phenotypes) and 51399 features (variants)\nFraction of Nan entries: 0.000\n\n\n\n\nCode\nmf_methods = ['ialm', 'nnm_sparse', 'tsvd']\n\nmethod_prefix = {\n    'ialm' : 'ialm_maxiter10000_admm',\n    'nnm_sparse' : 'nnm_sparse_maxiter1000'\n}\n\nmethod_names = {\n    'tsvd' : 'Raw Data',\n    'ialm' : 'RPCA-IALM',\n    'nnm'  : 'NNM-FW',\n    'nnm_weighted' : 'NNM-Weighted',\n    'nnm_sparse' : 'NNM-Sparse-FW',\n}\n\nwith open (f\"{result_dir}/{method_prefix['ialm']}_progress.pkl\", 'rb') as handle:\n    ialm = pickle.load(handle)\n    \nwith open (f\"{result_dir}/{method_prefix['nnm_sparse']}_progress.pkl\", 'rb') as handle:\n    nnm_sparse = pickle.load(handle)\n\n\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps, S\n\nlowrank_X = dict()\nloadings  = dict()\npcomps    = dict()\neigenvals = dict()\n\nfor m in mf_methods:\n    if m != 'tsvd':\n        with open (f\"{result_dir}/{method_prefix[m]}_lowrank_X.pkl\", 'rb') as handle:\n            lowrank_X[m] = pickle.load(handle)\nlowrank_X['tsvd'] = X_cent.copy()\nfor m in mf_methods:\n    loadings[m], pcomps[m], eigenvals[m] = get_principal_components(lowrank_X[m])\n\n\n\n\nCode\ndef get_cos2_scores(pcomps):\n    ntrait, npcomp = pcomps.shape\n    x = np.zeros((ntrait, npcomp))\n    std_pcomps = pcomps / np.sqrt(np.var(pcomps[:,:], axis = 0))\n    for i in range(ntrait):\n        cos2_trait = np.array([np.square(std_pcomps[i, pcidx]) for pcidx in range(npcomp)])\n        x[i, :] = cos2_trait / np.sum(cos2_trait)\n    return x\n\ndef get_contribution_scores(pcomps):\n    ntrait, npcomp = pcomps.shape\n    x = np.zeros((ntrait, npcomp))\n    std_pcomps = pcomps / np.sqrt(np.var(pcomps[:,:], axis = 0)).reshape(1, -1)\n    for i in range(npcomp):\n        trait_contr = np.array([np.square(std_pcomps[j, i]) for j in range(ntrait)])\n        x[:, i] = trait_contr / np.sum(trait_contr)\n    return x\n\nnpcomp = 100\ncos2_scores = dict()\nfor m in mf_methods:\n    cos2_scores[m] = get_cos2_scores(pcomps[m][:,:npcomp])\n\ntrait_contributions = dict()\nfor m in mf_methods:\n    trait_contributions[m] = get_contribution_scores(pcomps[m][:,:npcomp])\n\n\n\n\nExplanatory power of hidden factors\nFor the top 20 principal components (hidden factors), we rank the cos2 scores for the top traits in Figure 1\n\n\nCode\nimax = 30\nnpcomp = 20\nm = 'ialm'\n\nfig = plt.figure(figsize= (12, 10 * npcomp))\ngs = GridSpec(nrows = npcomp, ncols=1, figure=fig, height_ratios=[1 for i in range(npcomp)])\nax = [None for i in range(npcomp)]\n\nfor i in range(npcomp):\n    ax[i] = fig.add_subplot(gs[i, 0])\n\n    top_trait_indices = np.argsort(cos2_scores[m][:, i])[::-1][:imax]\n    top_trait_labels  = trait_df_mod.loc[top_trait_indices]['trait_name'].tolist()\n    top_trait_labels  = [x[:30] for x in top_trait_labels]\n\n    ax[i].bar(np.arange(imax), cos2_scores[m][top_trait_indices, i])\n    ax[i].set_xticks(np.arange(imax))\n    ax[i].set_xticklabels(top_trait_labels, rotation = 90, ha = 'center')\n    ax[i].set_title(f\"PC{i+1}\")\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Explanatory power of hidden factors. For each hidden factors (PC1 to PC20), we rank the diseases associated with the hidden factors."
  },
  {
    "objectID": "notebooks/ukbb/2023-12-11-pleiotropy-from-contribution-scores.html",
    "href": "notebooks/ukbb/2023-12-11-pleiotropy-from-contribution-scores.html",
    "title": "Pan-UKB Pleitropy of Diseases v01",
    "section": "",
    "text": "Getting Setup\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\nfrom matplotlib.gridspec import GridSpec\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport simulate as mpy_simulate\n\n\n\n\nLoad data and results\n\n\nCode\ndata_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/data\"\nresult_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/nnwmf\"\nzscore_filename = f\"{data_dir}/GWAS_Zscore.tsv\"\ntrait_filename = f\"{data_dir}/trait_manifest_TableS6_no_readme.tsv\"\nzscore_df = pd.read_csv(zscore_filename, sep = '\\t')\ntrait_df = pd.read_csv(trait_filename, sep = '\\t')\n\n# remove extra columns from trait_df\n\ncolnames = trait_df.columns.tolist()\ncolnames[0] = \"zindex\"\ntrait_df.columns = colnames\ntrait_df_mod = trait_df.drop(labels = ['coding', 'modifier', 'coding_description', 'filename', 'aws_link'], axis=1)\ntrait_df_mod['trait_name'] = trait_df_mod['description']\ntrait_df_mod['trait_name'] = trait_df_mod['trait_name'].fillna(trait_df_mod['phenocode'])\n\n\n\n\nCode\nX_nan = np.array(zscore_df.loc[:, zscore_df.columns!='rsid']).T\nX_nan_cent = X_nan - np.nanmean(X_nan, axis = 0, keepdims = True)\nX_nan_mask = np.isnan(X_nan)\nX_cent = np.nan_to_num(X_nan_cent, copy = True, nan = 0.0)\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(X_nan_mask) / np.prod(X_cent.shape):.3f}\")\n\n\nWe have 2483 samples (phenotypes) and 51399 features (variants)\nFraction of Nan entries: 0.000\n\n\n\n\nCode\nmf_methods = ['ialm', 'nnm_sparse', 'tsvd']\n\nmethod_prefix = {\n    'ialm' : 'ialm_maxiter10000_admm',\n    'nnm_sparse' : 'nnm_sparse_maxiter1000'\n}\n\nmethod_names = {\n    'tsvd' : 'Raw Data',\n    'ialm' : 'RPCA-IALM',\n    'nnm'  : 'NNM-FW',\n    'nnm_weighted' : 'NNM-Weighted',\n    'nnm_sparse' : 'NNM-Sparse-FW',\n}\n\nwith open (f\"{result_dir}/{method_prefix['ialm']}_progress.pkl\", 'rb') as handle:\n    ialm = pickle.load(handle)\n    \nwith open (f\"{result_dir}/{method_prefix['nnm_sparse']}_progress.pkl\", 'rb') as handle:\n    nnm_sparse = pickle.load(handle)\n\n\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps, S\n\nlowrank_X = dict()\nloadings  = dict()\npcomps    = dict()\neigenvals = dict()\n\nfor m in mf_methods:\n    if m != 'tsvd':\n        with open (f\"{result_dir}/{method_prefix[m]}_lowrank_X.pkl\", 'rb') as handle:\n            lowrank_X[m] = pickle.load(handle)\nlowrank_X['tsvd'] = X_cent.copy()\nfor m in mf_methods:\n    loadings[m], pcomps[m], eigenvals[m] = get_principal_components(lowrank_X[m])\n\n\n\n\nCode\ndef get_cos2_scores(pcomps):\n    ntrait, npcomp = pcomps.shape\n    x = np.zeros((ntrait, npcomp))\n    std_pcomps = pcomps / np.sqrt(np.var(pcomps[:,:], axis = 0))\n    for i in range(ntrait):\n        cos2_trait = np.array([np.square(std_pcomps[i, pcidx]) for pcidx in range(npcomp)])\n        x[i, :] = cos2_trait / np.sum(cos2_trait)\n    return x\n\ndef get_contribution_scores(pcomps):\n    ntrait, npcomp = pcomps.shape\n    x = np.zeros((ntrait, npcomp))\n    std_pcomps = pcomps / np.sqrt(np.var(pcomps[:,:], axis = 0)).reshape(1, -1)\n    for i in range(npcomp):\n        trait_contr = np.array([np.square(std_pcomps[j, i]) for j in range(ntrait)])\n        x[:, i] = trait_contr / np.sum(trait_contr)\n    return x\n\nnpcomp = 100\ncos2_scores = dict()\nfor m in mf_methods:\n    cos2_scores[m] = get_cos2_scores(pcomps[m][:,:npcomp])\n\ntrait_contributions = dict()\nfor m in mf_methods:\n    trait_contributions[m] = get_contribution_scores(pcomps[m][:,:npcomp])\n\n\n\n\nPleiotropy\nIn the following, we look at the pleiotropy of different diseases reported in the UKB project. Given a phenotype (focal trait), we find the hidden factor with the highest contribution to the focal trait. We then report the other phenotypes which contribute to that hidden factor.\nFunction pan_ukb_pleiotropy(trait_index)\nInput: - Index of the focal trait as found in the manifest file. - Number of pleiotropic traits to be reported (optional, default 15).\nOutput: - Index of the top hidden factor. - Contribution of the hidden factor on the focal trait. - List of pleiotropic traits, and their contributions to the hidden factor.\nGlobal Variables: trait_df_mod, mf_methods, cos2_scores, trait_contributions, method_names\n\n\nCode\nfrom IPython.display import Markdown, display\nfrom IPython.core.display import HTML\n\nimport re\ndef titlecase(s):\n    return re.sub(r\"[A-Za-z]+('[A-Za-z]+)?\", lambda mo: mo.group(0).capitalize(),s)\n\ndef print_markdown(string, color = None):\n    colorstr = \"&lt;span style='color:{}'&gt;{}&lt;/span&gt;\".format(color, string)\n    display(Markdown(colorstr))\n    \ndef print_header(string):\n    headerstr=f\"&lt;h2&gt;{string}&lt;/h2&gt;\"\n    display(Markdown(headerstr))\n    \ndef print_table(list1, list2):\n    #table_css = 'table {align:left;display:block}'\n    tablestr = \"&lt;table style={float:left; align:left;}&gt;\"\n    for l1, l2 in zip(list1, list2):\n        tablestr += f\"&lt;tr&gt;&lt;td&gt;{l1:.4f}&lt;/td&gt;&lt;td&gt;{l2}&lt;/td&gt;&lt;/tr&gt;\"\n    tablestr +=\"&lt;/table&gt;\"\n    display(Markdown(tablestr))\n            \ndef pan_ukb_pleiotropy(znum, num_trait = 15):\n    zindex = znum - 1\n    focal_trait_name = trait_df_mod.loc[zindex, 'trait_name']\n    print_header(titlecase(focal_trait_name))\n    for m in mf_methods:\n        print_markdown (f\"**{method_names[m]}**\", color = \"#c8635a\")\n        top_factor = np.argsort(cos2_scores[m][zindex,:])[::-1][0]\n        print_markdown (f\"Top Factor: {top_factor + 1} (Contribution: {cos2_scores[m][zindex,top_factor]:.4f})\")\n        top_traits = np.argsort(trait_contributions[m][:,top_factor])[::-1][:num_trait]\n        trait_names = trait_df_mod.loc[top_traits]['trait_name'].tolist()\n        print_table(trait_contributions[m][top_traits, top_factor], trait_names)\n\ndef pan_ukb_pleiotropy_(znum, num_trait = 15):\n    zindex = znum - 1\n    focal_trait_name = trait_df_mod.loc[zindex, 'trait_name']\n    printheader(titlecase(focal_trait_name))\n    for m in mf_methods:\n        printmd (f\"**{method_names[m]}**\", color = \"#c8635a\")\n        top_factor = np.argsort(cos2_scores[m][zindex,:])[::-1][0]\n        printmd(f\"Top Factor: {top_factor + 1} (Contribution: {cos2_scores[m][zindex,top_factor]:.4f})\")\n        top_traits = np.argsort(trait_contributions[m][:,top_factor])[::-1][:num_trait]\n        trait_names = trait_df_mod.loc[top_traits]['trait_name'].tolist()\n        for i, x in zip(top_traits, trait_names):\n            print(f\"    {trait_contributions[m][i, top_factor]:.4f}   {x}\")\n        print(f\"\")\n\n\n\n\nCommon Diseases\n\n\nCode\npan_ukb_pleiotropy(930)\n\n\n\nRheumatoid Arthritis\n\n\n\nRPCA-IALM\n\n\nTop Factor: 47 (Contribution: 0.1697)\n\n\n\n\n\n0.0236\nmethotrexate\n\n\n0.0236\nanti-metabolite\n\n\n0.0167\nfolic acid supplement|Food Supplement\n\n\n0.0167\nfolic acid\n\n\n0.0146\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0141\nForced expiratory volume in 1-second (FEV1)\n\n\n0.0131\nForced vital capacity (FVC), Best measure\n\n\n0.0126\nForced vital capacity (FVC)\n\n\n0.0113\nanti-inflammatory\n\n\n0.0113\nsulfasalazine\n\n\n0.0104\nRheumatoid arthritis\n\n\n0.0097\nNeutrophill percentage\n\n\n0.0097\nRheumatoid arthritis and other inflammatory polyarthropathies\n\n\n0.0095\nCorneal resistance factor (left)\n\n\n0.0093\nCorneal resistance factor (right)\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 88 (Contribution: 0.0664)\n\n\n\n\n\n0.0319\nWhole body fat-free mass\n\n\n0.0251\nLeg fat percentage (right)\n\n\n0.0194\nHand grip strength (left)\n\n\n0.0120\nLeg predicted mass (right)\n\n\n0.0118\nForced vital capacity (FVC), Best measure\n\n\n0.0110\ncodine/paracetamol\n\n\n0.0103\nArm fat-free mass (right)\n\n\n0.0099\nMean arterial pressure, automated reading\n\n\n0.0096\nTrunk fat-free mass\n\n\n0.0096\nPeak expiratory flow (PEF)\n\n\n0.0092\nTrunk predicted mass\n\n\n0.0090\nBasal metabolic rate\n\n\n0.0089\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0071\nArm fat mass (left)\n\n\n0.0069\nAge completed full time education\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 73 (Contribution: 0.1319)\n\n\n\n\n\n0.0213\nI35 Nonrheumatic aortic valve disorders\n\n\n0.0211\nNonrheumatic aortic valve disorders\n\n\n0.0173\nCongenital anomalies of great vessels\n\n\n0.0157\nCardiac congenital anomalies\n\n\n0.0147\nCardiac and circulatory congenital anomalies\n\n\n0.0137\nAlbumin\n\n\n0.0128\nanti-metabolite\n\n\n0.0128\nmethotrexate\n\n\n0.0126\nHeart valve disorders\n\n\n0.0094\nAortic valve disease\n\n\n0.0093\nRheumatoid arthritis\n\n\n0.0090\nRheumatoid arthritis and other inflammatory polyarthropathies\n\n\n0.0080\nOsteoporosis\n\n\n0.0080\nOsteoporosis NOS\n\n\n0.0080\nTreatment/medication code\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(420)\n\n\n\nCancer Of Prostate\n\n\n\nRPCA-IALM\n\n\nTop Factor: 78 (Contribution: 0.0875)\n\n\n\n\n\n0.0272\nContracture of palmar fascia [Dupuytren's disease]\n\n\n0.0262\nDisorders of muscle, ligament, and fascia\n\n\n0.0243\nFasciitis\n\n\n0.0186\nMalignant neoplasm of bladder\n\n\n0.0182\nC67 Malignant neoplasm of bladder\n\n\n0.0157\nCancer of bladder\n\n\n0.0156\nM72 Fibroblastic disorders\n\n\n0.0131\nK76 Other diseases of liver\n\n\n0.0130\nOther disorders of liver\n\n\n0.0127\nSebaceous cyst\n\n\n0.0125\nDiseases of sebaceous glands\n\n\n0.0114\nProstatitis\n\n\n0.0112\nChronic liver disease and cirrhosis\n\n\n0.0110\nL72 Follicular cysts of skin and subcutaneous tissue\n\n\n0.0107\nCancer of urinary organs (incl. kidney and bladder)\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 48 (Contribution: 0.0815)\n\n\n\n\n\n0.0247\nWeight\n\n\n0.0190\nWhole body water mass\n\n\n0.0134\nTrunk fat mass\n\n\n0.0132\nAge at first live birth\n\n\n0.0128\nWeight\n\n\n0.0123\nArm fat-free mass (left)\n\n\n0.0119\nOverweight, obesity and other hyperalimentation\n\n\n0.0112\nBody mass index (BMI)\n\n\n0.0108\nLeg fat percentage (right)\n\n\n0.0089\nHip circumference\n\n\n0.0088\nLeg fat-free mass (right)\n\n\n0.0081\nTrunk predicted mass\n\n\n0.0064\naspirin\n\n\n0.0064\nLeg fat-free mass (left)\n\n\n0.0063\nsodium alginate / potassium bicarbonate\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 54 (Contribution: 0.0663)\n\n\n\n\n\n0.0277\nAlbumin\n\n\n0.0221\nIGF-1\n\n\n0.0168\nCalcium\n\n\n0.0163\nAlkaline phosphatase\n\n\n0.0114\nCorneal resistance factor (left)\n\n\n0.0106\nCorneal resistance factor (right)\n\n\n0.0098\nTotal protein\n\n\n0.0089\nCorneal hysteresis (left)\n\n\n0.0079\nCorneal hysteresis (right)\n\n\n0.0065\nPhosphate\n\n\n0.0065\nHome location - east co-ordinate (rounded)\n\n\n0.0063\nUrate\n\n\n0.0062\nHome location at assessment - east co-ordinate (rounded)\n\n\n0.0060\nOther disorders of bladder\n\n\n0.0056\nAlbumin/Globulin ratio\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(410)\n\n\n\nBreast Cancer\n\n\n\nRPCA-IALM\n\n\nTop Factor: 25 (Contribution: 0.1298)\n\n\n\n\n\n0.0221\nBreast cancer [female]\n\n\n0.0215\nBreast cancer\n\n\n0.0210\nMalignant neoplasm of female breast\n\n\n0.0200\nC50 Malignant neoplasm of breast\n\n\n0.0167\nArthropathy NOS\n\n\n0.0167\nOther arthropathies\n\n\n0.0129\nM17 Gonarthrosis [arthrosis of knee]\n\n\n0.0110\nOsteoarthrosis\n\n\n0.0108\nUnspecified monoarthritis\n\n\n0.0098\nDiseases of esophagus\n\n\n0.0095\nDiaphragmatic hernia\n\n\n0.0093\nOsteoarthritis; localized\n\n\n0.0091\nK44 Diaphragmatic hernia\n\n\n0.0090\nEsophagitis, GERD and related diseases\n\n\n0.0088\nMalignant neoplasm, other\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 66 (Contribution: 0.0840)\n\n\n\n\n\n0.0300\nLeg fat-free mass (right)\n\n\n0.0273\nWhole body fat-free mass\n\n\n0.0264\nSitting height\n\n\n0.0208\nArm fat-free mass (left)\n\n\n0.0128\nImpedance of leg (left)\n\n\n0.0128\nBody mass index (BMI)\n\n\n0.0120\nArm predicted mass (right)\n\n\n0.0109\nWhole body water mass\n\n\n0.0098\nType 2 diabetes\n\n\n0.0097\nWaist circumference\n\n\n0.0097\nArm predicted mass (left)\n\n\n0.0085\nArm fat-free mass (right)\n\n\n0.0081\nE66 Obesity\n\n\n0.0078\nImpedance of leg (right)\n\n\n0.0074\nlansoprazole\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 48 (Contribution: 0.0812)\n\n\n\n\n\n0.0133\nUrate\n\n\n0.0109\nPast tobacco smoking\n\n\n0.0108\nBreast cancer\n\n\n0.0108\nBreast cancer [female]\n\n\n0.0101\nMalignant neoplasm of female breast\n\n\n0.0099\nC50 Malignant neoplasm of breast\n\n\n0.0099\nSmoking status, ever vs never\n\n\n0.0090\nAge first had sexual intercourse\n\n\n0.0086\nAlkaline phosphatase\n\n\n0.0075\nDiseases of esophagus\n\n\n0.0072\nGamma glutamyltransferase\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0070\nEsophagitis, GERD and related diseases\n\n\n0.0067\nDuration to first press of snap-button in each round\n\n\n0.0062\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(458)\n\n\n\nDiabetes Mellitus\n\n\n\nRPCA-IALM\n\n\nTop Factor: 19 (Contribution: 0.1403)\n\n\n\n\n\n0.0364\nRed blood cell (erythrocyte) count\n\n\n0.0189\nGlycated haemoglobin (HbA1c)\n\n\n0.0178\nMean corpuscular volume\n\n\n0.0159\nLymphocyte count\n\n\n0.0150\nMean sphered cell volume\n\n\n0.0146\nHaemoglobin concentration\n\n\n0.0137\nPlatelet count\n\n\n0.0136\nHaematocrit percentage\n\n\n0.0132\nMean corpuscular haemoglobin\n\n\n0.0128\nType 2 diabetes\n\n\n0.0127\nNon-albumin protein\n\n\n0.0124\nE11 Non-insulin-dependent diabetes mellitus\n\n\n0.0119\nDiabetes mellitus\n\n\n0.0108\nPlatelet crit\n\n\n0.0106\nNeutrophill percentage\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 57 (Contribution: 0.0559)\n\n\n\n\n\n0.0178\nArm fat-free mass (left)\n\n\n0.0162\nTrunk fat mass\n\n\n0.0144\nBody mass index (BMI)\n\n\n0.0140\ncodine/paracetamol\n\n\n0.0135\nType 2 diabetes\n\n\n0.0131\nComparative height size at age 10\n\n\n0.0107\ntetracycline antibiotic|antibiotic\n\n\n0.0095\nImpedance of arm (right)\n\n\n0.0092\nHip circumference\n\n\n0.0092\nDiabetes mellitus\n\n\n0.0089\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0082\nACE inhibitor|anti-hypertensive\n\n\n0.0077\nTrunk fat-free mass\n\n\n0.0069\ntrimethoprim\n\n\n0.0069\nWeight\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 26 (Contribution: 0.1503)\n\n\n\n\n\n0.1042\nHDL cholesterol\n\n\n0.0898\nApolipoprotein A\n\n\n0.0473\nGlycated haemoglobin (HbA1c)\n\n\n0.0273\nAlbumin/Globulin ratio\n\n\n0.0231\nNon-albumin protein\n\n\n0.0208\nPlatelet crit\n\n\n0.0192\nNeutrophill percentage\n\n\n0.0175\nNeutrophill count\n\n\n0.0154\nLymphocyte percentage\n\n\n0.0138\nIGF-1\n\n\n0.0131\nGamma glutamyltransferase\n\n\n0.0112\nType 2 diabetes\n\n\n0.0111\nDiabetes mellitus\n\n\n0.0111\nE11 Non-insulin-dependent diabetes mellitus\n\n\n0.0109\nMean platelet (thrombocyte) volume\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(529)\n\n\n\nParkinson’s Disease\n\n\n\nRPCA-IALM\n\n\nTop Factor: 97 (Contribution: 0.0888)\n\n\n\n\n\n0.0441\nPsoriasis\n\n\n0.0425\nL40 Psoriasis\n\n\n0.0403\nPsoriasis and related disorders\n\n\n0.0187\nCardiac arrest\n\n\n0.0184\nCardiac arrest and ventricular fibrillation\n\n\n0.0181\nI46 Cardiac arrest\n\n\n0.0117\nK90 Intestinal malabsorption\n\n\n0.0114\nIntestinal malabsorption (non-celiac)\n\n\n0.0109\nObesity\n\n\n0.0108\nE66 Obesity\n\n\n0.0097\nOverweight, obesity and other hyperalimentation\n\n\n0.0095\nAortic aneurysm\n\n\n0.0094\nI71 Aortic aneurysm and dissection\n\n\n0.0090\nCeliac disease\n\n\n0.0085\nNoninflammatory disorders of ovary, fallopian tube, and broad ligament\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 70 (Contribution: 0.0852)\n\n\n\n\n\n0.0262\nTrunk fat percentage\n\n\n0.0240\nWhole body fat-free mass\n\n\n0.0207\nLeg predicted mass (left)\n\n\n0.0201\nArm fat-free mass (left)\n\n\n0.0172\nLeg fat percentage (left)\n\n\n0.0147\nLeg fat mass (right)\n\n\n0.0105\nLeg fat mass (left)\n\n\n0.0083\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0079\nUsual walking pace\n\n\n0.0078\nproton pump inhibitor|PPI\n\n\n0.0073\nImpedance of leg (left)\n\n\n0.0071\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0065\nArm fat mass (right)\n\n\n0.0061\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0059\ncodine/paracetamol\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 92 (Contribution: 0.1208)\n\n\n\n\n\n0.0149\nUrinary calculus\n\n\n0.0149\nN20 Calculus of kidney and ureter\n\n\n0.0147\nMaximum heart rate during fitness test\n\n\n0.0146\nECG, load\n\n\n0.0146\nMaximum workload during fitness test\n\n\n0.0139\nPulse rate, automated reading\n\n\n0.0138\nECG, heart rate\n\n\n0.0130\nECG, number of stages in a phase\n\n\n0.0126\nNumber of trend entries\n\n\n0.0117\nECG, phase time\n\n\n0.0110\nCalculus of ureter\n\n\n0.0102\nProcessed meat intake\n\n\n0.0101\nCalculus of kidney\n\n\n0.0088\nBeef intake\n\n\n0.0080\nLamb/mutton intake\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(504)\n\n\n\nDementias\n\n\n\nRPCA-IALM\n\n\nTop Factor: 99 (Contribution: 0.1643)\n\n\n\n\n\n0.0148\nCardiac arrest\n\n\n0.0146\nCardiac arrest and ventricular fibrillation\n\n\n0.0144\nI46 Cardiac arrest\n\n\n0.0130\nOther disorders of pancreatic internal secretion\n\n\n0.0125\nHypoglycemia\n\n\n0.0122\nE16 Other disorders of pancreatic internal secretion\n\n\n0.0112\nObesity\n\n\n0.0111\nE66 Obesity\n\n\n0.0104\nE21 Hyperparathyroidism and other disorders of parathyroid gland\n\n\n0.0104\nG47 Sleep disorders\n\n\n0.0103\nHyperparathyroidism\n\n\n0.0101\nSleep disorders\n\n\n0.0101\nOverweight, obesity and other hyperalimentation\n\n\n0.0090\nVaricose veins of lower extremity\n\n\n0.0088\nDisorders of parathyroid gland\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 74 (Contribution: 0.0601)\n\n\n\n\n\n0.0203\nNumber of self-reported non-cancer illnesses\n\n\n0.0169\nLeg predicted mass (left)\n\n\n0.0162\nWaist circumference\n\n\n0.0160\nHip circumference\n\n\n0.0139\nMean arterial pressure, combined automated + manual reading\n\n\n0.0136\nArm fat mass (left)\n\n\n0.0130\nDiastolic blood pressure, combined automated + manual reading\n\n\n0.0129\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0129\nArm fat-free mass (left)\n\n\n0.0128\nSystolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0128\nImpedance of arm (left)\n\n\n0.0109\nEssential hypertension\n\n\n0.0106\nBody mass index (BMI)\n\n\n0.0098\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0092\nPeak expiratory flow (PEF)\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 61 (Contribution: 0.0862)\n\n\n\n\n\n0.0324\nHeel bone mineral density (BMD)\n\n\n0.0322\nHeel bone mineral density (BMD) T-score, automated\n\n\n0.0322\nHeel quantitative ultrasound index (QUI), direct entry\n\n\n0.0258\nHeel Broadband ultrasound attenuation, direct entry\n\n\n0.0139\nUrate\n\n\n0.0119\nHeel bone mineral density (BMD) T-score, automated (left)\n\n\n0.0119\nHeel quantitative ultrasound index (QUI), direct entry (left)\n\n\n0.0119\nHeel broadband ultrasound attenuation (left)\n\n\n0.0116\nHeel bone mineral density (BMD) (left)\n\n\n0.0115\nHeel bone mineral density (BMD) T-score, automated (right)\n\n\n0.0115\nHeel quantitative ultrasound index (QUI), direct entry (right)\n\n\n0.0115\nHeel broadband ultrasound attenuation (right)\n\n\n0.0114\nHeel bone mineral density (BMD) (right)\n\n\n0.0103\nCerebrovascular disease\n\n\n0.0102\nCreatinine\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(61)\n\n\n\nG47 Sleep Disorders\n\n\n\nRPCA-IALM\n\n\nTop Factor: 86 (Contribution: 0.1228)\n\n\n\n\n\n0.0469\nAcquired foot deformities\n\n\n0.0468\nM20 Acquired deformities of fingers and toes\n\n\n0.0290\nHemoptysis\n\n\n0.0286\nR04 Haemorrhage from respiratory passages\n\n\n0.0275\nObesity\n\n\n0.0272\nE66 Obesity\n\n\n0.0272\nHallux valgus (Bunion)\n\n\n0.0253\nAcquired toe deformities\n\n\n0.0248\nAbnormal sputum\n\n\n0.0247\nOverweight, obesity and other hyperalimentation\n\n\n0.0123\nG47 Sleep disorders\n\n\n0.0119\nSleep disorders\n\n\n0.0111\nHammer toe (acquired)\n\n\n0.0110\nEpistaxis or throat hemorrhage\n\n\n0.0095\nProstatitis\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 43 (Contribution: 0.0683)\n\n\n\n\n\n0.0234\nACE inhibitor|anti-hypertensive\n\n\n0.0141\nHigh light scatter reticulocyte count\n\n\n0.0133\nUsual walking pace\n\n\n0.0121\nWeight\n\n\n0.0119\nLeg predicted mass (left)\n\n\n0.0109\nTrunk fat-free mass\n\n\n0.0104\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0100\nReticulocyte count\n\n\n0.0092\nTrunk fat percentage\n\n\n0.0091\nWaist circumference\n\n\n0.0081\nAge first had sexual intercourse\n\n\n0.0077\nSeated height\n\n\n0.0066\nTreatment/medication code\n\n\n0.0064\nOther arthropathies\n\n\n0.0061\nAnkle spacing width (right)\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 91 (Contribution: 0.1037)\n\n\n\n\n\n0.0136\nAspartate aminotransferase\n\n\n0.0125\nPlatelet distribution width\n\n\n0.0112\nAlanine aminotransferase\n\n\n0.0084\nInflammatory bowel disease and other gastroenteritis and colitis\n\n\n0.0074\nSpherical power (right)\n\n\n0.0070\nGenital prolapse\n\n\n0.0070\nProlapse of vaginal walls\n\n\n0.0069\nSpherical power (left)\n\n\n0.0068\nN81 Female genital prolapse\n\n\n0.0068\nOpen wounds of head; neck; and trunk\n\n\n0.0063\nC79 Secondary malignant neoplasm of other sites\n\n\n0.0061\nE87 Other disorders of fluid, electrolyte and acid-base balance\n\n\n0.0061\nUlcerative colitis\n\n\n0.0061\nElectrolyte imbalance\n\n\n0.0061\nI77 Other disorders of arteries and arterioles\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(59)\n\n\n\nG43 Migraine\n\n\n\nRPCA-IALM\n\n\nTop Factor: 80 (Contribution: 0.0792)\n\n\n\n\n\n0.0312\nAppendicitis\n\n\n0.0307\nAcute appendicitis\n\n\n0.0287\nAppendiceal conditions\n\n\n0.0284\nK35 Acute appendicitis\n\n\n0.0208\nSebaceous cyst\n\n\n0.0203\nDiseases of sebaceous glands\n\n\n0.0186\nL72 Follicular cysts of skin and subcutaneous tissue\n\n\n0.0140\nContracture of palmar fascia [Dupuytren's disease]\n\n\n0.0134\nDisorders of muscle, ligament, and fascia\n\n\n0.0133\nIron deficiency anemias, unspecified or not due to blood loss\n\n\n0.0133\nIron deficiency anemias\n\n\n0.0121\nFasciitis\n\n\n0.0118\nD50 Iron deficiency anaemia\n\n\n0.0115\nHome location - north co-ordinate (rounded)\n\n\n0.0113\nHome location at assessment - north co-ordinate (rounded)\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 46 (Contribution: 0.0576)\n\n\n\n\n\n0.0320\nLeg predicted mass (right)\n\n\n0.0257\nWhole body fat-free mass\n\n\n0.0243\nLeg fat-free mass (right)\n\n\n0.0192\nBody mass index (BMI)\n\n\n0.0167\nLeg fat mass (left)\n\n\n0.0144\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0120\nsimvastatin\n\n\n0.0112\nthiazide diuretic|diuretic\n\n\n0.0109\naspirin\n\n\n0.0092\nlansoprazole\n\n\n0.0091\nACE inhibitor|anti-hypertensive\n\n\n0.0089\nDiastolic blood pressure, automated reading\n\n\n0.0085\nNon-cancer illness code, self-reported\n\n\n0.0070\nbendroflumethiazide\n\n\n0.0070\namoxicillin\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 56 (Contribution: 0.0733)\n\n\n\n\n\n0.0227\nAlkaline phosphatase\n\n\n0.0134\nRed blood cell (erythrocyte) distribution width\n\n\n0.0104\nAlbumin\n\n\n0.0079\nAlanine aminotransferase\n\n\n0.0077\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0072\nOther disorders of bladder\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0071\nOther retinal disorders\n\n\n0.0065\nCystatin C\n\n\n0.0064\nN32 Other disorders of bladder\n\n\n0.0062\nCalcium\n\n\n0.0061\nGamma glutamyltransferase\n\n\n0.0057\nDiabetic retinopathy\n\n\n0.0057\nHereditary retinal dystrophies\n\n\n0.0057\nH36 Retinal disorders in diseases classified elsewhere\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(591)\n\n\n\nHypertension\n\n\n\nRPCA-IALM\n\n\nTop Factor: 2 (Contribution: 0.0982)\n\n\n\n\n\n0.0595\nStanding height\n\n\n0.0290\nSitting height\n\n\n0.0227\nSeated height\n\n\n0.0197\nForced vital capacity (FVC)\n\n\n0.0193\nForced vital capacity (FVC), Best measure\n\n\n0.0169\nMean arterial pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0163\nSystolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0157\nComparative height size at age 10\n\n\n0.0157\nForced expiratory volume in 1-second (FEV1)\n\n\n0.0156\nMean arterial pressure, automated reading, adjusted by medication\n\n\n0.0152\nForced expiratory volume in 1-second (FEV1), predicted\n\n\n0.0150\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0149\nSystolic blood pressure, automated reading, adjusted by medication\n\n\n0.0138\nTrunk fat-free mass\n\n\n0.0138\nTrunk predicted mass\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 15 (Contribution: 0.1904)\n\n\n\n\n\n0.0670\nEssential hypertension\n\n\n0.0596\nHypertension\n\n\n0.0278\nE78 Disorders of lipoprotein metabolism and other lipidaemias\n\n\n0.0271\nDisorders of lipoid metabolism\n\n\n0.0265\nHypercholesterolemia\n\n\n0.0256\nHyperlipidemia\n\n\n0.0252\nIschemic Heart Disease\n\n\n0.0197\nOther chronic ischemic heart disease, unspecified\n\n\n0.0174\nI25 Chronic ischaemic heart disease\n\n\n0.0138\nantipyretic\n\n\n0.0138\nparacetamol\n\n\n0.0134\nopioid analgesic|antipyretic\n\n\n0.0130\nAngina pectoris\n\n\n0.0111\nE11 Non-insulin-dependent diabetes mellitus\n\n\n0.0100\nLeg fat mass (left)\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 3 (Contribution: 0.1210)\n\n\n\n\n\n0.0461\nSystolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0425\nSystolic blood pressure, automated reading, adjusted by medication\n\n\n0.0405\nMean arterial pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0373\nMean arterial pressure, automated reading, adjusted by medication\n\n\n0.0353\nSystolic blood pressure, combined automated + manual reading\n\n\n0.0334\nSystolic blood pressure, automated reading\n\n\n0.0304\nPulse pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0288\nMean arterial pressure, combined automated + manual reading\n\n\n0.0282\nPulse pressure, automated reading, adjusted by medication\n\n\n0.0277\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0274\nMean arterial pressure, automated reading\n\n\n0.0261\nDiastolic blood pressure, automated reading, adjusted by medication\n\n\n0.0246\nPulse pressure, combined automated + manual reading\n\n\n0.0235\nPulse pressure, automated reading\n\n\n0.0180\nDiastolic blood pressure, combined automated + manual reading\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(2350)\n\n\n\nStanding Height\n\n\n\nRPCA-IALM\n\n\nTop Factor: 7 (Contribution: 0.2952)\n\n\n\n\n\n0.0748\nImpedance of whole body\n\n\n0.0676\nStanding height\n\n\n0.0580\nImpedance of arm (left)\n\n\n0.0572\nImpedance of arm (right)\n\n\n0.0569\nImpedance of leg (left)\n\n\n0.0559\nImpedance of leg (right)\n\n\n0.0233\nSitting height\n\n\n0.0217\nSeated height\n\n\n0.0181\nComparative height size at age 10\n\n\n0.0144\nForced expiratory volume in 1-second (FEV1), predicted\n\n\n0.0132\nTrunk fat percentage\n\n\n0.0101\nBody mass index (BMI)\n\n\n0.0098\nBody mass index (BMI)\n\n\n0.0095\nReticulocyte count\n\n\n0.0094\nHeel quantitative ultrasound index (QUI), direct entry (right)\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 4 (Contribution: 0.1740)\n\n\n\n\n\n0.1274\nStanding height\n\n\n0.0721\nSitting height\n\n\n0.0263\nSeated height\n\n\n0.0192\nComparative height size at age 10\n\n\n0.0190\nTrunk fat-free mass\n\n\n0.0189\nTrunk predicted mass\n\n\n0.0134\nWhole body fat-free mass\n\n\n0.0117\nWhole body water mass\n\n\n0.0114\nWaist circumference\n\n\n0.0112\nLeg fat mass (left)\n\n\n0.0112\nArm fat mass (right)\n\n\n0.0107\nLeg fat mass (right)\n\n\n0.0101\nArm fat mass (left)\n\n\n0.0099\nE66 Obesity\n\n\n0.0099\nObesity\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 2 (Contribution: 0.2623)\n\n\n\n\n\n0.0640\nStanding height\n\n\n0.0351\nSitting height\n\n\n0.0250\nSeated height\n\n\n0.0206\nForced vital capacity (FVC)\n\n\n0.0197\nForced vital capacity (FVC), Best measure\n\n\n0.0186\nComparative height size at age 10\n\n\n0.0170\nForced expiratory volume in 1-second (FEV1), predicted\n\n\n0.0161\nForced expiratory volume in 1-second (FEV1)\n\n\n0.0152\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0132\nMean arterial pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0126\nSystolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0121\nMean arterial pressure, automated reading, adjusted by medication\n\n\n0.0120\nLeg fat percentage (left)\n\n\n0.0120\nLeg fat percentage (right)\n\n\n0.0118\nTrunk fat-free mass\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(1747)\n\n\n\nIntake Of Sugar Added To Coffee\n\n\n\nRPCA-IALM\n\n\nTop Factor: 52 (Contribution: 0.1127)\n\n\n\n\n\n0.0152\nSaturated fat\n\n\n0.0138\nAlbumin\n\n\n0.0124\nFat\n\n\n0.0108\nOsteoporosis\n\n\n0.0108\nOsteoporosis NOS\n\n\n0.0093\nVitamin C\n\n\n0.0086\nHome location - north co-ordinate (rounded)\n\n\n0.0084\nmethotrexate\n\n\n0.0084\nanti-metabolite\n\n\n0.0081\nRetinol\n\n\n0.0079\nHome location at assessment - north co-ordinate (rounded)\n\n\n0.0072\nCarotene\n\n\n0.0071\nOsteoporosis, osteopenia and pathological fracture\n\n\n0.0069\nxanthine oxidase inhibitor|anti-gout agent\n\n\n0.0069\nallopurinol\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 54 (Contribution: 0.0723)\n\n\n\n\n\n0.0427\nWhole body fat-free mass\n\n\n0.0224\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0193\nLeg fat mass (right)\n\n\n0.0167\nUsual walking pace\n\n\n0.0119\nZ86 Personal history of certain other diseases\n\n\n0.0118\nWeight\n\n\n0.0100\nclarithromycin\n\n\n0.0099\nArm predicted mass (left)\n\n\n0.0085\nWhole body fat mass\n\n\n0.0071\nPulse pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0066\nLeg fat mass (left)\n\n\n0.0065\nlansoprazole\n\n\n0.0063\naspirin\n\n\n0.0063\ndiclofenac\n\n\n0.0061\nOther mental disorder\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 75 (Contribution: 0.0839)\n\n\n\n\n\n0.0341\nSHBG\n\n\n0.0298\nPulse rate, automated reading\n\n\n0.0129\nPulse rate\n\n\n0.0122\nTestosterone\n\n\n0.0118\nCOVID-19 positive (controls include untested)\n\n\n0.0118\nCOVID-19 positive (controls include untested), only patients from centers in England\n\n\n0.0113\nCOVID-19 positive (controls include untested)\n\n\n0.0112\nCOVID-19 positive (controls include untested), only patients from centers in England\n\n\n0.0099\nCOVID-19 positive (controls only COVID-19 negative)\n\n\n0.0097\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0096\nCOVID-19 positive (controls only COVID-19 negative)\n\n\n0.0090\nCystatin C\n\n\n0.0090\nFasciitis\n\n\n0.0089\nContracture of palmar fascia [Dupuytren's disease]\n\n\n0.0088\nDisorders of muscle, ligament, and fascia\n\n\n\n\n\n\n\nCode\npan_ukb_pleiotropy(1677)\n\n\n\nCovid-19 Positive (Controls Only Covid-19 Negative)\n\n\n\nRPCA-IALM\n\n\nTop Factor: 42 (Contribution: 0.4360)\n\n\n\n\n\n0.0801\nCOVID-19 positive (controls include untested)\n\n\n0.0800\nCOVID-19 positive (controls include untested), only patients from centers in England\n\n\n0.0792\nCOVID-19 positive (controls include untested)\n\n\n0.0791\nCOVID-19 positive (controls include untested), only patients from centers in England\n\n\n0.0557\nCOVID-19 positive (controls only COVID-19 negative)\n\n\n0.0498\nCOVID-19 positive (controls only COVID-19 negative)\n\n\n0.0135\nCholelithiasis\n\n\n0.0132\nCholelithiasis and cholecystitis\n\n\n0.0130\nK80 Cholelithiasis\n\n\n0.0094\nChronic dermatitis due to solar radiation\n\n\n0.0093\nL57 Skin changes due to chronic exposure to nonionising radiation\n\n\n0.0082\nDermatitis due to solar radiation\n\n\n0.0069\nActinic keratosis\n\n\n0.0068\nBreast cancer [female]\n\n\n0.0066\nBreast cancer\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 47 (Contribution: 0.0847)\n\n\n\n\n\n0.0245\nAge first had sexual intercourse\n\n\n0.0170\nForced expiratory volume in 1-second (FEV1), predicted\n\n\n0.0135\nACE inhibitor|anti-hypertensive\n\n\n0.0126\nbeta-lactam antibiotic|antibiotic\n\n\n0.0115\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0115\nWhole body fat-free mass\n\n\n0.0112\nArm fat-free mass (left)\n\n\n0.0096\nTrunk fat percentage\n\n\n0.0093\nWaist circumference\n\n\n0.0087\nLeg predicted mass (left)\n\n\n0.0085\nImpedance of leg (right)\n\n\n0.0081\nLeg fat-free mass (left)\n\n\n0.0070\nAnkle spacing width\n\n\n0.0069\nBody fat percentage\n\n\n0.0059\nCholesterol\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 67 (Contribution: 0.5227)\n\n\n\n\n\n0.0846\nCOVID-19 positive (controls include untested), only patients from centers in England\n\n\n0.0846\nCOVID-19 positive (controls include untested)\n\n\n0.0844\nCOVID-19 positive (controls include untested), only patients from centers in England\n\n\n0.0843\nCOVID-19 positive (controls include untested)\n\n\n0.0765\nCOVID-19 positive (controls only COVID-19 negative)\n\n\n0.0720\nCOVID-19 positive (controls only COVID-19 negative)\n\n\n0.0317\nIGF-1\n\n\n0.0114\nPlatelet distribution width\n\n\n0.0088\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0069\nCystatin C\n\n\n0.0047\nSystolic blood pressure, manual reading\n\n\n0.0044\nSystolic blood pressure, manual reading, adjusted by medication\n\n\n0.0040\nMean arterial pressure, manual reading\n\n\n0.0038\nEstimated glomerular filtration rate, serum creatinine\n\n\n0.0037\nCreatinine\n\n\n\n\n\n\n\nNeuropsychiatric Disorders\n\n\nCode\nfor z in range(52,58):\n    pan_ukb_pleiotropy(z)\n\n\n\nF05 Delirium, Not Induced By Alcohol And Other Psychoactive Substances\n\n\n\nRPCA-IALM\n\n\nTop Factor: 99 (Contribution: 0.1305)\n\n\n\n\n\n0.0148\nCardiac arrest\n\n\n0.0146\nCardiac arrest and ventricular fibrillation\n\n\n0.0144\nI46 Cardiac arrest\n\n\n0.0130\nOther disorders of pancreatic internal secretion\n\n\n0.0125\nHypoglycemia\n\n\n0.0122\nE16 Other disorders of pancreatic internal secretion\n\n\n0.0112\nObesity\n\n\n0.0111\nE66 Obesity\n\n\n0.0104\nE21 Hyperparathyroidism and other disorders of parathyroid gland\n\n\n0.0104\nG47 Sleep disorders\n\n\n0.0103\nHyperparathyroidism\n\n\n0.0101\nSleep disorders\n\n\n0.0101\nOverweight, obesity and other hyperalimentation\n\n\n0.0090\nVaricose veins of lower extremity\n\n\n0.0088\nDisorders of parathyroid gland\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 28 (Contribution: 0.0542)\n\n\n\n\n\n0.0267\nWaist circumference\n\n\n0.0242\nBody fat percentage\n\n\n0.0220\nE66 Obesity\n\n\n0.0212\nObesity\n\n\n0.0160\nArm fat mass (left)\n\n\n0.0151\nTrunk fat percentage\n\n\n0.0128\nLeg fat percentage (left)\n\n\n0.0121\nLeg fat percentage (right)\n\n\n0.0117\npheno 48 / pheno 49\n\n\n0.0112\nDisorders of lipoid metabolism\n\n\n0.0108\nForced expiratory volume in 1-second (FEV1), predicted\n\n\n0.0107\nACE inhibitor|anti-hypertensive\n\n\n0.0104\nTriglycerides\n\n\n0.0092\nOverweight, obesity and other hyperalimentation\n\n\n0.0091\nLeg fat-free mass (right)\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 33 (Contribution: 0.0945)\n\n\n\n\n\n0.1299\nMonocyte percentage\n\n\n0.0755\nMonocyte count\n\n\n0.0237\nLymphocyte count\n\n\n0.0195\nEosinophill count\n\n\n0.0162\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0155\nEosinophill percentage\n\n\n0.0154\nForced expiratory volume in 1-second (FEV1)\n\n\n0.0152\nLymphocyte percentage\n\n\n0.0132\nHDL cholesterol\n\n\n0.0119\nForced expiratory volume in 1-second (FEV1), predicted percentage\n\n\n0.0116\nApolipoprotein A\n\n\n0.0102\nForced vital capacity (FVC), Best measure\n\n\n0.0099\nForced vital capacity (FVC)\n\n\n0.0082\nNon-cancer illness code, self-reported\n\n\n0.0075\nFEV1/FVC ratio\n\n\n\n\n\n\nF10 Mental And Behavioural Disorders Due To Use Of Alcohol\n\n\n\nRPCA-IALM\n\n\nTop Factor: 94 (Contribution: 0.0704)\n\n\n\n\n\n0.0258\nIntra-ocular pressure, corneal-compensated (right)\n\n\n0.0234\nIntra-ocular pressure, corneal-compensated (left)\n\n\n0.0195\nHypotension NOS\n\n\n0.0195\nI95 Hypotension\n\n\n0.0191\nH40 Glaucoma\n\n\n0.0180\nGlaucoma\n\n\n0.0178\nOpen-angle glaucoma\n\n\n0.0176\nPrimary open angle glaucoma\n\n\n0.0155\nHypotension\n\n\n0.0132\nCorneal hysteresis (right)\n\n\n0.0124\nIntra-ocular pressure, Goldmann-correlated (right)\n\n\n0.0124\nCorneal hysteresis (left)\n\n\n0.0111\nIntra-ocular pressure, Goldmann-correlated (left)\n\n\n0.0110\nInguinal hernia\n\n\n0.0102\nK40 Inguinal hernia\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 32 (Contribution: 0.1297)\n\n\n\n\n\n0.0387\nTrunk fat mass\n\n\n0.0221\nTrunk fat-free mass\n\n\n0.0160\nWhole body fat mass\n\n\n0.0147\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0113\nAnkle spacing width\n\n\n0.0108\nI25 Chronic ischaemic heart disease\n\n\n0.0106\nMulti-site chronic pain\n\n\n0.0102\nComparative height size at age 10\n\n\n0.0098\nLeg predicted mass (right)\n\n\n0.0094\nDrive faster than motorway speed limit\n\n\n0.0092\nHand grip strength (left)\n\n\n0.0089\nibuprofen\n\n\n0.0087\nImpedance of leg (right)\n\n\n0.0078\nHip circumference\n\n\n0.0078\nDiastolic blood pressure, combined automated + manual reading\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 56 (Contribution: 0.1128)\n\n\n\n\n\n0.0227\nAlkaline phosphatase\n\n\n0.0134\nRed blood cell (erythrocyte) distribution width\n\n\n0.0104\nAlbumin\n\n\n0.0079\nAlanine aminotransferase\n\n\n0.0077\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0072\nOther disorders of bladder\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0071\nOther retinal disorders\n\n\n0.0065\nCystatin C\n\n\n0.0064\nN32 Other disorders of bladder\n\n\n0.0062\nCalcium\n\n\n0.0061\nGamma glutamyltransferase\n\n\n0.0057\nDiabetic retinopathy\n\n\n0.0057\nHereditary retinal dystrophies\n\n\n0.0057\nH36 Retinal disorders in diseases classified elsewhere\n\n\n\n\n\n\nF17 Mental And Behavioural Disorders Due To Use Of Tobacco\n\n\n\nRPCA-IALM\n\n\nTop Factor: 51 (Contribution: 0.0921)\n\n\n\n\n\n0.0368\nAlbumin/Globulin ratio\n\n\n0.0319\nNon-albumin protein\n\n\n0.0142\nLymphocyte count\n\n\n0.0085\nTotal protein\n\n\n0.0073\nLymphocyte percentage\n\n\n0.0072\nAlbumin\n\n\n0.0069\nNeutrophill percentage\n\n\n0.0067\nPlatelet count\n\n\n0.0063\nPast tobacco smoking\n\n\n0.0062\nSmoking status, ever vs never\n\n\n0.0062\nForced expiratory volume in 1-second (FEV1)\n\n\n0.0057\nCystitis\n\n\n0.0055\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0053\nAcute pulmonary heart disease\n\n\n0.0053\nPulmonary embolism and infarction, acute\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 5 (Contribution: 0.0615)\n\n\n\n\n\n0.0297\nMean arterial pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0277\nMean arterial pressure, automated reading, adjusted by medication\n\n\n0.0277\nSystolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0263\nSystolic blood pressure, automated reading, adjusted by medication\n\n\n0.0218\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0211\nDiastolic blood pressure, automated reading, adjusted by medication\n\n\n0.0198\nMean arterial pressure, combined automated + manual reading\n\n\n0.0194\nSystolic blood pressure, combined automated + manual reading\n\n\n0.0187\nMean arterial pressure, automated reading\n\n\n0.0183\nSystolic blood pressure, automated reading\n\n\n0.0139\nDiastolic blood pressure, combined automated + manual reading\n\n\n0.0137\nNon-cancer illness code, self-reported\n\n\n0.0127\nDiastolic blood pressure, automated reading\n\n\n0.0116\nPulse pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0109\nPulse pressure, automated reading, adjusted by medication\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 38 (Contribution: 0.0717)\n\n\n\n\n\n0.0276\nCorneal resistance factor (right)\n\n\n0.0261\nCorneal resistance factor (left)\n\n\n0.0205\nUrate\n\n\n0.0165\nLipoprotein A\n\n\n0.0159\nRed blood cell (erythrocyte) distribution width\n\n\n0.0155\nIntra-ocular pressure, Goldmann-correlated (left)\n\n\n0.0151\nAlkaline phosphatase\n\n\n0.0150\nIntra-ocular pressure, Goldmann-correlated (right)\n\n\n0.0148\nCorneal hysteresis (right)\n\n\n0.0137\nCorneal hysteresis (left)\n\n\n0.0133\nNon-cancer illness code, self-reported\n\n\n0.0112\nHypothyroidism NOS\n\n\n0.0111\nHypothyroidism\n\n\n0.0110\nE03 Other hypothyroidism\n\n\n0.0106\nSHBG\n\n\n\n\n\n\nF31 Bipolar Affective Disorder\n\n\n\nRPCA-IALM\n\n\nTop Factor: 92 (Contribution: 0.1087)\n\n\n\n\n\n0.0288\nVaricose veins of lower extremity\n\n\n0.0265\nVaricose veins\n\n\n0.0247\nI83 Varicose veins of lower extremities\n\n\n0.0180\nAcquired foot deformities\n\n\n0.0176\nM20 Acquired deformities of fingers and toes\n\n\n0.0154\nOther non-epithelial cancer of skin\n\n\n0.0148\nC44 Other malignant neoplasms of skin\n\n\n0.0143\nSkin cancer\n\n\n0.0118\nAortic aneurysm\n\n\n0.0117\nI71 Aortic aneurysm and dissection\n\n\n0.0105\nR06 Abnormalities of breathing\n\n\n0.0103\nHallux valgus (Bunion)\n\n\n0.0102\nRespiratory abnormalities\n\n\n0.0091\nMacular degeneration (senile) of retina NOS\n\n\n0.0090\nDegeneration of macula and posterior pole of retina\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 23 (Contribution: 0.0730)\n\n\n\n\n\n0.0272\nE66 Obesity\n\n\n0.0271\nObesity\n\n\n0.0186\nOverweight, obesity and other hyperalimentation\n\n\n0.0160\nBody mass index (BMI)\n\n\n0.0137\nAnkle spacing width (right)\n\n\n0.0113\nStanding height\n\n\n0.0086\nLeg fat percentage (right)\n\n\n0.0085\nAnkle spacing width (left)\n\n\n0.0075\nWeight\n\n\n0.0074\nPulse pressure, automated reading, adjusted by medication\n\n\n0.0074\nPulse pressure, automated reading\n\n\n0.0074\nPulse pressure, combined automated + manual reading\n\n\n0.0065\nproton pump inhibitor|PPI\n\n\n0.0063\nWeight\n\n\n0.0062\ncalcium channel blocker|dihydropyridine|anti-hypertensive\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 45 (Contribution: 0.0787)\n\n\n\n\n\n0.0188\nMean carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0180\nMean carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0177\nLipoprotein A\n\n\n0.0166\nMean carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0164\nMean carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0164\nMaximum carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0158\nMaximum carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0155\nMinimum carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0145\nMinimum carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0138\nMinimum carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0138\nMaximum carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0136\nMaximum carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0132\nMinimum carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0076\nUrate\n\n\n0.0068\nDisorders of menstruation and other abnormal bleeding from female genital tract\n\n\n\n\n\n\nF32 Depressive Episode\n\n\n\nRPCA-IALM\n\n\nTop Factor: 50 (Contribution: 0.1116)\n\n\n\n\n\n0.0225\nI35 Nonrheumatic aortic valve disorders\n\n\n0.0224\nNonrheumatic aortic valve disorders\n\n\n0.0149\nLate effects of cerebrovascular disease\n\n\n0.0145\nI69 Sequelae of cerebrovascular disease\n\n\n0.0142\nCerebrovascular disease\n\n\n0.0142\nCongenital anomalies of great vessels\n\n\n0.0131\nCardiac congenital anomalies\n\n\n0.0123\nSuicide or self-inflicted injury\n\n\n0.0119\nHeart valve disorders\n\n\n0.0117\nX61 Intentional self-poisoning by and exposure to antiepileptic, sedative-hypnotic, anti-Parkinsonism and psychotropic drugs, not elsewhere classified\n\n\n0.0117\nHemiplegia\n\n\n0.0115\nG81 Hemiplegia\n\n\n0.0114\nSuicidal ideation or attempt\n\n\n0.0114\nT51 Toxic effect of alcohol\n\n\n0.0114\nToxic effect of (non-ethyl) alcohol and petroleum and other solvents\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 5 (Contribution: 0.0612)\n\n\n\n\n\n0.0297\nMean arterial pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0277\nMean arterial pressure, automated reading, adjusted by medication\n\n\n0.0277\nSystolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0263\nSystolic blood pressure, automated reading, adjusted by medication\n\n\n0.0218\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0211\nDiastolic blood pressure, automated reading, adjusted by medication\n\n\n0.0198\nMean arterial pressure, combined automated + manual reading\n\n\n0.0194\nSystolic blood pressure, combined automated + manual reading\n\n\n0.0187\nMean arterial pressure, automated reading\n\n\n0.0183\nSystolic blood pressure, automated reading\n\n\n0.0139\nDiastolic blood pressure, combined automated + manual reading\n\n\n0.0137\nNon-cancer illness code, self-reported\n\n\n0.0127\nDiastolic blood pressure, automated reading\n\n\n0.0116\nPulse pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0109\nPulse pressure, automated reading, adjusted by medication\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 45 (Contribution: 0.0855)\n\n\n\n\n\n0.0188\nMean carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0180\nMean carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0177\nLipoprotein A\n\n\n0.0166\nMean carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0164\nMean carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0164\nMaximum carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0158\nMaximum carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0155\nMinimum carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0145\nMinimum carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0138\nMinimum carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0138\nMaximum carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0136\nMaximum carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0132\nMinimum carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0076\nUrate\n\n\n0.0068\nDisorders of menstruation and other abnormal bleeding from female genital tract\n\n\n\n\n\n\nF41 Other Anxiety Disorders\n\n\n\nRPCA-IALM\n\n\nTop Factor: 100 (Contribution: 0.0925)\n\n\n\n\n\n0.0203\nVaricose veins of lower extremity\n\n\n0.0185\nVaricose veins\n\n\n0.0172\nI83 Varicose veins of lower extremities\n\n\n0.0169\nIndirect bilirubin\n\n\n0.0165\nTotal bilirubin\n\n\n0.0130\nDirect bilirubin\n\n\n0.0102\nBacterial pneumonia\n\n\n0.0102\nPsoriasis\n\n\n0.0099\nPneumococcal pneumonia\n\n\n0.0097\nMonocyte percentage\n\n\n0.0096\nL40 Psoriasis\n\n\n0.0095\nPneumonia\n\n\n0.0092\nJ18 Pneumonia, organism unspecified\n\n\n0.0092\nPsoriasis and related disorders\n\n\n0.0078\nCardiac arrest\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 93 (Contribution: 0.0847)\n\n\n\n\n\n0.0244\nImpedance of arm (right)\n\n\n0.0191\nTrunk fat mass\n\n\n0.0179\nE11 Non-insulin-dependent diabetes mellitus\n\n\n0.0148\nImpedance of leg (right)\n\n\n0.0123\nType 2 diabetes\n\n\n0.0104\nLeg fat-free mass (left)\n\n\n0.0102\nLeg fat mass (left)\n\n\n0.0100\nLeg fat mass (right)\n\n\n0.0100\ndoxycycline\n\n\n0.0100\nWhole body water mass\n\n\n0.0090\nBody mass index (BMI)\n\n\n0.0087\nTrunk fat percentage\n\n\n0.0086\ntetracycline antibiotic|antibiotic\n\n\n0.0081\nramipril\n\n\n0.0077\nCystatin C\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 53 (Contribution: 0.0682)\n\n\n\n\n\n0.0102\nCorneal resistance factor (right)\n\n\n0.0096\nX61 Intentional self-poisoning by and exposure to antiepileptic, sedative-hypnotic, anti-Parkinsonism and psychotropic drugs, not elsewhere classified\n\n\n0.0091\nCorneal resistance factor (left)\n\n\n0.0090\nPoisoning by psychotropic agents\n\n\n0.0085\nSHBG\n\n\n0.0083\nT43 Poisoning by psychotropic drugs, not elsewhere classified\n\n\n0.0083\nSuicide or self-inflicted injury\n\n\n0.0080\nSuicidal ideation or attempt\n\n\n0.0075\nHome location - north co-ordinate (rounded)\n\n\n0.0075\nHome location at assessment - north co-ordinate (rounded)\n\n\n0.0074\nToxic effect of (non-ethyl) alcohol and petroleum and other solvents\n\n\n0.0073\nT51 Toxic effect of alcohol\n\n\n0.0073\nT39 Poisoning by nonopioid analgesics, antipyretics and antirheumatics\n\n\n0.0073\nCorneal hysteresis (right)\n\n\n0.0072\nX60 Intentional self-poisoning by and exposure to nonopioid analgesics, antipyretics and antirheumatics\n\n\n\n\n\n\n\nCode\nfor z in range(520,538):\n    pan_ukb_pleiotropy(z)\n\n\n\nOther Mental Disorder\n\n\n\nRPCA-IALM\n\n\nTop Factor: 6 (Contribution: 0.1623)\n\n\n\n\n\n0.0290\nTrunk fat percentage\n\n\n0.0231\nBody fat percentage\n\n\n0.0229\nImpedance of whole body\n\n\n0.0226\nImpedance of arm (right)\n\n\n0.0224\nImpedance of arm (left)\n\n\n0.0194\nTrunk fat mass\n\n\n0.0155\nArm fat percentage (left)\n\n\n0.0154\nWhole body fat mass\n\n\n0.0153\nArm fat percentage (right)\n\n\n0.0139\nLeg fat percentage (right)\n\n\n0.0133\nLeg fat percentage (left)\n\n\n0.0132\nImpedance of leg (left)\n\n\n0.0129\nImpedance of leg (right)\n\n\n0.0090\nLeg fat mass (right)\n\n\n0.0088\nArm fat mass (left)\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 82 (Contribution: 0.0722)\n\n\n\n\n\n0.0312\nBody mass index (BMI)\n\n\n0.0297\nLeg predicted mass (left)\n\n\n0.0226\nLeg fat percentage (left)\n\n\n0.0159\nArm fat mass (left)\n\n\n0.0128\nBody mass index (BMI)\n\n\n0.0097\nWhole body fat mass\n\n\n0.0094\nDiastolic blood pressure, automated reading, adjusted by medication\n\n\n0.0088\nHand grip strength (left)\n\n\n0.0082\naspirin\n\n\n0.0082\nArm fat-free mass (left)\n\n\n0.0081\nSHBG\n\n\n0.0073\nTreatment/medication code\n\n\n0.0072\nI25 Chronic ischaemic heart disease\n\n\n0.0069\ncorticosteroid\n\n\n0.0068\nC-reactive protein\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 10 (Contribution: 0.1058)\n\n\n\n\n\n0.0384\nHigh light scatter reticulocyte percentage\n\n\n0.0375\nMean corpuscular haemoglobin\n\n\n0.0339\nMean corpuscular volume\n\n\n0.0326\nHigh light scatter reticulocyte count\n\n\n0.0324\nReticulocyte percentage\n\n\n0.0254\nReticulocyte count\n\n\n0.0235\nImpedance of whole body\n\n\n0.0212\nImpedance of arm (right)\n\n\n0.0203\nImpedance of arm (left)\n\n\n0.0188\nImmature reticulocyte fraction\n\n\n0.0176\nImpedance of leg (left)\n\n\n0.0172\nImpedance of leg (right)\n\n\n0.0145\nTrunk fat percentage\n\n\n0.0140\nRed blood cell (erythrocyte) distribution width\n\n\n0.0121\nStanding height\n\n\n\n\n\n\nSubstance Addiction And Disorders\n\n\n\nRPCA-IALM\n\n\nTop Factor: 94 (Contribution: 0.0746)\n\n\n\n\n\n0.0258\nIntra-ocular pressure, corneal-compensated (right)\n\n\n0.0234\nIntra-ocular pressure, corneal-compensated (left)\n\n\n0.0195\nHypotension NOS\n\n\n0.0195\nI95 Hypotension\n\n\n0.0191\nH40 Glaucoma\n\n\n0.0180\nGlaucoma\n\n\n0.0178\nOpen-angle glaucoma\n\n\n0.0176\nPrimary open angle glaucoma\n\n\n0.0155\nHypotension\n\n\n0.0132\nCorneal hysteresis (right)\n\n\n0.0124\nIntra-ocular pressure, Goldmann-correlated (right)\n\n\n0.0124\nCorneal hysteresis (left)\n\n\n0.0111\nIntra-ocular pressure, Goldmann-correlated (left)\n\n\n0.0110\nInguinal hernia\n\n\n0.0102\nK40 Inguinal hernia\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 32 (Contribution: 0.0623)\n\n\n\n\n\n0.0387\nTrunk fat mass\n\n\n0.0221\nTrunk fat-free mass\n\n\n0.0160\nWhole body fat mass\n\n\n0.0147\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0113\nAnkle spacing width\n\n\n0.0108\nI25 Chronic ischaemic heart disease\n\n\n0.0106\nMulti-site chronic pain\n\n\n0.0102\nComparative height size at age 10\n\n\n0.0098\nLeg predicted mass (right)\n\n\n0.0094\nDrive faster than motorway speed limit\n\n\n0.0092\nHand grip strength (left)\n\n\n0.0089\nibuprofen\n\n\n0.0087\nImpedance of leg (right)\n\n\n0.0078\nHip circumference\n\n\n0.0078\nDiastolic blood pressure, combined automated + manual reading\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 56 (Contribution: 0.1156)\n\n\n\n\n\n0.0227\nAlkaline phosphatase\n\n\n0.0134\nRed blood cell (erythrocyte) distribution width\n\n\n0.0104\nAlbumin\n\n\n0.0079\nAlanine aminotransferase\n\n\n0.0077\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0072\nOther disorders of bladder\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0071\nOther retinal disorders\n\n\n0.0065\nCystatin C\n\n\n0.0064\nN32 Other disorders of bladder\n\n\n0.0062\nCalcium\n\n\n0.0061\nGamma glutamyltransferase\n\n\n0.0057\nDiabetic retinopathy\n\n\n0.0057\nHereditary retinal dystrophies\n\n\n0.0057\nH36 Retinal disorders in diseases classified elsewhere\n\n\n\n\n\n\nAlcohol-Related Disorders\n\n\n\nRPCA-IALM\n\n\nTop Factor: 57 (Contribution: 0.0858)\n\n\n\n\n\n0.0175\nJ44 Other chronic obstructive pulmonary disease\n\n\n0.0170\nObstructive chronic bronchitis\n\n\n0.0160\nChronic bronchitis\n\n\n0.0132\nPulmonary embolism and infarction, acute\n\n\n0.0132\nAcute pulmonary heart disease\n\n\n0.0130\nChronic airway obstruction\n\n\n0.0121\nI26 Pulmonary embolism\n\n\n0.0113\nHDL cholesterol\n\n\n0.0105\nApolipoprotein A\n\n\n0.0093\nTotal thigh muscle volume\n\n\n0.0082\nPulmonary heart disease\n\n\n0.0075\nAlcohol intake frequency.\n\n\n0.0073\nPosterior thigh lean muscle volume (right)\n\n\n0.0072\nPosterior thigh lean muscle volume (left)\n\n\n0.0068\nK57 Diverticular disease of intestine\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 5 (Contribution: 0.0513)\n\n\n\n\n\n0.0297\nMean arterial pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0277\nMean arterial pressure, automated reading, adjusted by medication\n\n\n0.0277\nSystolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0263\nSystolic blood pressure, automated reading, adjusted by medication\n\n\n0.0218\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0211\nDiastolic blood pressure, automated reading, adjusted by medication\n\n\n0.0198\nMean arterial pressure, combined automated + manual reading\n\n\n0.0194\nSystolic blood pressure, combined automated + manual reading\n\n\n0.0187\nMean arterial pressure, automated reading\n\n\n0.0183\nSystolic blood pressure, automated reading\n\n\n0.0139\nDiastolic blood pressure, combined automated + manual reading\n\n\n0.0137\nNon-cancer illness code, self-reported\n\n\n0.0127\nDiastolic blood pressure, automated reading\n\n\n0.0116\nPulse pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0109\nPulse pressure, automated reading, adjusted by medication\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 56 (Contribution: 0.1049)\n\n\n\n\n\n0.0227\nAlkaline phosphatase\n\n\n0.0134\nRed blood cell (erythrocyte) distribution width\n\n\n0.0104\nAlbumin\n\n\n0.0079\nAlanine aminotransferase\n\n\n0.0077\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0072\nOther disorders of bladder\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0071\nOther retinal disorders\n\n\n0.0065\nCystatin C\n\n\n0.0064\nN32 Other disorders of bladder\n\n\n0.0062\nCalcium\n\n\n0.0061\nGamma glutamyltransferase\n\n\n0.0057\nDiabetic retinopathy\n\n\n0.0057\nHereditary retinal dystrophies\n\n\n0.0057\nH36 Retinal disorders in diseases classified elsewhere\n\n\n\n\n\n\nAlcoholism\n\n\n\nRPCA-IALM\n\n\nTop Factor: 94 (Contribution: 0.0867)\n\n\n\n\n\n0.0258\nIntra-ocular pressure, corneal-compensated (right)\n\n\n0.0234\nIntra-ocular pressure, corneal-compensated (left)\n\n\n0.0195\nHypotension NOS\n\n\n0.0195\nI95 Hypotension\n\n\n0.0191\nH40 Glaucoma\n\n\n0.0180\nGlaucoma\n\n\n0.0178\nOpen-angle glaucoma\n\n\n0.0176\nPrimary open angle glaucoma\n\n\n0.0155\nHypotension\n\n\n0.0132\nCorneal hysteresis (right)\n\n\n0.0124\nIntra-ocular pressure, Goldmann-correlated (right)\n\n\n0.0124\nCorneal hysteresis (left)\n\n\n0.0111\nIntra-ocular pressure, Goldmann-correlated (left)\n\n\n0.0110\nInguinal hernia\n\n\n0.0102\nK40 Inguinal hernia\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 15 (Contribution: 0.0573)\n\n\n\n\n\n0.0670\nEssential hypertension\n\n\n0.0596\nHypertension\n\n\n0.0278\nE78 Disorders of lipoprotein metabolism and other lipidaemias\n\n\n0.0271\nDisorders of lipoid metabolism\n\n\n0.0265\nHypercholesterolemia\n\n\n0.0256\nHyperlipidemia\n\n\n0.0252\nIschemic Heart Disease\n\n\n0.0197\nOther chronic ischemic heart disease, unspecified\n\n\n0.0174\nI25 Chronic ischaemic heart disease\n\n\n0.0138\nantipyretic\n\n\n0.0138\nparacetamol\n\n\n0.0134\nopioid analgesic|antipyretic\n\n\n0.0130\nAngina pectoris\n\n\n0.0111\nE11 Non-insulin-dependent diabetes mellitus\n\n\n0.0100\nLeg fat mass (left)\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 56 (Contribution: 0.0766)\n\n\n\n\n\n0.0227\nAlkaline phosphatase\n\n\n0.0134\nRed blood cell (erythrocyte) distribution width\n\n\n0.0104\nAlbumin\n\n\n0.0079\nAlanine aminotransferase\n\n\n0.0077\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0072\nOther disorders of bladder\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0071\nOther retinal disorders\n\n\n0.0065\nCystatin C\n\n\n0.0064\nN32 Other disorders of bladder\n\n\n0.0062\nCalcium\n\n\n0.0061\nGamma glutamyltransferase\n\n\n0.0057\nDiabetic retinopathy\n\n\n0.0057\nHereditary retinal dystrophies\n\n\n0.0057\nH36 Retinal disorders in diseases classified elsewhere\n\n\n\n\n\n\nAlcoholic Liver Damage\n\n\n\nRPCA-IALM\n\n\nTop Factor: 94 (Contribution: 0.0981)\n\n\n\n\n\n0.0258\nIntra-ocular pressure, corneal-compensated (right)\n\n\n0.0234\nIntra-ocular pressure, corneal-compensated (left)\n\n\n0.0195\nHypotension NOS\n\n\n0.0195\nI95 Hypotension\n\n\n0.0191\nH40 Glaucoma\n\n\n0.0180\nGlaucoma\n\n\n0.0178\nOpen-angle glaucoma\n\n\n0.0176\nPrimary open angle glaucoma\n\n\n0.0155\nHypotension\n\n\n0.0132\nCorneal hysteresis (right)\n\n\n0.0124\nIntra-ocular pressure, Goldmann-correlated (right)\n\n\n0.0124\nCorneal hysteresis (left)\n\n\n0.0111\nIntra-ocular pressure, Goldmann-correlated (left)\n\n\n0.0110\nInguinal hernia\n\n\n0.0102\nK40 Inguinal hernia\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 88 (Contribution: 0.0513)\n\n\n\n\n\n0.0319\nWhole body fat-free mass\n\n\n0.0251\nLeg fat percentage (right)\n\n\n0.0194\nHand grip strength (left)\n\n\n0.0120\nLeg predicted mass (right)\n\n\n0.0118\nForced vital capacity (FVC), Best measure\n\n\n0.0110\ncodine/paracetamol\n\n\n0.0103\nArm fat-free mass (right)\n\n\n0.0099\nMean arterial pressure, automated reading\n\n\n0.0096\nTrunk fat-free mass\n\n\n0.0096\nPeak expiratory flow (PEF)\n\n\n0.0092\nTrunk predicted mass\n\n\n0.0090\nBasal metabolic rate\n\n\n0.0089\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0071\nArm fat mass (left)\n\n\n0.0069\nAge completed full time education\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 89 (Contribution: 0.1356)\n\n\n\n\n\n0.0327\nGenital prolapse\n\n\n0.0325\nProlapse of vaginal walls\n\n\n0.0322\nN81 Female genital prolapse\n\n\n0.0185\nUterine/Uterovaginal prolapse\n\n\n0.0143\nPulse rate, automated reading\n\n\n0.0115\nC-reactive protein\n\n\n0.0084\nOther vitamin B12 deficiency anemia\n\n\n0.0084\nMegaloblastic anemia\n\n\n0.0084\nOther deficiency anemia\n\n\n0.0084\nD51 Vitamin B12 deficiency anaemia\n\n\n0.0077\nChronic liver disease and cirrhosis\n\n\n0.0076\nPulse rate\n\n\n0.0074\nGamma glutamyltransferase\n\n\n0.0073\nK76 Other diseases of liver\n\n\n0.0072\nOther disorders of liver\n\n\n\n\n\n\nTobacco Use Disorder\n\n\n\nRPCA-IALM\n\n\nTop Factor: 51 (Contribution: 0.0944)\n\n\n\n\n\n0.0368\nAlbumin/Globulin ratio\n\n\n0.0319\nNon-albumin protein\n\n\n0.0142\nLymphocyte count\n\n\n0.0085\nTotal protein\n\n\n0.0073\nLymphocyte percentage\n\n\n0.0072\nAlbumin\n\n\n0.0069\nNeutrophill percentage\n\n\n0.0067\nPlatelet count\n\n\n0.0063\nPast tobacco smoking\n\n\n0.0062\nSmoking status, ever vs never\n\n\n0.0062\nForced expiratory volume in 1-second (FEV1)\n\n\n0.0057\nCystitis\n\n\n0.0055\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0053\nAcute pulmonary heart disease\n\n\n0.0053\nPulmonary embolism and infarction, acute\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 42 (Contribution: 0.0911)\n\n\n\n\n\n0.0228\nLeg fat mass (right)\n\n\n0.0227\nArm predicted mass (left)\n\n\n0.0162\nAnkle spacing width (right)\n\n\n0.0147\nHip circumference\n\n\n0.0114\nselective serotonin re-uptake inhibitor|SSRI\n\n\n0.0110\nBody mass index (BMI)\n\n\n0.0108\nTrunk predicted mass\n\n\n0.0094\nproton pump inhibitor|PPI\n\n\n0.0091\nbeta-lactam antibiotic|antibiotic\n\n\n0.0087\nImpedance of leg (left)\n\n\n0.0085\nSitting height\n\n\n0.0073\nHeel bone mineral density (BMD) T-score, automated (left)\n\n\n0.0073\nHeel quantitative ultrasound index (QUI), direct entry (left)\n\n\n0.0071\nHypertension\n\n\n0.0071\nBody fat percentage\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 38 (Contribution: 0.0719)\n\n\n\n\n\n0.0276\nCorneal resistance factor (right)\n\n\n0.0261\nCorneal resistance factor (left)\n\n\n0.0205\nUrate\n\n\n0.0165\nLipoprotein A\n\n\n0.0159\nRed blood cell (erythrocyte) distribution width\n\n\n0.0155\nIntra-ocular pressure, Goldmann-correlated (left)\n\n\n0.0151\nAlkaline phosphatase\n\n\n0.0150\nIntra-ocular pressure, Goldmann-correlated (right)\n\n\n0.0148\nCorneal hysteresis (right)\n\n\n0.0137\nCorneal hysteresis (left)\n\n\n0.0133\nNon-cancer illness code, self-reported\n\n\n0.0112\nHypothyroidism NOS\n\n\n0.0111\nHypothyroidism\n\n\n0.0110\nE03 Other hypothyroidism\n\n\n0.0106\nSHBG\n\n\n\n\n\n\nSleep Disorders\n\n\n\nRPCA-IALM\n\n\nTop Factor: 86 (Contribution: 0.1209)\n\n\n\n\n\n0.0469\nAcquired foot deformities\n\n\n0.0468\nM20 Acquired deformities of fingers and toes\n\n\n0.0290\nHemoptysis\n\n\n0.0286\nR04 Haemorrhage from respiratory passages\n\n\n0.0275\nObesity\n\n\n0.0272\nE66 Obesity\n\n\n0.0272\nHallux valgus (Bunion)\n\n\n0.0253\nAcquired toe deformities\n\n\n0.0248\nAbnormal sputum\n\n\n0.0247\nOverweight, obesity and other hyperalimentation\n\n\n0.0123\nG47 Sleep disorders\n\n\n0.0119\nSleep disorders\n\n\n0.0111\nHammer toe (acquired)\n\n\n0.0110\nEpistaxis or throat hemorrhage\n\n\n0.0095\nProstatitis\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 4 (Contribution: 0.0595)\n\n\n\n\n\n0.1274\nStanding height\n\n\n0.0721\nSitting height\n\n\n0.0263\nSeated height\n\n\n0.0192\nComparative height size at age 10\n\n\n0.0190\nTrunk fat-free mass\n\n\n0.0189\nTrunk predicted mass\n\n\n0.0134\nWhole body fat-free mass\n\n\n0.0117\nWhole body water mass\n\n\n0.0114\nWaist circumference\n\n\n0.0112\nLeg fat mass (left)\n\n\n0.0112\nArm fat mass (right)\n\n\n0.0107\nLeg fat mass (right)\n\n\n0.0101\nArm fat mass (left)\n\n\n0.0099\nE66 Obesity\n\n\n0.0099\nObesity\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 91 (Contribution: 0.1044)\n\n\n\n\n\n0.0136\nAspartate aminotransferase\n\n\n0.0125\nPlatelet distribution width\n\n\n0.0112\nAlanine aminotransferase\n\n\n0.0084\nInflammatory bowel disease and other gastroenteritis and colitis\n\n\n0.0074\nSpherical power (right)\n\n\n0.0070\nGenital prolapse\n\n\n0.0070\nProlapse of vaginal walls\n\n\n0.0069\nSpherical power (left)\n\n\n0.0068\nN81 Female genital prolapse\n\n\n0.0068\nOpen wounds of head; neck; and trunk\n\n\n0.0063\nC79 Secondary malignant neoplasm of other sites\n\n\n0.0061\nE87 Other disorders of fluid, electrolyte and acid-base balance\n\n\n0.0061\nUlcerative colitis\n\n\n0.0061\nElectrolyte imbalance\n\n\n0.0061\nI77 Other disorders of arteries and arterioles\n\n\n\n\n\n\nSleep Apnea\n\n\n\nRPCA-IALM\n\n\nTop Factor: 86 (Contribution: 0.1234)\n\n\n\n\n\n0.0469\nAcquired foot deformities\n\n\n0.0468\nM20 Acquired deformities of fingers and toes\n\n\n0.0290\nHemoptysis\n\n\n0.0286\nR04 Haemorrhage from respiratory passages\n\n\n0.0275\nObesity\n\n\n0.0272\nE66 Obesity\n\n\n0.0272\nHallux valgus (Bunion)\n\n\n0.0253\nAcquired toe deformities\n\n\n0.0248\nAbnormal sputum\n\n\n0.0247\nOverweight, obesity and other hyperalimentation\n\n\n0.0123\nG47 Sleep disorders\n\n\n0.0119\nSleep disorders\n\n\n0.0111\nHammer toe (acquired)\n\n\n0.0110\nEpistaxis or throat hemorrhage\n\n\n0.0095\nProstatitis\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 99 (Contribution: 0.0651)\n\n\n\n\n\n0.0275\nImpedance of arm (right)\n\n\n0.0160\nproton pump inhibitor|PPI\n\n\n0.0146\nWhole body fat-free mass\n\n\n0.0119\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0113\nLeg fat percentage (left)\n\n\n0.0110\nForced expiratory volume in 1-second (FEV1), predicted\n\n\n0.0089\ncorticosteroid\n\n\n0.0087\nanti-fungal\n\n\n0.0078\nImpedance of whole body\n\n\n0.0077\nRenal failure\n\n\n0.0076\nAnkle spacing width\n\n\n0.0075\nWaist circumference\n\n\n0.0073\nHand grip strength (right)\n\n\n0.0072\nMean arterial pressure, combined automated + manual reading\n\n\n0.0069\ndoxycycline\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 91 (Contribution: 0.1055)\n\n\n\n\n\n0.0136\nAspartate aminotransferase\n\n\n0.0125\nPlatelet distribution width\n\n\n0.0112\nAlanine aminotransferase\n\n\n0.0084\nInflammatory bowel disease and other gastroenteritis and colitis\n\n\n0.0074\nSpherical power (right)\n\n\n0.0070\nGenital prolapse\n\n\n0.0070\nProlapse of vaginal walls\n\n\n0.0069\nSpherical power (left)\n\n\n0.0068\nN81 Female genital prolapse\n\n\n0.0068\nOpen wounds of head; neck; and trunk\n\n\n0.0063\nC79 Secondary malignant neoplasm of other sites\n\n\n0.0061\nE87 Other disorders of fluid, electrolyte and acid-base balance\n\n\n0.0061\nUlcerative colitis\n\n\n0.0061\nElectrolyte imbalance\n\n\n0.0061\nI77 Other disorders of arteries and arterioles\n\n\n\n\n\n\nOther Cerebral Degenerations\n\n\n\nRPCA-IALM\n\n\nTop Factor: 32 (Contribution: 0.0892)\n\n\n\n\n\n0.0219\nMean carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0207\nMean carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0195\nMean carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0187\nMean carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0165\nMaximum carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0158\nMaximum carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0151\nMinimum carotid IMT (intima-medial thickness) at 210 degrees\n\n\n0.0139\nMinimum carotid IMT (intima-medial thickness) at 240 degrees\n\n\n0.0138\nMaximum carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0138\nMinimum carotid IMT (intima-medial thickness) at 120 degrees\n\n\n0.0133\nMinimum carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0132\nMaximum carotid IMT (intima-medial thickness) at 150 degrees\n\n\n0.0097\nHeart valve disorders\n\n\n0.0093\nNeutrophill percentage\n\n\n0.0088\nNonrheumatic aortic valve disorders\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 76 (Contribution: 0.1215)\n\n\n\n\n\n0.0407\nArm fat-free mass (right)\n\n\n0.0200\ncorticosteroid\n\n\n0.0140\nArm fat-free mass (left)\n\n\n0.0105\nAnkle spacing width (left)\n\n\n0.0103\nWhole body fat-free mass\n\n\n0.0098\nCoronary atherosclerosis\n\n\n0.0097\nBody fat percentage\n\n\n0.0096\nclotrimazole\n\n\n0.0096\nArm predicted mass (right)\n\n\n0.0093\nLeg predicted mass (right)\n\n\n0.0080\nTrunk fat mass\n\n\n0.0079\nBody mass index (BMI)\n\n\n0.0072\nForced vital capacity (FVC), Best measure\n\n\n0.0068\nWhole body water mass\n\n\n0.0067\nArm fat percentage (left)\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 61 (Contribution: 0.1172)\n\n\n\n\n\n0.0324\nHeel bone mineral density (BMD)\n\n\n0.0322\nHeel bone mineral density (BMD) T-score, automated\n\n\n0.0322\nHeel quantitative ultrasound index (QUI), direct entry\n\n\n0.0258\nHeel Broadband ultrasound attenuation, direct entry\n\n\n0.0139\nUrate\n\n\n0.0119\nHeel bone mineral density (BMD) T-score, automated (left)\n\n\n0.0119\nHeel quantitative ultrasound index (QUI), direct entry (left)\n\n\n0.0119\nHeel broadband ultrasound attenuation (left)\n\n\n0.0116\nHeel bone mineral density (BMD) (left)\n\n\n0.0115\nHeel bone mineral density (BMD) T-score, automated (right)\n\n\n0.0115\nHeel quantitative ultrasound index (QUI), direct entry (right)\n\n\n0.0115\nHeel broadband ultrasound attenuation (right)\n\n\n0.0114\nHeel bone mineral density (BMD) (right)\n\n\n0.0103\nCerebrovascular disease\n\n\n0.0102\nCreatinine\n\n\n\n\n\n\nParkinson’s Disease\n\n\n\nRPCA-IALM\n\n\nTop Factor: 97 (Contribution: 0.0888)\n\n\n\n\n\n0.0441\nPsoriasis\n\n\n0.0425\nL40 Psoriasis\n\n\n0.0403\nPsoriasis and related disorders\n\n\n0.0187\nCardiac arrest\n\n\n0.0184\nCardiac arrest and ventricular fibrillation\n\n\n0.0181\nI46 Cardiac arrest\n\n\n0.0117\nK90 Intestinal malabsorption\n\n\n0.0114\nIntestinal malabsorption (non-celiac)\n\n\n0.0109\nObesity\n\n\n0.0108\nE66 Obesity\n\n\n0.0097\nOverweight, obesity and other hyperalimentation\n\n\n0.0095\nAortic aneurysm\n\n\n0.0094\nI71 Aortic aneurysm and dissection\n\n\n0.0090\nCeliac disease\n\n\n0.0085\nNoninflammatory disorders of ovary, fallopian tube, and broad ligament\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 70 (Contribution: 0.0852)\n\n\n\n\n\n0.0262\nTrunk fat percentage\n\n\n0.0240\nWhole body fat-free mass\n\n\n0.0207\nLeg predicted mass (left)\n\n\n0.0201\nArm fat-free mass (left)\n\n\n0.0172\nLeg fat percentage (left)\n\n\n0.0147\nLeg fat mass (right)\n\n\n0.0105\nLeg fat mass (left)\n\n\n0.0083\nNSAID|non-steroidal anti-inflammatory drug\n\n\n0.0079\nUsual walking pace\n\n\n0.0078\nproton pump inhibitor|PPI\n\n\n0.0073\nImpedance of leg (left)\n\n\n0.0071\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0065\nArm fat mass (right)\n\n\n0.0061\nDiastolic blood pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0059\ncodine/paracetamol\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 92 (Contribution: 0.1208)\n\n\n\n\n\n0.0149\nUrinary calculus\n\n\n0.0149\nN20 Calculus of kidney and ureter\n\n\n0.0147\nMaximum heart rate during fitness test\n\n\n0.0146\nECG, load\n\n\n0.0146\nMaximum workload during fitness test\n\n\n0.0139\nPulse rate, automated reading\n\n\n0.0138\nECG, heart rate\n\n\n0.0130\nECG, number of stages in a phase\n\n\n0.0126\nNumber of trend entries\n\n\n0.0117\nECG, phase time\n\n\n0.0110\nCalculus of ureter\n\n\n0.0102\nProcessed meat intake\n\n\n0.0101\nCalculus of kidney\n\n\n0.0088\nBeef intake\n\n\n0.0080\nLamb/mutton intake\n\n\n\n\n\n\nExtrapyramidal Disease And Abnormal Movement Disorders\n\n\n\nRPCA-IALM\n\n\nTop Factor: 68 (Contribution: 0.0752)\n\n\n\n\n\n0.0221\nProlapse of vaginal walls\n\n\n0.0212\nGenital prolapse\n\n\n0.0210\nN81 Female genital prolapse\n\n\n0.0119\nPlatelet count\n\n\n0.0116\nAlbumin\n\n\n0.0114\nPrimary/intrinsic cardiomyopathies\n\n\n0.0114\nI42 Cardiomyopathy\n\n\n0.0106\nCardiomyopathy\n\n\n0.0097\nSaturated fat\n\n\n0.0097\nLymphocyte count\n\n\n0.0096\nEosinophill percentage\n\n\n0.0089\nFat\n\n\n0.0085\nPolyp of corpus uteri\n\n\n0.0085\nTotal thigh muscle volume\n\n\n0.0084\nN84 Polyp of female genital tract\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 95 (Contribution: 0.0865)\n\n\n\n\n\n0.0408\nTrunk fat-free mass\n\n\n0.0198\nTrunk predicted mass\n\n\n0.0149\nLeg fat mass (right)\n\n\n0.0139\nLeg fat-free mass (right)\n\n\n0.0126\ntetracycline antibiotic|antibiotic\n\n\n0.0112\nE66 Obesity\n\n\n0.0111\nImpedance of leg (left)\n\n\n0.0093\nReticulocyte percentage\n\n\n0.0091\nBody mass index (BMI)\n\n\n0.0090\nArm predicted mass (right)\n\n\n0.0087\nselective serotonin re-uptake inhibitor|SSRI\n\n\n0.0086\nNon-cancer illness code, self-reported\n\n\n0.0084\nsimvastatin\n\n\n0.0078\nImpedance of whole body\n\n\n0.0072\nHigh light scatter reticulocyte count\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 95 (Contribution: 0.0773)\n\n\n\n\n\n0.0487\nSebaceous cyst\n\n\n0.0484\nDiseases of sebaceous glands\n\n\n0.0473\nL72 Follicular cysts of skin and subcutaneous tissue\n\n\n0.0373\nDiseases of hair and hair follicles\n\n\n0.0088\nIGF-1\n\n\n0.0086\nAcquired foot deformities\n\n\n0.0084\nM20 Acquired deformities of fingers and toes\n\n\n0.0078\nSodium in urine\n\n\n0.0071\nAcquired toe deformities\n\n\n0.0068\nFEV1/FVC ratio\n\n\n0.0067\nUrea\n\n\n0.0067\nCancer of bladder\n\n\n0.0065\nC67 Malignant neoplasm of bladder\n\n\n0.0065\nMalignant neoplasm of bladder\n\n\n0.0064\nCystitis\n\n\n\n\n\n\nDegenerative Disease Of The Spinal Cord\n\n\n\nRPCA-IALM\n\n\nTop Factor: 62 (Contribution: 0.0881)\n\n\n\n\n\n0.0405\nOther vitamin B12 deficiency anemia\n\n\n0.0405\nD51 Vitamin B12 deficiency anaemia\n\n\n0.0340\nMegaloblastic anemia\n\n\n0.0315\nOther deficiency anemia\n\n\n0.0202\nIntervertebral disc disorders\n\n\n0.0195\nG55 Nerve root and plexus compressions in diseases classified elsewhere\n\n\n0.0189\nOther and unspecified disc disorder\n\n\n0.0181\nNerve root and plexus disorders\n\n\n0.0163\nContracture of palmar fascia [Dupuytren's disease]\n\n\n0.0157\nDisorders of muscle, ligament, and fascia\n\n\n0.0146\nFasciitis\n\n\n0.0117\nM51 Other intervertebral disk disorders\n\n\n0.0103\nM72 Fibroblastic disorders\n\n\n0.0097\nNon-albumin protein\n\n\n0.0084\nEosinophill percentage\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 98 (Contribution: 0.0866)\n\n\n\n\n\n0.0240\nLeg predicted mass (right)\n\n\n0.0211\nArm fat percentage (right)\n\n\n0.0197\nWaist circumference\n\n\n0.0146\nImpedance of leg (left)\n\n\n0.0115\nLeg fat-free mass (left)\n\n\n0.0113\nArm fat-free mass (left)\n\n\n0.0112\nOverall health rating\n\n\n0.0108\nopioid analgesic|antipyretic\n\n\n0.0105\nLeg fat mass (right)\n\n\n0.0101\nArm fat percentage (left)\n\n\n0.0089\nantispasmodic\n\n\n0.0088\nHip circumference\n\n\n0.0085\nTrunk predicted mass\n\n\n0.0084\nArm predicted mass (left)\n\n\n0.0082\nWhole body fat-free mass\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 68 (Contribution: 0.0984)\n\n\n\n\n\n0.0098\nCorneal hysteresis (right)\n\n\n0.0095\nCorneal hysteresis (left)\n\n\n0.0094\nIntra-ocular pressure, corneal-compensated (left)\n\n\n0.0092\nI35 Nonrheumatic aortic valve disorders\n\n\n0.0092\nIntra-ocular pressure, corneal-compensated (right)\n\n\n0.0091\nNonrheumatic aortic valve disorders\n\n\n0.0079\nAlkaline phosphatase\n\n\n0.0075\nCongenital anomalies of great vessels\n\n\n0.0075\nIntervertebral disc disorders\n\n\n0.0072\n3mm strong meridian angle (left)\n\n\n0.0072\nOsteoporosis\n\n\n0.0072\nOsteoporosis NOS\n\n\n0.0071\nAortic aneurysm\n\n\n0.0070\nI71 Aortic aneurysm and dissection\n\n\n0.0070\nOther and unspecified disc disorder\n\n\n\n\n\n\nMultiple Sclerosis\n\n\n\nRPCA-IALM\n\n\nTop Factor: 74 (Contribution: 0.0911)\n\n\n\n\n\n0.0203\nInflammatory bowel disease and other gastroenteritis and colitis\n\n\n0.0168\nUlcerative colitis\n\n\n0.0164\nK51 Ulcerative colitis\n\n\n0.0120\nAlbumin\n\n\n0.0119\nDental caries\n\n\n0.0117\nTotal protein\n\n\n0.0113\nK02 Dental caries\n\n\n0.0112\nDiseases of hard tissues of teeth\n\n\n0.0107\nAortic aneurysm\n\n\n0.0103\nI71 Aortic aneurysm and dissection\n\n\n0.0096\nAppendicitis\n\n\n0.0095\nAcute appendicitis\n\n\n0.0093\nRegional enteritis\n\n\n0.0092\nK50 Crohn's disease [regional enteritis]\n\n\n0.0090\nPlatelet count\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 65 (Contribution: 0.0684)\n\n\n\n\n\n0.0205\nLeg fat mass (left)\n\n\n0.0158\nForced expiratory volume in 1-second (FEV1), predicted\n\n\n0.0104\nImpedance of whole body\n\n\n0.0095\nflucloxacillin\n\n\n0.0082\nArm predicted mass (right)\n\n\n0.0077\nArm fat percentage (left)\n\n\n0.0076\nEssential hypertension\n\n\n0.0073\nOverweight, obesity and other hyperalimentation\n\n\n0.0073\nBasal metabolic rate\n\n\n0.0073\nSitting height\n\n\n0.0073\nbendroflumethiazide\n\n\n0.0070\nForced expiratory volume in 1-second (FEV1), Best measure\n\n\n0.0067\nTrunk fat mass\n\n\n0.0067\nLeg fat-free mass (left)\n\n\n0.0065\natorvastatin\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 77 (Contribution: 0.0818)\n\n\n\n\n\n0.0424\nContracture of palmar fascia [Dupuytren's disease]\n\n\n0.0416\nDisorders of muscle, ligament, and fascia\n\n\n0.0413\nFasciitis\n\n\n0.0344\nM72 Fibroblastic disorders\n\n\n0.0276\nECG, number of stages in a phase\n\n\n0.0267\nECG, load\n\n\n0.0264\nMaximum workload during fitness test\n\n\n0.0229\nNumber of trend entries\n\n\n0.0226\nECG, phase time\n\n\n0.0129\nCorneal hysteresis (left)\n\n\n0.0129\nCorneal hysteresis (right)\n\n\n0.0118\nDuration of fitness test\n\n\n0.0077\nGetting up in morning\n\n\n0.0075\nCorneal resistance factor (left)\n\n\n0.0073\nCorneal resistance factor (right)\n\n\n\n\n\n\nOther Headache Syndromes\n\n\n\nRPCA-IALM\n\n\nTop Factor: 92 (Contribution: 0.0529)\n\n\n\n\n\n0.0288\nVaricose veins of lower extremity\n\n\n0.0265\nVaricose veins\n\n\n0.0247\nI83 Varicose veins of lower extremities\n\n\n0.0180\nAcquired foot deformities\n\n\n0.0176\nM20 Acquired deformities of fingers and toes\n\n\n0.0154\nOther non-epithelial cancer of skin\n\n\n0.0148\nC44 Other malignant neoplasms of skin\n\n\n0.0143\nSkin cancer\n\n\n0.0118\nAortic aneurysm\n\n\n0.0117\nI71 Aortic aneurysm and dissection\n\n\n0.0105\nR06 Abnormalities of breathing\n\n\n0.0103\nHallux valgus (Bunion)\n\n\n0.0102\nRespiratory abnormalities\n\n\n0.0091\nMacular degeneration (senile) of retina NOS\n\n\n0.0090\nDegeneration of macula and posterior pole of retina\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 48 (Contribution: 0.0612)\n\n\n\n\n\n0.0247\nWeight\n\n\n0.0190\nWhole body water mass\n\n\n0.0134\nTrunk fat mass\n\n\n0.0132\nAge at first live birth\n\n\n0.0128\nWeight\n\n\n0.0123\nArm fat-free mass (left)\n\n\n0.0119\nOverweight, obesity and other hyperalimentation\n\n\n0.0112\nBody mass index (BMI)\n\n\n0.0108\nLeg fat percentage (right)\n\n\n0.0089\nHip circumference\n\n\n0.0088\nLeg fat-free mass (right)\n\n\n0.0081\nTrunk predicted mass\n\n\n0.0064\naspirin\n\n\n0.0064\nLeg fat-free mass (left)\n\n\n0.0063\nsodium alginate / potassium bicarbonate\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 56 (Contribution: 0.1011)\n\n\n\n\n\n0.0227\nAlkaline phosphatase\n\n\n0.0134\nRed blood cell (erythrocyte) distribution width\n\n\n0.0104\nAlbumin\n\n\n0.0079\nAlanine aminotransferase\n\n\n0.0077\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0072\nOther disorders of bladder\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0071\nOther retinal disorders\n\n\n0.0065\nCystatin C\n\n\n0.0064\nN32 Other disorders of bladder\n\n\n0.0062\nCalcium\n\n\n0.0061\nGamma glutamyltransferase\n\n\n0.0057\nDiabetic retinopathy\n\n\n0.0057\nHereditary retinal dystrophies\n\n\n0.0057\nH36 Retinal disorders in diseases classified elsewhere\n\n\n\n\n\n\nMigraine\n\n\n\nRPCA-IALM\n\n\nTop Factor: 80 (Contribution: 0.0772)\n\n\n\n\n\n0.0312\nAppendicitis\n\n\n0.0307\nAcute appendicitis\n\n\n0.0287\nAppendiceal conditions\n\n\n0.0284\nK35 Acute appendicitis\n\n\n0.0208\nSebaceous cyst\n\n\n0.0203\nDiseases of sebaceous glands\n\n\n0.0186\nL72 Follicular cysts of skin and subcutaneous tissue\n\n\n0.0140\nContracture of palmar fascia [Dupuytren's disease]\n\n\n0.0134\nDisorders of muscle, ligament, and fascia\n\n\n0.0133\nIron deficiency anemias, unspecified or not due to blood loss\n\n\n0.0133\nIron deficiency anemias\n\n\n0.0121\nFasciitis\n\n\n0.0118\nD50 Iron deficiency anaemia\n\n\n0.0115\nHome location - north co-ordinate (rounded)\n\n\n0.0113\nHome location at assessment - north co-ordinate (rounded)\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 100 (Contribution: 0.0520)\n\n\n\n\n\n0.0150\nImpedance of arm (left)\n\n\n0.0145\nApolipoprotein A\n\n\n0.0134\nOverweight, obesity and other hyperalimentation\n\n\n0.0119\nHand grip strength (right)\n\n\n0.0117\nWeight\n\n\n0.0108\ncorticosteroid\n\n\n0.0107\nImpedance of leg (left)\n\n\n0.0102\nLeg predicted mass (right)\n\n\n0.0096\nBasal metabolic rate\n\n\n0.0096\nMaximum workload during fitness test\n\n\n0.0094\nForced vital capacity (FVC)\n\n\n0.0077\nAnkle spacing width\n\n\n0.0075\nantispasmodic\n\n\n0.0070\nPeak expiratory flow (PEF)\n\n\n0.0064\nRenal failure\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 56 (Contribution: 0.0774)\n\n\n\n\n\n0.0227\nAlkaline phosphatase\n\n\n0.0134\nRed blood cell (erythrocyte) distribution width\n\n\n0.0104\nAlbumin\n\n\n0.0079\nAlanine aminotransferase\n\n\n0.0077\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0072\nOther disorders of bladder\n\n\n0.0071\nAspartate aminotransferase\n\n\n0.0071\nOther retinal disorders\n\n\n0.0065\nCystatin C\n\n\n0.0064\nN32 Other disorders of bladder\n\n\n0.0062\nCalcium\n\n\n0.0061\nGamma glutamyltransferase\n\n\n0.0057\nDiabetic retinopathy\n\n\n0.0057\nHereditary retinal dystrophies\n\n\n0.0057\nH36 Retinal disorders in diseases classified elsewhere\n\n\n\n\n\n\nHemiplegia\n\n\n\nRPCA-IALM\n\n\nTop Factor: 50 (Contribution: 0.1700)\n\n\n\n\n\n0.0225\nI35 Nonrheumatic aortic valve disorders\n\n\n0.0224\nNonrheumatic aortic valve disorders\n\n\n0.0149\nLate effects of cerebrovascular disease\n\n\n0.0145\nI69 Sequelae of cerebrovascular disease\n\n\n0.0142\nCerebrovascular disease\n\n\n0.0142\nCongenital anomalies of great vessels\n\n\n0.0131\nCardiac congenital anomalies\n\n\n0.0123\nSuicide or self-inflicted injury\n\n\n0.0119\nHeart valve disorders\n\n\n0.0117\nX61 Intentional self-poisoning by and exposure to antiepileptic, sedative-hypnotic, anti-Parkinsonism and psychotropic drugs, not elsewhere classified\n\n\n0.0117\nHemiplegia\n\n\n0.0115\nG81 Hemiplegia\n\n\n0.0114\nSuicidal ideation or attempt\n\n\n0.0114\nT51 Toxic effect of alcohol\n\n\n0.0114\nToxic effect of (non-ethyl) alcohol and petroleum and other solvents\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 25 (Contribution: 0.0582)\n\n\n\n\n\n0.0297\nObesity\n\n\n0.0293\nE66 Obesity\n\n\n0.0172\nOverweight, obesity and other hyperalimentation\n\n\n0.0159\nLeg fat mass (left)\n\n\n0.0155\nPulse pressure, automated reading, adjusted by medication\n\n\n0.0134\nEstimated glomerular filtration rate, serum creatinine + cystain C\n\n\n0.0128\nSeating box height\n\n\n0.0127\nPulse pressure, combined automated + manual reading, adjusted by medication\n\n\n0.0127\nCoronary atherosclerosis\n\n\n0.0118\nPulse pressure, combined automated + manual reading\n\n\n0.0106\nSitting height\n\n\n0.0105\nHMG CoA reductase inhibitor|statin\n\n\n0.0102\nlansoprazole\n\n\n0.0099\nTrunk fat percentage\n\n\n0.0095\nWhole body fat mass\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 64 (Contribution: 0.2000)\n\n\n\n\n\n0.0165\nCerebrovascular disease\n\n\n0.0127\nSHBG\n\n\n0.0124\nAcute pulmonary heart disease\n\n\n0.0124\nPulmonary embolism and infarction, acute\n\n\n0.0123\nI26 Pulmonary embolism\n\n\n0.0113\nPulmonary heart disease\n\n\n0.0104\nBreast cancer [female]\n\n\n0.0103\nLate effects of cerebrovascular disease\n\n\n0.0103\nBreast cancer\n\n\n0.0100\nI69 Sequelae of cerebrovascular disease\n\n\n0.0099\nCerebral ischemia\n\n\n0.0097\nMalignant neoplasm of female breast\n\n\n0.0095\nC50 Malignant neoplasm of breast\n\n\n0.0093\nHemiplegia\n\n\n0.0092\nOcclusion of cerebral arteries\n\n\n\n\n\n\nEpilepsy, Recurrent Seizures, Convulsions\n\n\n\nRPCA-IALM\n\n\nTop Factor: 98 (Contribution: 0.0502)\n\n\n\n\n\n0.0171\nPlatelet count\n\n\n0.0121\nEosinophill count\n\n\n0.0101\nPolyp of corpus uteri\n\n\n0.0099\nR06 Abnormalities of breathing\n\n\n0.0098\nN84 Polyp of female genital tract\n\n\n0.0097\nRespiratory abnormalities\n\n\n0.0093\nPolyp of female genital organs\n\n\n0.0091\nI71 Aortic aneurysm and dissection\n\n\n0.0089\nAortic aneurysm\n\n\n0.0083\nPlatelet crit\n\n\n0.0083\nEosinophill percentage\n\n\n0.0074\nBundle branch block\n\n\n0.0071\nPurpura and other hemorrhagic conditions\n\n\n0.0070\nCardiac conduction disorders\n\n\n0.0070\nD69 Purpura and other haemorrhagic conditions\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 96 (Contribution: 0.1166)\n\n\n\n\n\n0.0249\nAnkle spacing width (right)\n\n\n0.0187\nImpedance of leg (left)\n\n\n0.0154\nproton pump inhibitor|PPI\n\n\n0.0153\nAngina pectoris\n\n\n0.0143\nWhole body fat-free mass\n\n\n0.0134\nNon-cancer illness code, self-reported\n\n\n0.0127\nbenzodiazepine|sedative|anxiety\n\n\n0.0115\nIschemic Heart Disease\n\n\n0.0111\nSitting height\n\n\n0.0088\nLeg predicted mass (left)\n\n\n0.0088\nSeated height\n\n\n0.0085\nLeg fat mass (left)\n\n\n0.0084\nPeak expiratory flow (PEF)\n\n\n0.0072\nArm predicted mass (right)\n\n\n0.0070\nAge at first live birth\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 64 (Contribution: 0.1090)\n\n\n\n\n\n0.0165\nCerebrovascular disease\n\n\n0.0127\nSHBG\n\n\n0.0124\nAcute pulmonary heart disease\n\n\n0.0124\nPulmonary embolism and infarction, acute\n\n\n0.0123\nI26 Pulmonary embolism\n\n\n0.0113\nPulmonary heart disease\n\n\n0.0104\nBreast cancer [female]\n\n\n0.0103\nLate effects of cerebrovascular disease\n\n\n0.0103\nBreast cancer\n\n\n0.0100\nI69 Sequelae of cerebrovascular disease\n\n\n0.0099\nCerebral ischemia\n\n\n0.0097\nMalignant neoplasm of female breast\n\n\n0.0095\nC50 Malignant neoplasm of breast\n\n\n0.0093\nHemiplegia\n\n\n0.0092\nOcclusion of cerebral arteries\n\n\n\n\n\n\nEpilepsy\n\n\n\nRPCA-IALM\n\n\nTop Factor: 98 (Contribution: 0.0738)\n\n\n\n\n\n0.0171\nPlatelet count\n\n\n0.0121\nEosinophill count\n\n\n0.0101\nPolyp of corpus uteri\n\n\n0.0099\nR06 Abnormalities of breathing\n\n\n0.0098\nN84 Polyp of female genital tract\n\n\n0.0097\nRespiratory abnormalities\n\n\n0.0093\nPolyp of female genital organs\n\n\n0.0091\nI71 Aortic aneurysm and dissection\n\n\n0.0089\nAortic aneurysm\n\n\n0.0083\nPlatelet crit\n\n\n0.0083\nEosinophill percentage\n\n\n0.0074\nBundle branch block\n\n\n0.0071\nPurpura and other hemorrhagic conditions\n\n\n0.0070\nCardiac conduction disorders\n\n\n0.0070\nD69 Purpura and other haemorrhagic conditions\n\n\n\n\n\nNNM-Sparse-FW\n\n\nTop Factor: 96 (Contribution: 0.1300)\n\n\n\n\n\n0.0249\nAnkle spacing width (right)\n\n\n0.0187\nImpedance of leg (left)\n\n\n0.0154\nproton pump inhibitor|PPI\n\n\n0.0153\nAngina pectoris\n\n\n0.0143\nWhole body fat-free mass\n\n\n0.0134\nNon-cancer illness code, self-reported\n\n\n0.0127\nbenzodiazepine|sedative|anxiety\n\n\n0.0115\nIschemic Heart Disease\n\n\n0.0111\nSitting height\n\n\n0.0088\nLeg predicted mass (left)\n\n\n0.0088\nSeated height\n\n\n0.0085\nLeg fat mass (left)\n\n\n0.0084\nPeak expiratory flow (PEF)\n\n\n0.0072\nArm predicted mass (right)\n\n\n0.0070\nAge at first live birth\n\n\n\n\n\nRaw Data\n\n\nTop Factor: 61 (Contribution: 0.0849)\n\n\n\n\n\n0.0324\nHeel bone mineral density (BMD)\n\n\n0.0322\nHeel bone mineral density (BMD) T-score, automated\n\n\n0.0322\nHeel quantitative ultrasound index (QUI), direct entry\n\n\n0.0258\nHeel Broadband ultrasound attenuation, direct entry\n\n\n0.0139\nUrate\n\n\n0.0119\nHeel bone mineral density (BMD) T-score, automated (left)\n\n\n0.0119\nHeel quantitative ultrasound index (QUI), direct entry (left)\n\n\n0.0119\nHeel broadband ultrasound attenuation (left)\n\n\n0.0116\nHeel bone mineral density (BMD) (left)\n\n\n0.0115\nHeel bone mineral density (BMD) T-score, automated (right)\n\n\n0.0115\nHeel quantitative ultrasound index (QUI), direct entry (right)\n\n\n0.0115\nHeel broadband ultrasound attenuation (right)\n\n\n0.0114\nHeel bone mineral density (BMD) (right)\n\n\n0.0103\nCerebrovascular disease\n\n\n0.0102\nCreatinine"
  },
  {
    "objectID": "notebooks/ukbb/2023-12-08-panukb-principal-components-v01.html",
    "href": "notebooks/ukbb/2023-12-08-panukb-principal-components-v01.html",
    "title": "Pan-UKB Principal Components v01",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport matplotlib.pyplot as plt\nfrom pymir import mpl_stylesheet\nfrom pymir import mpl_utils\nmpl_stylesheet.banskt_presentation(splinecolor = 'black', dpi = 120, colors = 'kelly')\nfrom matplotlib.gridspec import GridSpec\n\nfrom nnwmf.optimize import IALM\nfrom nnwmf.optimize import FrankWolfe, FrankWolfe_CV\nfrom nnwmf.utils import model_errors as merr\n\nimport sys\nsys.path.append(\"../utils/\")\nimport simulate as mpy_simulate\n\n\n\nRead Data and Results\n\n\nCode\ndata_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/data\"\nresult_dir = \"/gpfs/commons/home/sbanerjee/work/npd/PanUKB/nnwmf\"\nzscore_filename = f\"{data_dir}/GWAS_Zscore.tsv\"\ntrait_filename = f\"{data_dir}/trait_manifest_TableS6_no_readme.tsv\"\nzscore_df = pd.read_csv(zscore_filename, sep = '\\t')\ntrait_df = pd.read_csv(trait_filename, sep = '\\t')\n\n# remove extra columns from trait_df\n\ncolnames = trait_df.columns.tolist()\ncolnames[0] = \"zindex\"\ntrait_df.columns = colnames\ntrait_df_mod = trait_df.drop(labels = ['coding', 'modifier', 'coding_description', 'filename', 'aws_link'], axis=1)\ntrait_df_mod\n\n\n\nzscore_df\n\n\n\n\n\n\n\n\nrsid\nz1\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\n...\nz2474\nz2475\nz2476\nz2477\nz2478\nz2479\nz2480\nz2481\nz2482\nz2483\n\n\n\n\n0\nrs6657440\n-0.903532\n0.561842\n0.711068\n-0.109174\n0.223668\n-1.728199\n0.374988\n-0.265971\n-2.823282\n...\n1.521092\n0.612532\n1.405428\n0.018029\n0.895337\n-0.008761\n-2.069432\n-4.292948\n-4.701711\n2.952899\n\n\n1\nrs7418179\n0.398166\n1.163539\n0.512118\n0.144794\n-1.313903\n-1.547410\n0.450270\n0.560324\n-1.502268\n...\n-0.296537\n-0.734266\n-0.093081\n0.412077\n1.961159\n0.716049\n-2.171984\n-5.314085\n-6.612137\n3.817518\n\n\n2\nrs80125161\n-1.739115\n-0.172328\n0.349145\n-0.329335\n-0.870640\n-1.004155\n1.128148\n0.151244\n-1.816075\n...\n2.222433\n1.092969\n2.328233\n1.160767\n0.909524\n-1.467249\n-0.135785\n-2.187241\n-3.223529\n4.508578\n\n\n3\nrs7524174\n-0.884478\n-1.762000\n1.312823\n-0.550764\n2.132540\n0.519828\n0.834194\n0.699441\n-0.885281\n...\n3.356354\n1.990588\n3.092179\n-0.133810\n-0.072845\n-1.376310\n1.317044\n0.913491\n0.535188\n2.245657\n\n\n4\nrs3829740\n-1.469931\n-0.519628\n-0.281605\n-0.267729\n-1.060167\n0.058116\n-0.638319\n-0.589767\n0.228514\n...\n-0.320075\n-0.128047\n-0.524757\n-0.232900\n-1.051020\n-0.483644\n2.026508\n4.400092\n5.407316\n1.125536\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n51394\nrs9616937\n-0.211947\n1.371231\n-1.800776\n0.609980\n-0.619822\n0.947269\n-1.166021\n0.478601\n-0.359714\n...\n0.714167\n0.354347\n0.611158\n-0.354725\n1.073043\n-0.831737\n0.870924\n1.432076\n2.228501\n0.536104\n\n\n51395\nrs1024374\n0.027097\n-1.817082\n0.530216\n0.813498\n-0.076514\n0.784427\n1.411160\n-1.111740\n-0.224438\n...\n1.107098\n1.482684\n1.512723\n0.322355\n-0.374603\n1.320194\n-0.700092\n-1.395039\n-2.270186\n0.360025\n\n\n51396\nrs144480800\n0.545682\n0.391830\n0.520505\n-1.280976\n0.453876\n-1.388940\n0.025094\n0.737788\n1.178641\n...\n-0.562063\n-1.148515\n-0.994185\n-0.268232\n-0.069619\n0.013256\n-0.777667\n-1.544760\n-1.406344\n2.205817\n\n\n51397\nrs5770994\n1.441851\n1.152368\n-1.500000\n-0.468137\n-0.444156\n-0.780139\n-0.853550\n-0.316097\n0.311219\n...\n-1.185702\n-0.624073\n-0.859522\n0.549669\n1.809912\n0.268733\n0.947441\n1.533302\n1.658537\n2.218653\n\n\n51398\nrs9616824\n0.669372\n-0.308184\n-0.811311\n-0.154224\n1.476823\n0.065167\n-1.030063\n-0.215723\n-0.631385\n...\n0.308486\n0.612102\n0.492977\n-0.621278\n-2.657386\n-0.732402\n-0.106474\n-0.029006\n0.070647\n-0.805253\n\n\n\n\n51399 rows × 2484 columns\n\n\n\n\n\nCode\nX_nan = np.array(zscore_df.loc[:, zscore_df.columns!='rsid']).T\nX_nan_cent = X_nan - np.nanmean(X_nan, axis = 0, keepdims = True)\nX_nan_mask = np.isnan(X_nan)\nX_cent = np.nan_to_num(X_nan_cent, copy = True, nan = 0.0)\n\nprint (f\"We have {X_cent.shape[0]} samples (phenotypes) and {X_cent.shape[1]} features (variants)\")\nprint (f\"Fraction of Nan entries: {np.sum(X_nan_mask) / np.prod(X_cent.shape):.3f}\")\n\n\nWe have 2483 samples (phenotypes) and 51399 features (variants)\nFraction of Nan entries: 0.000\n\n\n\n\nCode\nmf_methods = ['ialm', 'nnm_sparse', 'tsvd']\n\nmethod_prefix = {\n    'ialm' : 'ialm_maxiter10000_admm',\n    'nnm_sparse' : 'nnm_sparse_maxiter1000'\n}\n\nmethod_names = {\n    'tsvd' : 'Raw Data',\n    'ialm' : 'RPCA-IALM',\n    'nnm'  : 'NNM-FW',\n    'nnm_weighted' : 'NNM-Weighted',\n    'nnm_sparse' : 'NNM-Sparse-FW',\n}\n\nwith open (f\"{result_dir}/{method_prefix['ialm']}_progress.pkl\", 'rb') as handle:\n    ialm = pickle.load(handle)\n    \nwith open (f\"{result_dir}/{method_prefix['nnm_sparse']}_progress.pkl\", 'rb') as handle:\n    nnm_sparse = pickle.load(handle)\n\n\n\n\nConvergence of low rank approximations\nIn the following two figures, Figure 1 and Figure 2, we look at the convergence properties of the different methods on the Pan-UKB dataset.\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(range(len(ialm['mu_list'])), ialm['mu_list'], 'o-', label = 'ADMM')\nax1.legend()\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"Step size\")\n\nax2.plot(range(len(ialm['primal_residual'])),   np.log10(ialm['primal_residual']),   label = 'Primal Residual')\nax2.plot(range(len(ialm['dual_residual']) - 1), np.log10(ialm['dual_residual'][1:]), label = 'Dual Residual')\nax2.legend()\nax2.set_xlabel(\"Iteration\")\nax2.set_ylabel(\"Primal and Dual Residuals (log10 scale)\")\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Convergence properties of Robust PCA using IALM.\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nax1.plot(range(len(nnm_sparse['steps'])), nnm_sparse['steps'])\n# ax1.legend()\nax1.set_xlabel(\"Iteration\")\nax1.set_ylabel(\"Step size\")\n\nax2b = ax2.twinx()\ncolors = mpl_stylesheet.kelly_colors()\n#p1 = ax2.plot(range(len(nnm_sparse['objective']) - 2), - np.diff(nnm_sparse['objective'])[1:], 'o-', label = 'Objective', color = colors[0])\np1 = ax2.plot(range(len(nnm_sparse['objective']) - 1), nnm_sparse['objective'][1:], label = 'Objective', color = colors[0])\np2 = ax2b.plot(range(len(nnm_sparse['duality_gap']) - 2), nnm_sparse['duality_gap'][2:], label = 'Duality Gap', color = colors[1])\n\nax2.set_xlabel(\"Iteration\")\nax2.set_ylabel(\"Objective Function\")\nax2.yaxis.label.set_color(p1[0].get_color())\nax2b.set_ylabel(\"Duality Gap\")\nax2b.yaxis.label.set_color(p2[0].get_color())\n\nplt.tight_layout(w_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Convergence properties of sparse Nuclear Norm matrix factorization using Frank-Wolfe algorithm\n\n\n\n\n\n\n\nPCA of Low Rank Matrix\n\n\nCode\ndef get_principal_components(X):\n    X_cent = mpy_simulate.do_standardize(X, scale = False)\n    X_cent /= np.sqrt(np.prod(X_cent.shape))\n    U, S, Vt = np.linalg.svd(X_cent, full_matrices = False)\n    pcomps = U @ np.diag(S)\n    loadings = Vt.T @ np.diag(S)\n    return loadings, pcomps, S\n\nlowrank_X = dict()\nloadings  = dict()\npcomps    = dict()\neigenvals = dict()\n\nfor m in mf_methods:\n    if m != 'tsvd':\n        with open (f\"{result_dir}/{method_prefix[m]}_lowrank_X.pkl\", 'rb') as handle:\n            lowrank_X[m] = pickle.load(handle)\nlowrank_X['tsvd'] = X_cent.copy()\nfor m in mf_methods:\n    loadings[m], pcomps[m], eigenvals[m] = get_principal_components(lowrank_X[m])\n\n\n\n\nNuclear Norm of Low Rank Matrix\n\n\nCode\nfor m in mf_methods:\n    print (f\"{method_names[m]}: {np.linalg.norm(lowrank_X[m], 'nuc'):g}\")\n\n\nRPCA-IALM: 180216\nNNM-Sparse-FW: 1023.04\nRaw Data: 495872\n\n\n\n\nL0 norm of error matrix\n\n\nCode\nlowrank_E = dict()\nfor m in mf_methods:\n    if m != 'tsvd':\n        with open (f\"{result_dir}/{method_prefix[m]}_lowrank_E.pkl\", 'rb') as handle:\n            lowrank_E[m] = pickle.load(handle)\n\n\n\n\nCode\nfor m in mf_methods:\n    if m != 'tsvd':\n        print (f\"{method_names[m]}: {np.linalg.norm(lowrank_E[m], ord = 1):g}\")\n    else:\n        print (f\"{method_names[m]}: {np.linalg.norm(lowrank_X[m], ord = 1):g}\")\n\n\nRPCA-IALM: 2003.58\nNNM-Sparse-FW: 3491.56\nRaw Data: 4527.88\n\n\n\n\nPlots for Principal Components (Hidden Factors)\nIn the following figures, we look at the first few principal components (hidden factors) obtained by applying PCA on the predicted low-rank matrices. The samples are colored using the six trait types for the Pan-UKB project. They are: - continuous: Quantitative traits obtained from questionnaires and assessment centers, e.g. standing height, distance between home and job workplace, etc. - biomarkers: Obtained from biological samples, e.g. cholesterol, triglycerides. - prescriptions: Varied treatment/medication/prescriptions, e.g. amoxicillin, corticosteroids, etc. - icd10: Set of ICD10 codes - phecode: PHESANT software? Manually curated by FinnGen? - categorical: Categorical traits obtained from questionnaires and assessment centers, e.g. different self-reported illness, COVID-19, self-reported treatment/medication, etc.\n\n\nCode\ntrait_types  = trait_df_mod['trait_type'].unique().tolist()\ntrait_colors = {trait: color for trait, color in zip(trait_types, mpl_stylesheet.kelly_colors()[:len(trait_types)])}\ntrait_types.reverse()\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 0\nipc2 = 1\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: PC1 vs PC2\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 1\nipc2 = 2\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: PC2 vs PC3\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 2\nipc2 = 3\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: PC3 vs PC4\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 3\nipc2 = 4\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: PC4 vs PC5\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 4\nipc2 = 5\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: PC5 vs PC6\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 5\nipc2 = 6\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: PC6 vs PC7\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 6\nipc2 = 7\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: PC7 vs PC8\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 7\nipc2 = 8\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: PC8 vs PC9\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (20, 6))\nax = [None for m in mf_methods]\n\nipc1 = 8\nipc2 = 9\n\nfor i, m in enumerate(mf_methods):\n    ax[i] = fig.add_subplot(1, 3, i+1)\n    for t in trait_types:\n        tidx = np.array(trait_df_mod[trait_df_mod['trait_type'] == t].index)\n        ax[i].scatter(pcomps[m][tidx, ipc1], pcomps[m][tidx, ipc2], alpha = 0.5, color = trait_colors[t], label = t)\n    ax[i].set_title(method_names[m])\n    if i == 0:\n        ax[i].legend()\n    \nplt.tight_layout(h_pad = 2.0)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: PC9 vs PC10"
  },
  {
    "objectID": "meetings/2023-07-12.html",
    "href": "meetings/2023-07-12.html",
    "title": "2023-07-12",
    "section": "",
    "text": "I presented a high level summary of the methods we are developing.\nNeuropsychiatric disorders (NPD) are complex, heterogeneous disorders. Matrix factorization is a method that has been used to identify the shared and distinct factors of NPDs. Truncated SVD and genomic structural equation modeling (SEM) has been used earlier for multi-phenotype analysis. We are exploring different convex methods to obtain a low rank approximation of the input matrix before doing a PCA. Convex methods are reproducible. Currently, we are looking at 4 different methods for nuclear norm regularization to obtain the low rank approximation. We expect the low rank approximation will provide a more accurate estimate of the SNP’s true effect on a phenotype, cell type or other biological trait.\nAs of now, we hope to use the z-score and standard error for our model. GWAS data does not have to be limited to only patients with a diagnosis of BD, MDD or SZ as summary statistics from any phenotype will be modeled appropriately.\nCurrent main things to consider in regards to the model are:\n\nWhat is the normalisation method used in each of the GWAS summary statistics?\nIs there sample overlap between the studies?\n\nShane will follow-up with:\n\nResearching the normalisation method used in the selected studies.\nResearching the cohorts used / sample overlap between studies.",
    "crumbs": [
      "Meetings",
      "2023-07-12"
    ]
  }
]